{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08955d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import numpy as np\n",
    "import torch\n",
    "import itertools\n",
    "from loguru import logger\n",
    "from typing import Tuple, Callable, Optional, Dict, List\n",
    "from dataclasses import dataclass, field\n",
    "from numba import njit\n",
    "# from CONFIG import CONFIG\n",
    "# from FEATURE_ENGINEERING import FEATURE_ENGINEERING\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
    "from cuml.explainer import TreeExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07cbdfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    WSL = True\n",
    "    LOCAL = True\n",
    "    TRAIN = True\n",
    "\n",
    "    RANDOM_STATE = 42\n",
    "\n",
    "    DATE_COL = \"date_id\"\n",
    "\n",
    "    MIN_INVESTMENT, MAX_INVESTMENT = 0, 2\n",
    "\n",
    "    START_DATE = 1006\n",
    "    END_DATE = 8989 - 180\n",
    "    BINARY_FEATURE = \"D\"\n",
    "\n",
    "    if LOCAL:\n",
    "        TRAIN_X_PATH = \"data/train.csv\"\n",
    "        TRAIN_Y_PATH = \"data/train_labels.csv\"\n",
    "        TEST_PATH = \"data/test.csv\"\n",
    "        TARGET_PAIRS_PATH = \"data/target_pairs.csv\"\n",
    "    else:\n",
    "        BASE_PATH = \"/kaggle/input/hull-tactical-market-prediction\"\n",
    "        TRAIN_X_PATH = f\"{BASE_PATH}/train.csv\"\n",
    "        TRAIN_Y_PATH = f\"{BASE_PATH}/train_labels.csv\"\n",
    "        TEST_PATH = f\"{BASE_PATH}/test.csv\"\n",
    "        TARGET_PAIRS_PATH = f\"{BASE_PATH}/target_pairs.csv\"\n",
    "        MODEL_PATH = \"/kaggle/input/mitsui-nn/pytorch/test/1/sample.pth\"  ### TO CHANGE\n",
    "\n",
    "    if WSL:\n",
    "        mount = \"/mnt/c/Users/Admin/Desktop/Personal-Projects/Kaggle/Hull Tactical - Market Prediction/\"\n",
    "        TRAIN_X_PATH = f\"{mount}{TRAIN_X_PATH}\"\n",
    "        TRAIN_Y_PATH = f\"{mount}{TRAIN_Y_PATH}\"\n",
    "        TEST_PATH = f\"{mount}{TEST_PATH}\"\n",
    "\n",
    "    N_FOLDS = 5\n",
    "    SEQ_LEN = 16\n",
    "\n",
    "    BATCH_SIZE = 100\n",
    "\n",
    "    VERBOSE_EVAL = 10\n",
    "    NUM_BOOST_ROUND = 100\n",
    "    EARLY_STOPPING_ROUNDS = 100\n",
    "    XGB_PARAMS = {\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"device\": \"gpu\",\n",
    "        \"colsample_bylevel\": 0.4778,\n",
    "        \"colsample_bynode\": 0.3628,\n",
    "        \"colsample_bytree\": 0.7107,\n",
    "        \"gamma\": 1.7095,\n",
    "        \"learning_rate\": 0.02213,\n",
    "        \"max_depth\": 20,\n",
    "        \"max_leaves\": 12,\n",
    "        \"min_child_weight\": 16,\n",
    "        \"n_estimators\": 1667,\n",
    "        \"subsample\": 0.06567,\n",
    "        \"reg_alpha\": 39.3524,\n",
    "        \"reg_lambda\": 75.4484,\n",
    "        \"verbosity\": 0,\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "        \"early_stopping_rounds\": 100,\n",
    "        \"n_jobs\": -1,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6521241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M* - Market Dynamics/Technical features.\n",
    "# E* - Macro Economic features.\n",
    "# I* - Interest Rate features.\n",
    "# P* - Price/Valuation features.\n",
    "# V* - Volatility features.\n",
    "# S* - Sentiment features.\n",
    "# MOM* - Momentum features.\n",
    "# D* - Dummy/Binary features.\n",
    "\n",
    "@dataclass(slots=True)\n",
    "class FEATURE_ENGINEERING:\n",
    "    df: pl.LazyFrame\n",
    "\n",
    "    # ----------------------\n",
    "    # Autocorrelation Features\n",
    "    # ----------------------\n",
    "    def _compute_autocorr_torch(self, df: pl.LazyFrame) -> pl.LazyFrame:\n",
    "        \"\"\"\n",
    "        Compute rolling autocorrelations for each asset using Torch and GPU acceleration.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pl.LazyFrame\n",
    "            Input dataframe containing log returns\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pl.LazyFrame\n",
    "            Dataframe with autocorrelation features for multiple windows (10, 90, 252)\n",
    "        \"\"\"\n",
    "        names = [i for i in df.collect_schema().names() if i != CONFIG.DATE_COL]\n",
    "        windows = [10, 90, 252]\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        data_np = df.collect().select(names).to_numpy().astype(np.float32)\n",
    "        dates = df.collect().select(CONFIG.DATE_COL).to_series().to_list()\n",
    "        data = torch.tensor(data_np, device=device)\n",
    "\n",
    "        autocorrs = []\n",
    "        for window in windows:\n",
    "            rolling = data.unfold(0, window, 1).transpose(1, 2)\n",
    "            mean = rolling.mean(dim=1, keepdim=True)\n",
    "            centered = rolling - mean\n",
    "            var = (centered**2).mean(dim=1)\n",
    "            autocorr_num = (centered[:, 1:, :] * centered[:, :-1, :]).mean(dim=1)\n",
    "            autocorr = autocorr_num / var\n",
    "            output_dates = dates[window - 1 :]\n",
    "            schema = [f\"{name}_auto_corr_{window}\" for name in names]\n",
    "            autocorr_df = (\n",
    "                pl.DataFrame(autocorr.detach().cpu().numpy(), schema=schema)\n",
    "                .with_columns(pl.Series(CONFIG.DATE_COL, output_dates))\n",
    "                .select([CONFIG.DATE_COL] + schema)\n",
    "            )\n",
    "            autocorrs.append(autocorr_df)\n",
    "\n",
    "        all_auto_corrs = pl.DataFrame().with_columns(pl.Series(CONFIG.DATE_COL, dates))\n",
    "        for autocorr in autocorrs:\n",
    "            all_auto_corrs = all_auto_corrs.join(autocorr, how=\"left\", on=CONFIG.DATE_COL)\n",
    "\n",
    "        return all_auto_corrs.fill_null(strategy=\"backward\").lazy()\n",
    "\n",
    "    # ----------------------\n",
    "    # Return Skew Features\n",
    "    # ----------------------\n",
    "    def _compute_return_skew(self, df: pl.LazyFrame) -> pl.LazyFrame:\n",
    "        \"\"\"\n",
    "        Compute rolling skewness of returns for each asset over windows [5, 10, 90, 252].\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pl.LazyFrame\n",
    "            Input dataframe with return features\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pl.LazyFrame\n",
    "            Dataframe with rolling skew features\n",
    "        \"\"\"\n",
    "        names = [i for i in df.collect_schema().names() if i != CONFIG.DATE_COL and CONFIG.BINARY_FEATURE not in i]\n",
    "        return (\n",
    "            df.with_columns(\n",
    "                [pl.col(col).rolling_skew(window_size=window).alias(f\"{col}_return_skew_{window}\") for col in names for window in [5, 10, 90, 252]]\n",
    "            )\n",
    "            .drop([i for i in df.collect_schema().names() if i != CONFIG.DATE_COL])\n",
    "            .fill_null(0)\n",
    "        )\n",
    "\n",
    "    # ----------------------\n",
    "    # Rolling Statistics Features\n",
    "    # ----------------------\n",
    "    def _compute_rolling(self, df: pl.LazyFrame) -> pl.LazyFrame:\n",
    "        \"\"\"\n",
    "        Compute rolling mean, std, and SMA ratio for return features over windows [5, 10, 90, 252].\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pl.LazyFrame\n",
    "            Input dataframe with return features\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pl.LazyFrame\n",
    "            Dataframe with rolling statistics and SMA ratio features\n",
    "        \"\"\"\n",
    "        df = df.select(\n",
    "            [CONFIG.DATE_COL] + [col for col in self.df.collect_schema().names() if col != CONFIG.DATE_COL and CONFIG.BINARY_FEATURE not in col]\n",
    "        )\n",
    "        names = df.collect_schema().names()\n",
    "        names = [i for i in names if i != CONFIG.DATE_COL]\n",
    "        windows = [5, 10, 90, 252]\n",
    "        return (\n",
    "            df.with_columns(\n",
    "                [\n",
    "                    pl.col(col).rolling_mean(window_size=window, min_periods=2).alias(f\"{col}_sma_{window}\")\n",
    "                    for col in names\n",
    "                    if col\n",
    "                    for window in windows\n",
    "                ]\n",
    "                + [\n",
    "                    pl.col(col).rolling_std(window_size=window, min_periods=2).alias(f\"{col}_vol_{window}\")\n",
    "                    for col in names\n",
    "                    if col\n",
    "                    for window in windows\n",
    "                ]\n",
    "                + [\n",
    "                    (\n",
    "                        pl.col(col).rolling_mean(window_size=window, min_periods=2)\n",
    "                        / (pl.when(pl.col(col) < 0).then(pl.col(col)).otherwise(0.0).rolling_std(window_size=window, min_periods=2) + 1e-8)\n",
    "                    ).alias(f\"{col}_{window}_sortino\")\n",
    "                    for col in names\n",
    "                    if col\n",
    "                    for window in windows\n",
    "                ]\n",
    "            )\n",
    "            .with_columns([(pl.col(col) / pl.col(f\"{col}_sma_{window}\")).alias(f\"{col}_{window}_sharpe\") for col in names for window in windows])\n",
    "            .drop(names)\n",
    "        )\n",
    "\n",
    "    # ----------------------\n",
    "    # Beta Features\n",
    "    # ----------------------\n",
    "    def _compute_betas(self, df: pl.LazyFrame) -> pl.LazyFrame:\n",
    "        \"\"\"\n",
    "        Compute rolling betas for all asset pairs using Polars rolling covariance.\n",
    "\n",
    "        Beta of asset i with respect to asset j at time t is computed as:\n",
    "            beta[i,j] = cov(i,j) / var(j)\n",
    "\n",
    "        This method uses Polars rolling_cov and rolling_var without GPU acceleration.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pl.LazyFrame\n",
    "            Input dataframe containing asset returns. Must include a date column defined in CONFIG.DATE_COL.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pl.LazyFrame\n",
    "            Dataframe with rolling beta features for all asset pairs over a 90-day window.\n",
    "            The output columns are named as \"beta_{asset_i}_{asset_j}\".\n",
    "        \"\"\"\n",
    "        names = [i for i in df.collect_schema().names() if i != CONFIG.DATE_COL]\n",
    "\n",
    "        # Generate all unique asset pairs\n",
    "        pairs = [(names[i], names[j]) for i in range(len(names)) for j in range(i + 1, len(names))]\n",
    "\n",
    "        return (\n",
    "            df.with_columns(\n",
    "                [\n",
    "                    pl.rolling_cov(a=pl.col(p1), b=pl.col(p2), window_size=90, min_periods=2).alias(f\"beta_{p1}_{p2}\")\n",
    "                    / pl.col(p1).rolling_var(window_size=90, min_periods=2)\n",
    "                    for p1, p2 in pairs\n",
    "                    if p1 != CONFIG.DATE_COL or p2 != CONFIG.DATE_COL\n",
    "                ]\n",
    "            )\n",
    "            .drop(names)\n",
    "            .fill_null(0)\n",
    "        )\n",
    "\n",
    "    # ----------------------\n",
    "    # Pair Features\n",
    "    # ----------------------\n",
    "    def _compute_pairs_features(self, df: pl.LazyFrame) -> pl.LazyFrame:\n",
    "        names = df.collect_schema().names()\n",
    "        names = [i for i in names if i != CONFIG.DATE_COL]\n",
    "        err = 1e-6\n",
    "\n",
    "        exprs = []\n",
    "        for pair1, pair2 in itertools.combinations(names, 2):\n",
    "            p1 = pl.col(f\"{pair1}\")\n",
    "            p2 = pl.col(f\"{pair2}\")\n",
    "            exprs.extend(\n",
    "                [\n",
    "                    # Polynomial\n",
    "                    ((p1 + p2) ** 2).alias(f\"{pair1}_{pair2}_poly2\"),\n",
    "                    ((p1 + p2) ** 3).alias(f\"{pair1}_{pair2}_poly3\"),\n",
    "                    ((p1 - p2) ** 2).alias(f\"{pair1}_{pair2}_diff_squared\"),\n",
    "                    # Nonlinear transforms\n",
    "                    (p1 * p2).sqrt().alias(f\"{pair1}_{pair2}_sqrt_mul\"),\n",
    "                    (1 + (p1 * p2)).log().alias(f\"{pair1}_{pair2}_log_mul\"),\n",
    "                    (p1 - p2).exp().alias(f\"{pair1}_{pair2}_exp_diff\"),\n",
    "                    # Statistical / Comparative\n",
    "                    pl.when((p1 + p2) != 0).then(2 * p1 * p2 / (p1 + p2)).otherwise(None).alias(f\"{pair1}_{pair2}_harmonic_mean\"),\n",
    "                    pl.when(p2 != 0).then((p1 / p2).arctan()).otherwise(None).alias(f\"{pair1}_{pair2}_atan_ratio\"),\n",
    "                    pl.when(((p1 + err) / (p2 + err)) > 0).then(((p1 + err) / (p2 + err)).log()).otherwise(None).alias(f\"{pair1}_{pair2}_log_ratio\"),\n",
    "                    # Logistic / sigmoid\n",
    "                    (1 / (1 + (p1 - p2).neg().exp())).alias(f\"{pair1}_{pair2}_sigmoid_diff\"),\n",
    "                ]\n",
    "            )\n",
    "        return df.with_columns(exprs).fill_null(0).fill_nan(0).drop(names)\n",
    "\n",
    "    def create_market_features(self) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Create all engineered market features including temporal, returns, lags,\n",
    "        autocorrelation, OBV, skewness, volume z-score, market stats, ATR, and rolling stats.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pl.LazyFrame\n",
    "            Fully feature-engineered dataframe ready for modeling\n",
    "        \"\"\"\n",
    "        autocorr_df = self._compute_autocorr_torch(df=self.df)\n",
    "        skew_df = self._compute_return_skew(df=self.df)\n",
    "        rolling_stats_df = self._compute_rolling(df=self.df)\n",
    "        beta_df = self._compute_betas(df=self.df)\n",
    "        interactions_df = self._compute_pairs_features(df=self.df)\n",
    "\n",
    "        final_df = (\n",
    "            self.df.join(autocorr_df, on=CONFIG.DATE_COL)\n",
    "            .join(skew_df, on=CONFIG.DATE_COL)\n",
    "            .join(rolling_stats_df, on=CONFIG.DATE_COL)\n",
    "            .join(beta_df, on=CONFIG.DATE_COL)\n",
    "            .join(interactions_df, on=CONFIG.DATE_COL)\n",
    "            .collect()\n",
    "        )\n",
    "\n",
    "        non_binary_cols = [col for col in final_df.columns if final_df[col].n_unique() > 3 and col != CONFIG.DATE_COL]\n",
    "\n",
    "        windows = [10, 90, 252]\n",
    "\n",
    "        final_df = (\n",
    "            final_df.with_columns(\n",
    "                *[\n",
    "                    self.zscore(col=col, mean_window=mean_w, std_window=std_w)\n",
    "                    for col in non_binary_cols\n",
    "                    for (mean_w, std_w) in itertools.product(windows, repeat=2)\n",
    "                ],\n",
    "            )\n",
    "            .drop(non_binary_cols)\n",
    "            .fill_null(0)\n",
    "            .fill_nan(0)\n",
    "            .drop_nulls()\n",
    "        )\n",
    "\n",
    "        cols_to_drop = [col for col in final_df.select(cs.numeric()).columns if final_df[col].is_infinite().any()]\n",
    "        return final_df.drop(cols_to_drop)\n",
    "\n",
    "    def zscore(self, col: str, mean_window: int, std_window: int) -> pl.Expr:\n",
    "        return (\n",
    "            (pl.col(col) - pl.col(col).rolling_mean(window_size=mean_window, min_periods=2))\n",
    "            / pl.col(col).rolling_std(window_size=std_window, min_periods=2)\n",
    "        ).alias(f\"{col}_std_{mean_window}_{std_window}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d03775b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "class CustomMetrics:\n",
    "    \"\"\"Factory class for creating custom evaluation metrics\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def comp_metric(\n",
    "        predt: np.ndarray,\n",
    "        rfr_data: np.ndarray,\n",
    "        fwd_data: np.ndarray,\n",
    "    ) -> Tuple[str, float]:\n",
    "        position = np.clip(predt, CONFIG.MIN_INVESTMENT, CONFIG.MAX_INVESTMENT)\n",
    "\n",
    "        N = len(rfr_data)\n",
    "        strat_ret = rfr_data * (1 - position) + position * fwd_data\n",
    "        excess_ret = strat_ret - rfr_data\n",
    "        mean_excess = (1 + excess_ret).prod() ** (1 / N) - 1\n",
    "        std = strat_ret.std()\n",
    "\n",
    "        if std == 0:\n",
    "            return \"adj_sharpe\", 0.0\n",
    "\n",
    "        sharpe = mean_excess / std * np.sqrt(252)\n",
    "        strat_vol = std * np.sqrt(252) * 100\n",
    "        market_vol = fwd_data.std() * np.sqrt(252) * 100\n",
    "        market_mean = (1 + fwd_data - rfr_data).prod() ** (1 / N) - 1\n",
    "\n",
    "        vol_penalty = 1 + max(0, strat_vol / market_vol - 1.2) if market_vol > 0 else 0\n",
    "        return_penalty = 1 + ((max(0, (market_mean - mean_excess) * 100 * 252)) ** 2) / 100\n",
    "\n",
    "        return \"adj_sharpe\", min(sharpe / (vol_penalty * return_penalty), 1e6)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_volatility_adjusted_sharpe_xgb(\n",
    "        rfr_data: np.ndarray,\n",
    "        fwd_data: np.ndarray,\n",
    "    ) -> Callable:\n",
    "        \"\"\"\n",
    "        Create XGBoost custom metric with enclosed data.\n",
    "\n",
    "        Args:\n",
    "            rfr_data: Risk-free rate array\n",
    "            fwd_data: Forward returns array\n",
    "\n",
    "        Returns:\n",
    "            Custom metric function for XGBoost\n",
    "        \"\"\"\n",
    "\n",
    "        def metric(predt: np.ndarray, dtrain) -> Tuple[str, float]:\n",
    "            position = np.clip(predt, CONFIG.MIN_INVESTMENT, CONFIG.MAX_INVESTMENT)\n",
    "\n",
    "            N = len(rfr_data)\n",
    "            strat_ret = rfr_data * (1 - position) + position * fwd_data\n",
    "            excess_ret = strat_ret - rfr_data\n",
    "            mean_excess = (1 + excess_ret).prod() ** (1 / N) - 1\n",
    "            std = strat_ret.std()\n",
    "\n",
    "            if std == 0:\n",
    "                return \"adj_sharpe\", 0.0\n",
    "\n",
    "            sharpe = mean_excess / std * np.sqrt(252)\n",
    "            strat_vol = std * np.sqrt(252) * 100\n",
    "            market_vol = fwd_data.std() * np.sqrt(252) * 100\n",
    "            market_mean = (1 + fwd_data - rfr_data).prod() ** (1 / N) - 1\n",
    "\n",
    "            vol_penalty = 1 + max(0, strat_vol / market_vol - 1.2) if market_vol > 0 else 0\n",
    "            return_penalty = 1 + ((max(0, (market_mean - mean_excess) * 100 * 252)) ** 2) / 100\n",
    "\n",
    "            return \"adj_sharpe\", min(sharpe / (vol_penalty * return_penalty), 1e6)\n",
    "\n",
    "        return metric\n",
    "\n",
    "    @staticmethod\n",
    "    def create_volatility_adjusted_sharpe_lgb(\n",
    "        rfr_data: np.ndarray,\n",
    "        fwd_data: np.ndarray,\n",
    "    ) -> Callable:\n",
    "        \"\"\"\n",
    "        Create LightGBM custom metric with enclosed data.\n",
    "\n",
    "        Args:\n",
    "            rfr_data: Risk-free rate array\n",
    "            fwd_data: Forward returns array\n",
    "\n",
    "        Returns:\n",
    "            Custom metric function for LightGBM\n",
    "        \"\"\"\n",
    "\n",
    "        def metric(preds: np.ndarray, train_data) -> Tuple[str, float, bool]:\n",
    "            position = np.clip(preds, CONFIG.MIN_INVESTMENT, CONFIG.MAX_INVESTMENT)\n",
    "\n",
    "            N = len(rfr_data)\n",
    "            strat_ret = rfr_data * (1 - position) + position * fwd_data\n",
    "            excess_ret = strat_ret - rfr_data\n",
    "            mean_excess = (1 + excess_ret).prod() ** (1 / N) - 1\n",
    "            std = strat_ret.std()\n",
    "\n",
    "            if std == 0:\n",
    "                return \"adj_sharpe\", 0.0, True\n",
    "\n",
    "            sharpe = mean_excess / std * np.sqrt(252)\n",
    "            strat_vol = std * np.sqrt(252) * 100\n",
    "            market_vol = fwd_data.std() * np.sqrt(252) * 100\n",
    "            market_mean = (1 + fwd_data - rfr_data).prod() ** (1 / N) - 1\n",
    "\n",
    "            vol_penalty = 1 + max(0, strat_vol / market_vol - 1.2) if market_vol > 0 else 0\n",
    "            return_penalty = 1 + ((max(0, (market_mean - mean_excess) * 100 * 252)) ** 2) / 100\n",
    "\n",
    "            adj_sharpe = min(sharpe / (vol_penalty * return_penalty), 1e6)\n",
    "            return \"adj_sharpe\", adj_sharpe, True\n",
    "\n",
    "        return metric\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TimeSeriesModelTrainer:\n",
    "    \"\"\"Base class for time series model training with cross-validation\"\"\"\n",
    "\n",
    "    cv_results: List[Dict] = field(default_factory=list)\n",
    "    models: List = field(default_factory=list)\n",
    "    shap_df: Optional[pl.DataFrame] = None\n",
    "    feature_names: Optional[List[str]] = None\n",
    "\n",
    "    def train_cv(self, X: np.ndarray, y: np.ndarray, risk_free_rate: np.ndarray, forward_returns: np.ndarray) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform time series cross-validation training.\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix\n",
    "            y: Target variable\n",
    "            risk_free_rate: Risk-free rate array\n",
    "            forward_returns: Forward returns array\n",
    "            dates: Optional datetime index for logging\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with cross-validation results\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement train_cv method\")\n",
    "\n",
    "    def get_results_summary(self) -> pl.DataFrame:\n",
    "        \"\"\"Get summary of cross-validation results\"\"\"\n",
    "        df = pl.DataFrame(self.cv_results)\n",
    "        logger.info(f\"Mean Test Sharpe: {df['final_score'].mean():.4f} (+/- {df['final_score'].std():.4f})\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def compute_shap(self, model, x: np.ndarray, feature_names: Optional[List[str]] = None) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Compute SHAP values for feature importance using cuML TreeExplainer.\n",
    "\n",
    "        Args:\n",
    "            model: Trained XGBoost or LightGBM model\n",
    "            x: Feature matrix (numpy array or cupy array)\n",
    "            feature_names: List of feature names (optional)\n",
    "\n",
    "        Returns:\n",
    "            Polars DataFrame with mean absolute SHAP values per feature\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create TreeExplainer\n",
    "            explainer = TreeExplainer(model=model)\n",
    "\n",
    "            # Compute SHAP values\n",
    "            shap_values = explainer.shap_values(x)\n",
    "\n",
    "            # Convert to numpy if cupy array\n",
    "            if hasattr(shap_values, \"get\"):\n",
    "                shap_values = shap_values.get()\n",
    "\n",
    "            # Create DataFrame with absolute SHAP values\n",
    "            if feature_names is None:\n",
    "                feature_names = [f\"feature_{i}\" for i in range(x.shape[1])]\n",
    "\n",
    "            shap_df = pl.DataFrame(np.abs(shap_values), schema=feature_names).mean()\n",
    "\n",
    "            # Initialize or concatenate SHAP DataFrame\n",
    "            if self.shap_df is None:\n",
    "                self.shap_df = shap_df\n",
    "            else:\n",
    "                self.shap_df = pl.concat([self.shap_df, shap_df], how=\"vertical\").with_columns(pl.all().shrink_dtype())\n",
    "\n",
    "            return shap_df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error computing SHAP values: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_aggregated_shap_importance(self) -> pl.DataFrame:\n",
    "        \"\"\"\n",
    "        Get aggregated SHAP importance across all folds.\n",
    "\n",
    "        Returns:\n",
    "            Polars DataFrame with mean SHAP values and ranking\n",
    "        \"\"\"\n",
    "        if self.shap_df is None:\n",
    "            logger.warning(\"No SHAP values computed yet\")\n",
    "            return None\n",
    "\n",
    "        # Calculate mean SHAP importance across all folds\n",
    "        mean_shap = self.shap_df.mean()\n",
    "\n",
    "        # Create importance DataFrame\n",
    "        importance_df = (\n",
    "            pl.DataFrame({\"feature\": mean_shap.columns, \"shap_importance\": mean_shap.to_numpy()[0]})\n",
    "            .sort(\"shap_importance\", descending=True)\n",
    "            .with_columns(pl.col(\"shap_importance\").rank(descending=True).alias(\"rank\"))\n",
    "        )\n",
    "\n",
    "        return importance_df\n",
    "\n",
    "    def plot_shap_importance(self, top_n: int = 20):\n",
    "        \"\"\"\n",
    "        Plot top N features by SHAP importance.\n",
    "\n",
    "        Args:\n",
    "            top_n: Number of top features to display\n",
    "        \"\"\"\n",
    "        if self.shap_df is None:\n",
    "            logger.warning(\"No SHAP values computed yet\")\n",
    "            return\n",
    "\n",
    "        importance_df = self.get_aggregated_shap_importance()\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        # Get top N features\n",
    "        top_features = importance_df.head(top_n)\n",
    "\n",
    "        plt.figure(figsize=(10, max(6, top_n * 0.3)))\n",
    "        plt.barh(range(len(top_features)), top_features[\"shap_importance\"].to_list(), tick_label=top_features[\"feature\"].to_list())\n",
    "        plt.xlabel(\"Mean |SHAP value|\")\n",
    "        plt.ylabel(\"Feature\")\n",
    "        plt.title(f\"Top {top_n} Features by SHAP Importance\")\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class XGBoostTrainer(TimeSeriesModelTrainer):\n",
    "    \"\"\"XGBoost training pipeline with time series cross-validation\"\"\"\n",
    "\n",
    "    def __init__(self, params: Optional[Dict] = None):\n",
    "        super().__init__()\n",
    "        self.params = params or self._default_params()\n",
    "\n",
    "    def _default_params(self) -> Dict:\n",
    "        \"\"\"Default XGBoost parameters\"\"\"\n",
    "        return {\"max_depth\": 6, \"eta\": 0.1, \"device\": \"cuda\", \"tree_method\": \"hist\", \"seed\": CONFIG.RANDOM_STATE, \"disable_default_eval_metric\": 1}\n",
    "\n",
    "    def train_cv(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        risk_free_rate: np.ndarray,\n",
    "        forward_returns: np.ndarray,\n",
    "        batch_size: int,\n",
    "        NUM_BOOST_ROUND: int,\n",
    "        compute_shap: bool,\n",
    "        feature_names: list,\n",
    "    ) -> Tuple[pl.DataFrame, pl.DataFrame]:\n",
    "        \"\"\"Train XGBoost with time series cross-validation\"\"\"\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=CONFIG.N_FOLDS)\n",
    "\n",
    "        for fold, (train_index, test_index) in enumerate(tscv.split(X), 1):\n",
    "            # Split data\n",
    "            X_train, X_test = X[train_index], X[max(train_index) :]\n",
    "            y_train, y_test = y[train_index], y[max(train_index) :]\n",
    "\n",
    "            rfr_train, rfr_test = risk_free_rate[train_index], risk_free_rate[max(train_index) :]\n",
    "            fwd_train, fwd_test = forward_returns[train_index], forward_returns[max(train_index) :]\n",
    "\n",
    "            split_idx = int(len(X_train) * 0.8)\n",
    "            X_train, X_val = X_train[:split_idx], X_train[split_idx:]\n",
    "            y_train, y_val = y_train[:split_idx], y_train[split_idx:]\n",
    "            rfr_train, rfr_val = rfr_train[:split_idx], rfr_train[split_idx:]\n",
    "            fwd_train, fwd_val = fwd_train[:split_idx], fwd_train[split_idx:]\n",
    "\n",
    "            # Create DMatrix\n",
    "            dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "            dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "            # Create custom metrics\n",
    "            train_metric = CustomMetrics.create_volatility_adjusted_sharpe_xgb(rfr_train, fwd_train)\n",
    "            val_metric = CustomMetrics.create_volatility_adjusted_sharpe_xgb(rfr_val, fwd_val)\n",
    "\n",
    "            # Train\n",
    "            evals_result = {}\n",
    "            bst = xgb.train(\n",
    "                self.params,\n",
    "                dtrain,\n",
    "                num_boost_round=NUM_BOOST_ROUND,\n",
    "                evals=[(dval, \"val\")],\n",
    "                custom_metric=val_metric,\n",
    "                evals_result=evals_result,\n",
    "                early_stopping_rounds=CONFIG.EARLY_STOPPING_ROUNDS,\n",
    "                maximize=True,\n",
    "                verbose_eval=CONFIG.VERBOSE_EVAL,\n",
    "            )\n",
    "\n",
    "            BATCH_SIZE = batch_size  # Configure based on your timestep requirements\n",
    "            bst_incremental = bst\n",
    "            test_predictions = []\n",
    "\n",
    "            for batch_start in range(0, len(X_test), BATCH_SIZE):\n",
    "                batch_end = min(batch_start + BATCH_SIZE, len(X_test))\n",
    "\n",
    "                # Extract batch data\n",
    "                X_batch = X_test[batch_start:batch_end]\n",
    "                y_batch = y_test[batch_start:batch_end]\n",
    "                rfr_batch = rfr_test[batch_start:batch_end]\n",
    "                fwd_batch = fwd_test[batch_start:batch_end]\n",
    "\n",
    "                # Make predictions with current model before updating\n",
    "                d_batch_pred = xgb.DMatrix(X_batch)\n",
    "                batch_preds = bst_incremental.predict(d_batch_pred)\n",
    "                test_predictions.extend(batch_preds)\n",
    "\n",
    "                # Create DMatrix for incremental training\n",
    "                d_batch_train = xgb.DMatrix(X_batch, label=y_batch)\n",
    "                batch_metric = CustomMetrics.create_volatility_adjusted_sharpe_xgb(rfr_batch, fwd_batch)\n",
    "\n",
    "                # Update model with new batch\n",
    "                bst_incremental = xgb.train(\n",
    "                    {\n",
    "                        **self.params,\n",
    "                        \"process_type\": \"update\",\n",
    "                        \"updater\": \"refresh\",\n",
    "                        \"refresh_leaf\": True,\n",
    "                    },\n",
    "                    d_batch_train,\n",
    "                    num_boost_round=10,  # Can reduce this for faster updates\n",
    "                    xgb_model=bst_incremental,\n",
    "                    custom_metric=batch_metric,\n",
    "                    maximize=True,\n",
    "                    verbose_eval=False,\n",
    "                )\n",
    "\n",
    "            final_score = CustomMetrics.comp_metric(predt=np.array(test_predictions), rfr_data=rfr_test, fwd_data=fwd_test)\n",
    "\n",
    "            # Store results\n",
    "            self.models.append(bst)\n",
    "            self.cv_results.append(\n",
    "                {\n",
    "                    \"fold\": fold,\n",
    "                    \"model\": \"XGBoost\",\n",
    "                    \"train_size\": len(train_index),\n",
    "                    \"test_size\": len(test_index),\n",
    "                    \"best_iteration\": bst.best_iteration,\n",
    "                    \"best_score\": bst.best_score,\n",
    "                    \"train_start\": train_index[0],\n",
    "                    \"train_end\": train_index[-1],\n",
    "                    \"test_start\": test_index[0],\n",
    "                    \"test_end\": test_index[-1],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            logger.info(f\"Best iteration: {bst.best_iteration}, Best Sharpe: {bst.best_score:.4f}\")\n",
    "\n",
    "            if compute_shap:\n",
    "                logger.info(f\"  Computing SHAP values for fold {fold}...\")\n",
    "                X_test_float32 = X_test.astype(np.float32)\n",
    "                fold_shap = self.compute_shap(bst, X_test_float32, feature_names)\n",
    "                if fold_shap is not None:\n",
    "                    logger.info(\n",
    "                        f\"  Top 5 features: {fold_shap.transpose(include_header=True).sort(by='column_0', descending=True).head(5)['column'].to_list()}\"\n",
    "                    )\n",
    "        shap_importance = self.get_aggregated_shap_importance() if compute_shap else None\n",
    "\n",
    "        return shap_importance\n",
    "\n",
    "\n",
    "class LightGBMTrainer(TimeSeriesModelTrainer):\n",
    "    \"\"\"LightGBM training pipeline with time series cross-validation\"\"\"\n",
    "\n",
    "    def __init__(self, params: Optional[Dict] = None):\n",
    "        super().__init__()\n",
    "        self.params = params or self._default_params()\n",
    "\n",
    "    def _default_params(self) -> Dict:\n",
    "        \"\"\"Default LightGBM parameters\"\"\"\n",
    "        return {\n",
    "            \"max_depth\": 6,\n",
    "            # 'device': 'gpu',\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"seed\": CONFIG.RANDOM_STATE,\n",
    "            \"verbose\": -1,\n",
    "            \"metric\": \"None\",\n",
    "        }\n",
    "\n",
    "    def train_cv(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        risk_free_rate: np.ndarray,\n",
    "        forward_returns: np.ndarray,\n",
    "        batch_size: int,\n",
    "        NUM_BOOST_ROUND: int,\n",
    "        compute_shap: bool,\n",
    "        feature_names: list,\n",
    "    ) -> Tuple[pl.DataFrame, pl.DataFrame]:\n",
    "        \"\"\"Train LightGBM with time series cross-validation\"\"\"\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=CONFIG.N_FOLDS)\n",
    "\n",
    "        for fold, (train_index, test_index) in enumerate(tscv.split(X), 1):\n",
    "            # Split data\n",
    "            X_train, X_test = X[train_index], X[max(train_index) :]\n",
    "            y_train, y_test = y[train_index], y[max(train_index) :]\n",
    "\n",
    "            # Split risk_free_rate and forward_returns for train/test\n",
    "            rfr_train, rfr_test = risk_free_rate[train_index], risk_free_rate[max(train_index) :]\n",
    "            fwd_train, fwd_test = forward_returns[train_index], forward_returns[max(train_index) :]\n",
    "\n",
    "            # Further split train into train/val (including rfr and fwd)\n",
    "            split_idx = int(len(X_train) * 0.8)\n",
    "            X_train, X_val = X_train[:split_idx], X_train[split_idx:]\n",
    "            y_train, y_val = y_train[:split_idx], y_train[split_idx:]\n",
    "            rfr_train, rfr_val = rfr_train[:split_idx], rfr_train[split_idx:]\n",
    "            fwd_train, fwd_val = fwd_train[:split_idx], fwd_train[split_idx:]\n",
    "\n",
    "            # Create Datasets\n",
    "            train_data = lgb.Dataset(X_train, label=y_train)\n",
    "            val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "\n",
    "            # Create custom metric for validation\n",
    "            val_metric = CustomMetrics.create_volatility_adjusted_sharpe_lgb(rfr_val, fwd_val)\n",
    "\n",
    "            # Initial training with validation set\n",
    "            evals_result = {}\n",
    "            bst = lgb.train(\n",
    "                self.params,\n",
    "                train_data,\n",
    "                num_boost_round=NUM_BOOST_ROUND,\n",
    "                valid_sets=[val_data],\n",
    "                valid_names=[\"validation\"],\n",
    "                feval=val_metric,\n",
    "                callbacks=[\n",
    "                    lgb.early_stopping(stopping_rounds=CONFIG.EARLY_STOPPING_ROUNDS, verbose=True),\n",
    "                    lgb.log_evaluation(period=CONFIG.VERBOSE_EVAL),\n",
    "                    lgb.record_evaluation(evals_result),\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            # Get best iteration from initial training\n",
    "            best_iteration = bst.best_iteration\n",
    "            best_score = bst.best_score[\"validation\"][\"adj_sharpe\"]\n",
    "\n",
    "            # Incremental learning on test set with batch updates\n",
    "            BATCH_SIZE = batch_size  # Configure based on your timestep requirements\n",
    "            bst_incremental = bst\n",
    "            test_predictions = []\n",
    "\n",
    "            for batch_start in range(0, len(X_test), BATCH_SIZE):\n",
    "                batch_end = min(batch_start + BATCH_SIZE, len(X_test))\n",
    "\n",
    "                # Extract batch data\n",
    "                X_batch = X_test[batch_start:batch_end]\n",
    "                y_batch = y_test[batch_start:batch_end]\n",
    "                rfr_batch = rfr_test[batch_start:batch_end]\n",
    "                fwd_batch = fwd_test[batch_start:batch_end]\n",
    "\n",
    "                # Make predictions with current model before updating\n",
    "                batch_preds = bst_incremental.predict(X_batch)\n",
    "                test_predictions.extend(batch_preds)  # type:ignore\n",
    "\n",
    "                # Create Dataset for incremental training\n",
    "                batch_data = lgb.Dataset(X_batch, label=y_batch, reference=train_data)\n",
    "                batch_metric = CustomMetrics.create_volatility_adjusted_sharpe_lgb(rfr_batch, fwd_batch)\n",
    "\n",
    "                # Update model with new batch using init_model\n",
    "                bst_incremental = lgb.train(\n",
    "                    self.params,\n",
    "                    batch_data,\n",
    "                    num_boost_round=10,  # Add new trees per batch (adjust as needed)\n",
    "                    init_model=bst_incremental,  # Continue from previous model\n",
    "                    valid_sets=[batch_data],\n",
    "                    valid_names=[\"batch\"],\n",
    "                    feval=batch_metric,\n",
    "                    callbacks=[\n",
    "                        lgb.log_evaluation(period=0),  # Silent during batch updates\n",
    "                    ],\n",
    "                )\n",
    "\n",
    "            final_score = CustomMetrics.comp_metric(predt=np.array(test_predictions), rfr_data=rfr_test, fwd_data=fwd_test)\n",
    "\n",
    "            if compute_shap:\n",
    "                logger.info(f\"  Computing SHAP values for fold {fold}...\")\n",
    "                X_test_float32 = X_test.astype(np.float32)\n",
    "                fold_shap = self.compute_shap(bst, X_test_float32, feature_names)\n",
    "                if fold_shap is not None:\n",
    "                    logger.info(\n",
    "                        f\"  Top 5 features: {fold_shap.transpose(include_header=True).sort(by='column_0', descending=True).head(5)['column'].to_list()}\"\n",
    "                    )\n",
    "        shap_importance = self.get_aggregated_shap_importance() if compute_shap else None\n",
    "\n",
    "        return shap_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d471a51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelector:\n",
    "    def __init__(self, train_x, train_y):\n",
    "        self.train_x = train_x  # drop date\n",
    "        self.train_y = train_y\n",
    "        self.keep_features = None\n",
    "        self.total_features = train_x.columns.__len__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def basic_filters(self):\n",
    "        train_x_filter_1 = self.train_x.select([col for col in self.train_x.columns if self.train_x[col].var() > 1e-3])\n",
    "        train_x_filter_2 = train_x_filter_1.select(\n",
    "            [col for col in train_x_filter_1.columns if train_x_filter_1[col].value_counts()[\"count\"].max() / len(train_x_filter_1) < 0.80]\n",
    "        )\n",
    "\n",
    "        print(f\"After Basic Filter: {train_x_filter_2.columns.__len__()} / {self.total_features}\")\n",
    "\n",
    "        return train_x_filter_2\n",
    "\n",
    "    def run_correlation(self, x: torch.Tensor, y: torch.Tensor, names: list) -> list:\n",
    "        \"\"\"\n",
    "        Memory-optimized version using chunked processing\n",
    "        Best for when you have enough GPU memory\n",
    "\n",
    "        Args:\n",
    "            x: Tensor\n",
    "            y: Tensor\n",
    "\n",
    "        Returns:\n",
    "            correlations: Tensor\n",
    "        \"\"\"\n",
    "        N, D1 = x.shape\n",
    "        N2, D2 = y.shape\n",
    "        assert N == N2\n",
    "\n",
    "        device = x.device\n",
    "\n",
    "        # Handle NaNs by masking\n",
    "        x_valid = ~torch.isnan(x)\n",
    "        y_valid = ~torch.isnan(y)\n",
    "\n",
    "        # Convert NaNs to 0 for computation\n",
    "        x_clean = torch.where(x_valid, x, 0.0)\n",
    "        y_clean = torch.where(y_valid, y, 0.0)\n",
    "\n",
    "        # Compute valid sample counts for each pair efficiently\n",
    "        # This is the memory bottleneck, so we chunk it\n",
    "        chunk_size = 500  # Adjust based on GPU memory\n",
    "        correlations = torch.zeros(D1, D2, device=device)\n",
    "\n",
    "        for i in range(0, D1, chunk_size):\n",
    "            end_i = min(i + chunk_size, D1)\n",
    "\n",
    "            # Get chunk\n",
    "            x_chunk = x_clean[:, i:end_i]  # (N, chunk_size)\n",
    "            x_valid_chunk = x_valid[:, i:end_i]  # (N, chunk_size)\n",
    "\n",
    "            # Compute valid sample matrix for this chunk\n",
    "            valid_matrix = x_valid_chunk.unsqueeze(2) & y_valid.unsqueeze(1)  # (N, chunk_size, D2)\n",
    "            n_valid = valid_matrix.sum(dim=0).float()  # (chunk_size, D2)\n",
    "\n",
    "            # Sufficient samples mask\n",
    "            sufficient = n_valid >= 10\n",
    "\n",
    "            if sufficient.any():\n",
    "                # Compute means over valid samples\n",
    "                x_sum = (x_chunk.unsqueeze(2) * valid_matrix).sum(dim=0)  # (chunk_size, D2)\n",
    "                y_sum = (y_clean.unsqueeze(1) * valid_matrix).sum(dim=0)  # (chunk_size, D2)\n",
    "\n",
    "                x_mean = x_sum / (n_valid + 1e-10)\n",
    "                y_mean = y_sum / (n_valid + 1e-10)\n",
    "\n",
    "                # Center data\n",
    "                x_centered = (x_chunk.unsqueeze(2) - x_mean.unsqueeze(0)) * valid_matrix  # (N, chunk_size, D2)\n",
    "                y_centered = (y_clean.unsqueeze(1) - y_mean.unsqueeze(0)) * valid_matrix  # (N, chunk_size, D2)\n",
    "\n",
    "                # Compute correlation\n",
    "                numerator = (x_centered * y_centered).sum(dim=0)\n",
    "                x_var = (x_centered**2).sum(dim=0)\n",
    "                y_var = (y_centered**2).sum(dim=0)\n",
    "\n",
    "                denominator = torch.sqrt(x_var * y_var) + 1e-10\n",
    "                chunk_corr = numerator / denominator\n",
    "\n",
    "                # Apply sufficient samples mask\n",
    "                chunk_corr = torch.where(sufficient, chunk_corr, 0.0)\n",
    "                correlations[i:end_i] = torch.abs(chunk_corr)\n",
    "\n",
    "        self.correlations = correlations.cpu().numpy()\n",
    "\n",
    "    def run_selection(self):\n",
    "        # filtered = self.basic_filters()\n",
    "        train_x_arr = self.train_x.to_numpy()\n",
    "        train_y_arr = self.train_y.to_numpy()\n",
    "        x = torch.tensor(train_x_arr, device=\"cuda\")\n",
    "        y = torch.tensor(train_y_arr, device=\"cuda\")\n",
    "        self.run_correlation(x=x, y=y, names=self.train_x.columns)\n",
    "\n",
    "        corrs_df = pl.DataFrame(self.correlations, schema=[\"corr\"]).with_columns(pl.Series(name=\"feature\", values=self.train_x.columns))\n",
    "\n",
    "        keep_features = corrs_df.drop_nans().filter(pl.col(\"corr\") > 0.08)\n",
    "\n",
    "        print(f\"After correlation, keeping: {keep_features.__len__()} features\")\n",
    "\n",
    "        # MI_scores = self.run_MI(x=np.nan_to_num(self.train_x.select(keep_features).to_numpy()), y=np.nan_to_num(train_y_arr).T)\n",
    "        # return MI_scores, keep_features\n",
    "        return self.correlations, keep_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dcfa9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_engineered = pl.scan_parquet(\n",
    "    \"/mnt/c/Users/Admin/Desktop/Personal-Projects/Kaggle/Hull Tactical - Market Prediction/data/wsl_feature_impt_train.parquet\"\n",
    ")\n",
    "\n",
    "train_x = feat_engineered.filter(pl.col(CONFIG.DATE_COL).is_between(CONFIG.START_DATE, CONFIG.END_DATE)).collect()\n",
    "train_y = (\n",
    "    pl.scan_csv(CONFIG.TRAIN_Y_PATH, infer_schema_length=10_000)\n",
    "    .filter(pl.col(CONFIG.DATE_COL).is_between(CONFIG.START_DATE, CONFIG.END_DATE))\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "risk_free_rate = (\n",
    "    pl.scan_csv(CONFIG.TRAIN_X_PATH, infer_schema_length=10_000)\n",
    "    .filter(pl.col(CONFIG.DATE_COL).is_between(CONFIG.START_DATE, CONFIG.END_DATE))\n",
    "    .select([\"risk_free_rate\"])\n",
    "    .collect()\n",
    "    .to_numpy()\n",
    "    .flatten()\n",
    ")\n",
    "forward_returns = (\n",
    "    pl.scan_csv(CONFIG.TRAIN_X_PATH, infer_schema_length=10_000)\n",
    "    .filter(pl.col(CONFIG.DATE_COL).is_between(CONFIG.START_DATE, CONFIG.END_DATE))\n",
    "    .select([\"forward_returns\"])\n",
    "    .collect()\n",
    "    .to_numpy()\n",
    "    .flatten()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd413c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After correlation, keeping: 7530 features\n"
     ]
    }
   ],
   "source": [
    "feature_selector = FeatureSelector(train_x=train_x.drop(CONFIG.DATE_COL), train_y=train_y.drop(CONFIG.DATE_COL))\n",
    "corr, keep_features = feature_selector.run_selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "301831a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_features_list = keep_features.select(\"feature\").to_series().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9157fb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_x.drop(CONFIG.DATE_COL).select(keep_features_list).to_numpy()\n",
    "y = train_y.drop(CONFIG.DATE_COL).to_numpy().flatten()\n",
    "feature_names = train_x.drop(CONFIG.DATE_COL).select(keep_features_list).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b0d0e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tval-adj_sharpe:0.35977\n",
      "[10]\tval-adj_sharpe:0.40164\n",
      "[20]\tval-adj_sharpe:0.47632\n",
      "[30]\tval-adj_sharpe:0.39702\n",
      "[40]\tval-adj_sharpe:0.38688\n",
      "[50]\tval-adj_sharpe:0.46130\n",
      "[60]\tval-adj_sharpe:0.47974\n",
      "[70]\tval-adj_sharpe:0.57512\n",
      "[80]\tval-adj_sharpe:0.61050\n",
      "[90]\tval-adj_sharpe:0.66004\n",
      "[99]\tval-adj_sharpe:0.69459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edmund870/miniforge3/envs/rapids-25.06/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [23:53:49] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1748362148159/work/src/common/error_msg.cc:33: You have manually specified the `updater` parameter. The `tree_method` parameter will be ignored. Incorrect sequence of updaters will produce undefined behavior. For common uses, we recommend using `tree_method` parameter instead.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\u001b[32m2025-11-15 23:53:50.585\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_cv\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1mBest iteration: 98, Best Sharpe: 0.6955\u001b[0m\n",
      "\u001b[32m2025-11-15 23:53:50.585\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_cv\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1m  Computing SHAP values for fold 1...\u001b[0m\n",
      "\u001b[32m2025-11-15 23:53:51.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_cv\u001b[0m:\u001b[36m366\u001b[0m - \u001b[1m  Top 5 features: ['E2_M3_exp_diff_std_252_90', 'P2_std_252_10', 'E9_S10_sigmoid_diff_std_252_252', 'V3_V7_exp_diff_std_90_90', 'P11_std_252_10']\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tval-adj_sharpe:-0.04696\n",
      "[10]\tval-adj_sharpe:-0.09389\n",
      "[20]\tval-adj_sharpe:-0.13515\n",
      "[30]\tval-adj_sharpe:-0.07095\n",
      "[40]\tval-adj_sharpe:-0.07054\n",
      "[50]\tval-adj_sharpe:0.01726\n",
      "[60]\tval-adj_sharpe:-0.00023\n",
      "[70]\tval-adj_sharpe:0.04890\n",
      "[80]\tval-adj_sharpe:0.06450\n",
      "[90]\tval-adj_sharpe:0.08589\n",
      "[99]\tval-adj_sharpe:0.11354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-15 23:57:50.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_cv\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1mBest iteration: 99, Best Sharpe: 0.1135\u001b[0m\n",
      "\u001b[32m2025-11-15 23:57:50.034\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_cv\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1m  Computing SHAP values for fold 2...\u001b[0m\n",
      "\u001b[32m2025-11-15 23:57:50.492\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_cv\u001b[0m:\u001b[36m366\u001b[0m - \u001b[1m  Top 5 features: ['M16_M8_log_ratio_std_252_10', 'P10_sma_5_std_90_10', 'M16_M8_log_ratio_std_90_10', 'P11_P7_poly3_std_252_10', 'E9_S10_sigmoid_diff_std_252_252']\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tval-adj_sharpe:-0.25919\n",
      "[10]\tval-adj_sharpe:-0.19550\n",
      "[20]\tval-adj_sharpe:0.02488\n",
      "[30]\tval-adj_sharpe:-0.01675\n",
      "[40]\tval-adj_sharpe:-0.02537\n",
      "[50]\tval-adj_sharpe:-0.14632\n",
      "[60]\tval-adj_sharpe:-0.12184\n",
      "[70]\tval-adj_sharpe:-0.11006\n",
      "[80]\tval-adj_sharpe:-0.14058\n",
      "[90]\tval-adj_sharpe:-0.06828\n",
      "[99]\tval-adj_sharpe:-0.06690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-16 00:01:37.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_cv\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1mBest iteration: 36, Best Sharpe: 0.0557\u001b[0m\n",
      "\u001b[32m2025-11-16 00:01:37.193\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_cv\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1m  Computing SHAP values for fold 3...\u001b[0m\n",
      "\u001b[32m2025-11-16 00:01:35.135\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_cv\u001b[0m:\u001b[36m366\u001b[0m - \u001b[1m  Top 5 features: ['M17_P10_poly3_std_252_90', 'M8_V6_log_mul_std_10_10', 'M16_S11_log_ratio_std_90_10', 'E2_E8_exp_diff_std_252_252', 'E3_V3_exp_diff_std_252_10']\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tval-adj_sharpe:0.43123\n",
      "[10]\tval-adj_sharpe:0.41291\n",
      "[20]\tval-adj_sharpe:0.32984\n",
      "[30]\tval-adj_sharpe:0.32541\n",
      "[40]\tval-adj_sharpe:0.32861\n",
      "[50]\tval-adj_sharpe:0.29537\n",
      "[60]\tval-adj_sharpe:0.29125\n",
      "[70]\tval-adj_sharpe:0.31327\n",
      "[80]\tval-adj_sharpe:0.31349\n",
      "[90]\tval-adj_sharpe:0.29739\n",
      "[99]\tval-adj_sharpe:0.28623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-16 00:04:47.445\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_cv\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1mBest iteration: 1, Best Sharpe: 0.4397\u001b[0m\n",
      "\u001b[32m2025-11-16 00:04:47.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_cv\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1m  Computing SHAP values for fold 4...\u001b[0m\n",
      "\u001b[32m2025-11-16 00:04:47.695\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_cv\u001b[0m:\u001b[36m366\u001b[0m - \u001b[1m  Top 5 features: ['M17_P10_poly2_std_252_252', 'E3_P3_poly3_std_252_252', 'M8_V12_atan_ratio_std_90_90', 'P8_V1_log_mul_std_252_90', 'P8_V13_diff_squared_std_90_10']\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tval-adj_sharpe:0.49826\n",
      "[10]\tval-adj_sharpe:0.53899\n",
      "[20]\tval-adj_sharpe:0.51261\n",
      "[30]\tval-adj_sharpe:0.58617\n",
      "[40]\tval-adj_sharpe:0.58922\n",
      "[50]\tval-adj_sharpe:0.61618\n",
      "[60]\tval-adj_sharpe:0.64969\n",
      "[70]\tval-adj_sharpe:0.67923\n",
      "[80]\tval-adj_sharpe:0.69013\n",
      "[90]\tval-adj_sharpe:0.68959\n",
      "[99]\tval-adj_sharpe:0.68291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-16 00:07:53.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_cv\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1mBest iteration: 85, Best Sharpe: 0.6956\u001b[0m\n",
      "\u001b[32m2025-11-16 00:07:53.484\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_cv\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1m  Computing SHAP values for fold 5...\u001b[0m\n",
      "\u001b[32m2025-11-16 00:07:53.698\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_cv\u001b[0m:\u001b[36m366\u001b[0m - \u001b[1m  Top 5 features: ['P9_V10_poly3_std_252_252', 'I2_M1_poly3_std_252_252', 'E20_M1_diff_squared_std_252_90', 'M2_P1_poly3_std_252_10', 'M1_P11_diff_squared_std_252_252']\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "xgb_trainer = XGBoostTrainer()\n",
    "xgb_res = xgb_trainer.train_cv(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    risk_free_rate=risk_free_rate,\n",
    "    forward_returns=forward_returns,\n",
    "    batch_size=CONFIG.BATCH_SIZE,\n",
    "    NUM_BOOST_ROUND=CONFIG.NUM_BOOST_ROUND,\n",
    "    compute_shap=True,\n",
    "    feature_names=feature_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9ffa8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_res.write_csv(\"/mnt/c/Users/Admin/Desktop/Personal-Projects/Kaggle/Hull Tactical - Market Prediction/XGB_feature_importance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f46b4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalidation's adj_sharpe: 0.480006\n",
      "[20]\tvalidation's adj_sharpe: 0.310033\n",
      "[30]\tvalidation's adj_sharpe: 0.281605\n",
      "[40]\tvalidation's adj_sharpe: 0.282918\n",
      "[50]\tvalidation's adj_sharpe: 0.277143\n",
      "[60]\tvalidation's adj_sharpe: 0.241533\n",
      "[70]\tvalidation's adj_sharpe: 0.29549\n",
      "[80]\tvalidation's adj_sharpe: 0.28066\n",
      "[90]\tvalidation's adj_sharpe: 0.337667\n",
      "[100]\tvalidation's adj_sharpe: 0.37017\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6]\tvalidation's adj_sharpe: 0.576674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-16 00:08:14.290\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_cv\u001b[0m:\u001b[36m489\u001b[0m - \u001b[1m  Computing SHAP values for fold 1...\u001b[0m\n",
      "\u001b[32m2025-11-16 00:08:14.803\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_cv\u001b[0m:\u001b[36m493\u001b[0m - \u001b[1m  Top 5 features: ['P8_vol_5_std_252_10', 'P2_std_252_10', 'P2_V3_sigmoid_diff_std_90_252', 'P2_sma_10_std_10_10', 'M16_P8_poly3_std_252_252']\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalidation's adj_sharpe: 0.106539\n",
      "[20]\tvalidation's adj_sharpe: 0.200434\n",
      "[30]\tvalidation's adj_sharpe: 0.118707\n",
      "[40]\tvalidation's adj_sharpe: 0.0281534\n",
      "[50]\tvalidation's adj_sharpe: -0.0579819\n",
      "[60]\tvalidation's adj_sharpe: -0.0910764\n",
      "[70]\tvalidation's adj_sharpe: -0.0865236\n",
      "[80]\tvalidation's adj_sharpe: -0.00854346\n",
      "[90]\tvalidation's adj_sharpe: -0.0105604\n",
      "[100]\tvalidation's adj_sharpe: 0.0187411\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20]\tvalidation's adj_sharpe: 0.200434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-16 00:08:34.788\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_cv\u001b[0m:\u001b[36m489\u001b[0m - \u001b[1m  Computing SHAP values for fold 2...\u001b[0m\n",
      "\u001b[32m2025-11-16 00:08:35.098\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_cv\u001b[0m:\u001b[36m493\u001b[0m - \u001b[1m  Top 5 features: ['M17_P8_poly3_std_252_90', 'E19_V13_poly3_std_90_90', 'M8_V12_sigmoid_diff_std_10_10', 'M8_forward_returns_poly2_std_252_90', 'E3_P11_harmonic_mean_std_252_90']\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalidation's adj_sharpe: -0.435346\n",
      "[20]\tvalidation's adj_sharpe: -0.476897\n",
      "[30]\tvalidation's adj_sharpe: -0.40484\n",
      "[40]\tvalidation's adj_sharpe: -0.400977\n",
      "[50]\tvalidation's adj_sharpe: -0.389881\n",
      "[60]\tvalidation's adj_sharpe: -0.358356\n",
      "[70]\tvalidation's adj_sharpe: -0.363988\n",
      "[80]\tvalidation's adj_sharpe: -0.377634\n",
      "[90]\tvalidation's adj_sharpe: -0.394803\n",
      "[100]\tvalidation's adj_sharpe: -0.359085\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\tvalidation's adj_sharpe: -0.297938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-16 00:08:54.950\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_cv\u001b[0m:\u001b[36m489\u001b[0m - \u001b[1m  Computing SHAP values for fold 3...\u001b[0m\n",
      "\u001b[32m2025-11-16 00:08:55.196\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_cv\u001b[0m:\u001b[36m493\u001b[0m - \u001b[1m  Top 5 features: ['M17_P11_log_mul_std_252_252', 'M17_P10_log_mul_std_90_252', 'M2_P8_sigmoid_diff_std_252_90', 'M8_risk_free_rate_log_ratio_std_252_90', 'E8_P2_poly3_std_90_90']\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalidation's adj_sharpe: 0.390929\n",
      "[20]\tvalidation's adj_sharpe: 0.357827\n",
      "[30]\tvalidation's adj_sharpe: 0.335994\n",
      "[40]\tvalidation's adj_sharpe: 0.352491\n",
      "[50]\tvalidation's adj_sharpe: 0.350694\n",
      "[60]\tvalidation's adj_sharpe: 0.340364\n",
      "[70]\tvalidation's adj_sharpe: 0.350884\n",
      "[80]\tvalidation's adj_sharpe: 0.333479\n",
      "[90]\tvalidation's adj_sharpe: 0.314479\n",
      "[100]\tvalidation's adj_sharpe: 0.321898\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1]\tvalidation's adj_sharpe: 0.440672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-16 00:09:08.821\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_cv\u001b[0m:\u001b[36m489\u001b[0m - \u001b[1m  Computing SHAP values for fold 4...\u001b[0m\n",
      "\u001b[32m2025-11-16 00:09:08.981\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_cv\u001b[0m:\u001b[36m493\u001b[0m - \u001b[1m  Top 5 features: ['P8_S4_atan_ratio_std_252_252', 'E19_M16_atan_ratio_std_252_10', 'V13_V5_sigmoid_diff_std_252_90', 'E3_M7_exp_diff_std_252_90', 'P10_S7_sigmoid_diff_std_252_252']\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[10]\tvalidation's adj_sharpe: 0.47694\n",
      "[20]\tvalidation's adj_sharpe: 0.552748\n",
      "[30]\tvalidation's adj_sharpe: 0.551818\n",
      "[40]\tvalidation's adj_sharpe: 0.559648\n",
      "[50]\tvalidation's adj_sharpe: 0.560258\n",
      "[60]\tvalidation's adj_sharpe: 0.540424\n",
      "[70]\tvalidation's adj_sharpe: 0.564244\n",
      "[80]\tvalidation's adj_sharpe: 0.563864\n",
      "[90]\tvalidation's adj_sharpe: 0.555988\n",
      "[100]\tvalidation's adj_sharpe: 0.544453\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[27]\tvalidation's adj_sharpe: 0.589649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-16 00:09:21.447\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_cv\u001b[0m:\u001b[36m489\u001b[0m - \u001b[1m  Computing SHAP values for fold 5...\u001b[0m\n",
      "\u001b[32m2025-11-16 00:09:21.561\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_cv\u001b[0m:\u001b[36m493\u001b[0m - \u001b[1m  Top 5 features: ['P10_S9_atan_ratio_std_252_252', 'M16_P11_log_mul_std_252_252', 'M14_P2_sigmoid_diff_std_90_252', 'P2_P4_sigmoid_diff_std_252_90', 'E19_E9_log_mul_std_252_252']\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "lgb_trainer = LightGBMTrainer()\n",
    "lgb_res = lgb_trainer.train_cv(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    risk_free_rate=risk_free_rate,\n",
    "    forward_returns=forward_returns,\n",
    "    batch_size=CONFIG.BATCH_SIZE,\n",
    "    NUM_BOOST_ROUND=CONFIG.NUM_BOOST_ROUND,\n",
    "    compute_shap=True,\n",
    "    feature_names=feature_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3851009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_res.write_csv(\"/mnt/c/Users/Admin/Desktop/Personal-Projects/Kaggle/Hull Tactical - Market Prediction/LGBM_feature_importance.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.06",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
