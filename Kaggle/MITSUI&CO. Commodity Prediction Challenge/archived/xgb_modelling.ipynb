{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ff628d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "import xgboost as xgb\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "from CONFIG import CONFIG\n",
    "from PREPROCESSOR_V2 import PREPROCESSOR\n",
    "from FEATURE_ENGINEERING_V2 import FEATURE_ENGINEERING\n",
    "import time\n",
    "\n",
    "\n",
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        print(f\"{func.__name__} took {end - start:.4f} seconds\")\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d86c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"xgb_models/v1\"\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "with open(f\"{folder}/features.json\", \"w\") as f:\n",
    "    json.dump(CONFIG.IMPT_COL, f)\n",
    "\n",
    "with open(f\"{folder}/features.json\", \"r\") as f:\n",
    "    json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5613ca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_correlation_sharpe(targets, predictions) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the rank correlation between predictions and target values,\n",
    "    and returns its Sharpe ratio (mean / standard deviation).\n",
    "\n",
    "    :param merged_df: DataFrame containing prediction columns (starting with 'prediction_')\n",
    "                    and target columns (starting with 'target_')\n",
    "    :return: Sharpe ratio of the rank correlation\n",
    "    :raises ZeroDivisionError: If the standard deviation is zero\n",
    "    \"\"\"\n",
    "    correlations = []\n",
    "    targets = targets.reshape(-1, CONFIG.NUM_TARGET_COLUMNS)\n",
    "    targets = np.where(targets == 0, np.nan, targets)\n",
    "    for i, (pred_row, target_row) in enumerate(zip(predictions, targets)):\n",
    "        # Find valid (non-NaN) assets for this timestep\n",
    "\n",
    "        valid_mask = ~np.isnan(target_row)\n",
    "        valid_pred = pred_row[valid_mask]\n",
    "        valid_target = target_row[valid_mask]\n",
    "\n",
    "        if np.std(pred_row) == 0 or np.std(target_row) == 0:\n",
    "            raise ZeroDivisionError(\"Zero standard deviation in a row.\")\n",
    "\n",
    "        rho = np.corrcoef(rankdata(valid_pred, method=\"average\"), rankdata(valid_target, method=\"average\"))[0, 1]\n",
    "        correlations.append(rho)\n",
    "\n",
    "    daily_rank_corrs = np.array(correlations)\n",
    "    std_dev = daily_rank_corrs.std(ddof=0)\n",
    "    if std_dev == 0:\n",
    "        raise ZeroDivisionError(\"Denominator is zero, unable to compute Sharpe ratio.\")\n",
    "\n",
    "    sharpe_ratio = daily_rank_corrs.mean() / std_dev\n",
    "    return -float(sharpe_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48b1ccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelector:\n",
    "    def __init__(self, train_x, train_y, n_targets: int = 424):\n",
    "        self.train_x = train_x  # drop date\n",
    "        self.train_y = train_y\n",
    "        self.n_targets = n_targets\n",
    "        self.keep_features = None\n",
    "        self.total_features = train_x.columns.__len__()\n",
    "\n",
    "    @timer\n",
    "    def basic_filters(self):\n",
    "        train_x_filter_1 = self.train_x.select([col for col in self.train_x.columns if self.train_x[col].var() > 1e-3])\n",
    "        train_x_filter_2 = train_x_filter_1.select(\n",
    "            [col for col in train_x_filter_1.columns if train_x_filter_1[col].value_counts()[\"count\"].max() / len(train_x_filter_1) < 0.80]\n",
    "        )\n",
    "        train_x_filter_3 = train_x_filter_2.select([col for col in train_x_filter_2.columns if train_x_filter_2[col].n_unique() > 2])\n",
    "\n",
    "        print(f\"After Basic Filter: {train_x_filter_3.columns.__len__()} / {self.total_features}\")\n",
    "\n",
    "        return train_x_filter_3\n",
    "\n",
    "    @timer\n",
    "    def run_correlation(self, x: torch.Tensor, y: torch.Tensor, names: list) -> list:\n",
    "        \"\"\"\n",
    "        Memory-optimized version using chunked processing\n",
    "        Best for when you have enough GPU memory\n",
    "\n",
    "        Args:\n",
    "            x: Tensor\n",
    "            y: Tensor\n",
    "\n",
    "        Returns:\n",
    "            correlations: Tensor\n",
    "        \"\"\"\n",
    "        N, D1 = x.shape\n",
    "        N2, D2 = y.shape\n",
    "        assert N == N2\n",
    "\n",
    "        device = x.device\n",
    "\n",
    "        # Handle NaNs by masking\n",
    "        x_valid = ~torch.isnan(x)\n",
    "        y_valid = ~torch.isnan(y)\n",
    "\n",
    "        # Convert NaNs to 0 for computation\n",
    "        x_clean = torch.where(x_valid, x, 0.0)\n",
    "        y_clean = torch.where(y_valid, y, 0.0)\n",
    "\n",
    "        # Compute valid sample counts for each pair efficiently\n",
    "        # This is the memory bottleneck, so we chunk it\n",
    "        chunk_size = 500  # Adjust based on GPU memory\n",
    "        correlations = torch.zeros(D1, D2, device=device)\n",
    "\n",
    "        for i in range(0, D1, chunk_size):\n",
    "            end_i = min(i + chunk_size, D1)\n",
    "\n",
    "            # Get chunk\n",
    "            x_chunk = x_clean[:, i:end_i]  # (N, chunk_size)\n",
    "            x_valid_chunk = x_valid[:, i:end_i]  # (N, chunk_size)\n",
    "\n",
    "            # Compute valid sample matrix for this chunk\n",
    "            valid_matrix = x_valid_chunk.unsqueeze(2) & y_valid.unsqueeze(1)  # (N, chunk_size, D2)\n",
    "            n_valid = valid_matrix.sum(dim=0).float()  # (chunk_size, D2)\n",
    "\n",
    "            # Sufficient samples mask\n",
    "            sufficient = n_valid >= 10\n",
    "\n",
    "            if sufficient.any():\n",
    "                # Compute means over valid samples\n",
    "                x_sum = (x_chunk.unsqueeze(2) * valid_matrix).sum(dim=0)  # (chunk_size, D2)\n",
    "                y_sum = (y_clean.unsqueeze(1) * valid_matrix).sum(dim=0)  # (chunk_size, D2)\n",
    "\n",
    "                x_mean = x_sum / (n_valid + 1e-10)\n",
    "                y_mean = y_sum / (n_valid + 1e-10)\n",
    "\n",
    "                # Center data\n",
    "                x_centered = (x_chunk.unsqueeze(2) - x_mean.unsqueeze(0)) * valid_matrix  # (N, chunk_size, D2)\n",
    "                y_centered = (y_clean.unsqueeze(1) - y_mean.unsqueeze(0)) * valid_matrix  # (N, chunk_size, D2)\n",
    "\n",
    "                # Compute correlation\n",
    "                numerator = (x_centered * y_centered).sum(dim=0)\n",
    "                x_var = (x_centered**2).sum(dim=0)\n",
    "                y_var = (y_centered**2).sum(dim=0)\n",
    "\n",
    "                denominator = torch.sqrt(x_var * y_var) + 1e-10\n",
    "                chunk_corr = numerator / denominator\n",
    "\n",
    "                # Apply sufficient samples mask\n",
    "                chunk_corr = torch.where(sufficient, chunk_corr, 0.0)\n",
    "                correlations[i:end_i] = torch.abs(chunk_corr)\n",
    "\n",
    "        self.correlations = correlations.cpu().numpy()\n",
    "\n",
    "    def run_selection(self):\n",
    "        filtered = self.basic_filters()\n",
    "        train_x_arr = filtered.drop(CONFIG.DATE_COL).to_numpy()\n",
    "        train_y_arr = self.train_y.drop(CONFIG.DATE_COL).to_numpy()\n",
    "        self.run_correlation(\n",
    "            x=torch.tensor(train_x_arr, device=\"cuda\"), y=torch.tensor(train_y_arr, device=\"cuda\"), names=filtered.drop(CONFIG.DATE_COL).columns\n",
    "        )\n",
    "\n",
    "        return self.correlations, filtered.drop(CONFIG.DATE_COL).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0b5484a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_compute_lag_returns took 0.0130 seconds\n",
      "_compute_autocorr_torch took 0.3945 seconds\n",
      "_compute_obv took 0.0085 seconds\n",
      "_compute_return_skew took 0.0030 seconds\n",
      "_compute_volume_z took 0.0382 seconds\n",
      "_compute_market_stats took 0.1398 seconds\n",
      "_compute_atr took 0.0081 seconds\n",
      "_compute_rolling took 0.0134 seconds\n",
      "create_market_features took 2.2632 seconds\n",
      "_compute_lag_returns took 0.0095 seconds\n",
      "_compute_market_stats took 0.0131 seconds\n",
      "_compute_return_skew took 0.0030 seconds\n",
      "_compute_autocorr_torch took 0.0268 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_47592\\1075755042.py:66: RuntimeWarning: Mean of empty slice\n",
      "  (retrain_y_arr - np.nanmean(retrain_y_arr, axis=1).reshape(retrain_y_arr.shape[0], -1))\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\edmund\\Lib\\site-packages\\numpy\\lib\\nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    }
   ],
   "source": [
    "# --- Prepare DataLoader ---\n",
    "# Create the dataset\n",
    "\n",
    "train_x = pl.scan_csv(CONFIG.TRAIN_X_PATH)\n",
    "train_x = PREPROCESSOR(df=train_x)\n",
    "train_x = train_x.clean()\n",
    "\n",
    "features = FEATURE_ENGINEERING(df=train_x)\n",
    "train_x: pl.DataFrame = features.create_market_features()\n",
    "\n",
    "train_y = pl.scan_csv(CONFIG.TRAIN_Y_PATH)\n",
    "\n",
    "curr_y = (\n",
    "    train_y.with_columns([pl.col(CONFIG.LAGS[f\"lag{i}\"]).exclude(CONFIG.DATE_COL).shift(i + 1) for i in range(1, 5)])\n",
    "    .with_columns(pl.all().exclude(CONFIG.DATE_COL).shift())\n",
    "    .filter((pl.col(CONFIG.DATE_COL).is_in(train_x.select(CONFIG.DATE_COL).to_series())))\n",
    "    .collect()\n",
    "    .fill_null(0)\n",
    "    .lazy()\n",
    ")\n",
    "\n",
    "y_feat = FEATURE_ENGINEERING(df=curr_y)\n",
    "lags = y_feat._compute_lag_returns(df=curr_y)\n",
    "market = y_feat._compute_market_stats(df=curr_y)\n",
    "skew = y_feat._compute_return_skew(df=curr_y)\n",
    "auto_corr = y_feat._compute_autocorr_torch(df=curr_y)\n",
    "\n",
    "train_x = (\n",
    "    train_x.join(curr_y.collect(), on=CONFIG.DATE_COL)\n",
    "    .join(lags.collect(), on=CONFIG.DATE_COL)\n",
    "    .join(market.collect(), on=CONFIG.DATE_COL)\n",
    "    .join(skew.collect(), on=CONFIG.DATE_COL)\n",
    "    .join(auto_corr.collect(), on=CONFIG.DATE_COL)\n",
    ")\n",
    "\n",
    "\n",
    "train_y = train_y.filter((pl.col(CONFIG.DATE_COL).is_in(train_x.select(CONFIG.DATE_COL).to_series()))).collect()\n",
    "train_x = (\n",
    "    train_x.with_columns([pl.when(pl.col(col).is_infinite()).then(0.0).otherwise(pl.col(col)).alias(col) for col in train_x.columns])\n",
    "    .with_columns(pl.all().shrink_dtype())\n",
    "    .filter(pl.col(CONFIG.DATE_COL).is_in(train_y.select(CONFIG.DATE_COL).to_series()))\n",
    "    .with_columns(pl.col(CONFIG.DATE_COL).cast(pl.Int64))\n",
    "    .select([CONFIG.DATE_COL] + CONFIG.IMPT_COL)\n",
    ")\n",
    "\n",
    "retrain_x = train_x.with_columns(pl.all().exclude(CONFIG.DATE_COL).shift(5))\n",
    "retrain_y = train_y.filter((pl.col(CONFIG.DATE_COL).is_in(train_x.select(CONFIG.DATE_COL).to_series()))).with_columns(\n",
    "    pl.all().exclude(CONFIG.DATE_COL).shift(5)\n",
    ")\n",
    "\n",
    "train_y_arr = train_y.drop(CONFIG.DATE_COL).to_numpy()\n",
    "\n",
    "train_y = (\n",
    "    pl.DataFrame(\n",
    "        (train_y_arr - np.nanmean(train_y_arr, axis=1).reshape(train_y_arr.shape[0], -1))\n",
    "        / np.nanstd(train_y_arr, axis=1).reshape(train_y_arr.shape[0], -1),\n",
    "        schema=train_y.drop(CONFIG.DATE_COL).columns,\n",
    "    )\n",
    "    .insert_column(0, train_y.select(CONFIG.DATE_COL).to_series())\n",
    "    .fill_nan(0)\n",
    ")\n",
    "\n",
    "retrain_y_arr = retrain_y.drop(CONFIG.DATE_COL).to_numpy()\n",
    "retrain_y = (\n",
    "    pl.DataFrame(\n",
    "        (retrain_y_arr - np.nanmean(retrain_y_arr, axis=1).reshape(retrain_y_arr.shape[0], -1))\n",
    "        / np.nanstd(retrain_y_arr, axis=1).reshape(retrain_y_arr.shape[0], -1),\n",
    "        schema=train_y.drop(CONFIG.DATE_COL).columns,\n",
    "    )\n",
    "    .insert_column(0, train_y.select(CONFIG.DATE_COL).to_series())\n",
    "    .fill_nan(0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b53d72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    # \"objective\": RankCorrelationLoss().custom_loss,\n",
    "    \"eval_metric\": rank_correlation_sharpe,\n",
    "    \"max_depth\": 16,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"n_estimators\": 500,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"reg_alpha\": 0.1,\n",
    "    \"reg_lambda\": 1.0,\n",
    "    \"random_state\": CONFIG.RANDOM_STATE,\n",
    "    \"device\": \"gpu\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"early_stopping_rounds\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34a7dd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dates from 2 to 1826\n",
      "Valid dates from 1827 to 1916\n",
      "[0]\tvalidation_0-rmse:0.94374\tvalidation_0-rank_correlation_sharpe:-0.06801\n",
      "[1]\tvalidation_0-rmse:0.94410\tvalidation_0-rank_correlation_sharpe:-0.20558\n",
      "[2]\tvalidation_0-rmse:0.94453\tvalidation_0-rank_correlation_sharpe:-0.23522\n",
      "[3]\tvalidation_0-rmse:0.94558\tvalidation_0-rank_correlation_sharpe:-0.21142\n",
      "[4]\tvalidation_0-rmse:0.94611\tvalidation_0-rank_correlation_sharpe:-0.20633\n",
      "[5]\tvalidation_0-rmse:0.94655\tvalidation_0-rank_correlation_sharpe:-0.21293\n",
      "[6]\tvalidation_0-rmse:0.94660\tvalidation_0-rank_correlation_sharpe:-0.25092\n",
      "[7]\tvalidation_0-rmse:0.94710\tvalidation_0-rank_correlation_sharpe:-0.25607\n",
      "[8]\tvalidation_0-rmse:0.94735\tvalidation_0-rank_correlation_sharpe:-0.26706\n",
      "[9]\tvalidation_0-rmse:0.94838\tvalidation_0-rank_correlation_sharpe:-0.24774\n",
      "[10]\tvalidation_0-rmse:0.94879\tvalidation_0-rank_correlation_sharpe:-0.24301\n",
      "[11]\tvalidation_0-rmse:0.94923\tvalidation_0-rank_correlation_sharpe:-0.23637\n",
      "[12]\tvalidation_0-rmse:0.94993\tvalidation_0-rank_correlation_sharpe:-0.22834\n",
      "[13]\tvalidation_0-rmse:0.95049\tvalidation_0-rank_correlation_sharpe:-0.24421\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "dates_unique = train_x.filter(pl.col(CONFIG.DATE_COL) <= CONFIG.MAX_TRAIN_DATE).select(pl.col(CONFIG.DATE_COL).unique().sort()).to_series().to_numpy()\n",
    "real_dates_unique = (\n",
    "    train_x.filter(pl.col(CONFIG.DATE_COL) > CONFIG.MAX_TRAIN_DATE).select(pl.col(CONFIG.DATE_COL).unique().sort()).to_series().to_numpy()\n",
    ")\n",
    "\n",
    "if CONFIG.VERBOSE:\n",
    "    print(f\"Train dates from {dates_unique.min()} to {dates_unique.max()}\")\n",
    "    print(f\"Valid dates from {real_dates_unique.min()} to {real_dates_unique.max()}\")\n",
    "\n",
    "dates_train = dates_unique\n",
    "dates_valid = real_dates_unique\n",
    "\n",
    "df_train = train_x.filter(pl.col(CONFIG.DATE_COL).is_in(dates_train)).drop(CONFIG.DATE_COL)\n",
    "true_y = train_y.filter(pl.col(CONFIG.DATE_COL).is_in(dates_train)).drop(CONFIG.DATE_COL)\n",
    "\n",
    "valid_period = range(min(dates_valid), max(dates_valid) + 1)\n",
    "df_valid = train_x.filter(pl.col(CONFIG.DATE_COL).is_in(valid_period)).drop(CONFIG.DATE_COL)\n",
    "df_valid_current_y = train_y.filter(pl.col(CONFIG.DATE_COL).is_in(valid_period)).drop(CONFIG.DATE_COL)\n",
    "\n",
    "df_valid_retrain = retrain_x.filter(pl.col(CONFIG.DATE_COL).is_in(valid_period)).drop(CONFIG.DATE_COL)\n",
    "df_valid_current_y_retrain = retrain_y.filter(pl.col(CONFIG.DATE_COL).is_in(valid_period)).drop(CONFIG.DATE_COL)\n",
    "\n",
    "model = xgb.XGBRegressor(**params)\n",
    "# Train with custom objective\n",
    "model.fit(\n",
    "    df_train,\n",
    "    true_y,\n",
    "    eval_set=[(df_valid, df_valid_current_y)],\n",
    ")\n",
    "model.save_model(f\"{folder}/xgb_full.json\")\n",
    "print(\"model saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fed41c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e128e06aa524cd7a10daee65444c78b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAL Sharpe: 0.18294\n"
     ]
    }
   ],
   "source": [
    "retrain_params = {\n",
    "    # \"objective\": RankCorrelationLoss().custom_loss,\n",
    "    \"eval_metric\": rank_correlation_sharpe,\n",
    "    \"max_depth\": 16,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"n_estimators\": 5,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"reg_alpha\": 0.1,\n",
    "    \"reg_lambda\": 1.0,\n",
    "    \"random_state\": CONFIG.RANDOM_STATE,\n",
    "    \"device\": \"gpu\",\n",
    "    \"tree_method\": \"hist\",\n",
    "}\n",
    "\n",
    "retrain_model = xgb.XGBRegressor(**retrain_params)\n",
    "retrain_model.load_model(f\"{folder}/xgb_full.json\")\n",
    "preds = []\n",
    "cnt_dates = 0\n",
    "for date_id in tqdm(real_dates_unique):\n",
    "    period = range(date_id, date_id + 1)\n",
    "\n",
    "    df_valid_date = train_x.filter(pl.col(CONFIG.DATE_COL).is_in(period)).drop(CONFIG.DATE_COL)\n",
    "\n",
    "    df_upd = retrain_x.filter(pl.col(CONFIG.DATE_COL).is_in(date_id)).drop(CONFIG.DATE_COL)\n",
    "    df_upd_current_y = retrain_y.filter(pl.col(CONFIG.DATE_COL).is_in(date_id)).drop(CONFIG.DATE_COL)\n",
    "\n",
    "    if len(df_upd) > 0:\n",
    "        retrain_model.fit(df_upd, df_upd_current_y)\n",
    "\n",
    "    preds_i = retrain_model.predict(df_valid_date)\n",
    "\n",
    "    preds += list(preds_i[-1].reshape(-1, CONFIG.NUM_TARGET_COLUMNS))\n",
    "\n",
    "    cnt_dates += 1\n",
    "\n",
    "preds = np.array(preds)\n",
    "\n",
    "score = rank_correlation_sharpe(\n",
    "    train_y.filter(pl.col(CONFIG.DATE_COL).is_in(real_dates_unique)).drop(CONFIG.DATE_COL).to_numpy().astype(np.float64),\n",
    "    preds,\n",
    ")\n",
    "print(f\"REAL Sharpe: {score:.5f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edmund",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
