{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09d651b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional\n",
    "import copy\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "from abc import ABC, abstractmethod\n",
    "import math\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from numba import njit, prange\n",
    "from scipy.stats import spearmanr, rankdata\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, KFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.profiler import profile, ProfilerActivity, record_function\n",
    "\n",
    "from CONFIG import CONFIG\n",
    "from PREPROCESSOR_V2 import PREPROCESSOR\n",
    "from FEATURE_ENGINEERING_V2 import FEATURE_ENGINEERING\n",
    "from SEQUENTIAL_NN_MODEL import CNNTransformerModel, GRUModel, LSTMModel, PureTransformerModel\n",
    "from CROSS_SECTIONAL_NN_MODEL import DeepMLPModel, LinearModel, ResidualMLPModel\n",
    "from LOSS import CombinedICIRLoss\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        print(f\"{func.__name__} took {end - start:.4f} seconds\")\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bc2efde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Prepare DataLoader ---\n",
    "# # Create the dataset\n",
    "\n",
    "# train_x = pl.scan_csv(CONFIG.TRAIN_X_PATH).filter(pl.col(\"date_id\") <= CONFIG.MAX_TRAIN_DATE)\n",
    "# train_y = pl.scan_csv(CONFIG.TRAIN_Y_PATH).filter(pl.col(\"date_id\") <= CONFIG.MAX_TRAIN_DATE).fill_null(0).collect()\n",
    "\n",
    "# train_x = PREPROCESSOR(df=train_x)\n",
    "# train_x.clean()\n",
    "# train_x = train_x.transform().lazy()\n",
    "\n",
    "# train_x = FEATURE_ENGINEERING(df=train_x)\n",
    "# train_x = train_x.create_all_features().collect().pivot(index=CONFIG.DATE_COL, on=[\"type\", \"instr\"])\n",
    "# train_x = train_x.rename({col: re.sub(r'[{\",}]', \"\", col).replace(\" \", \"_\").replace(\",\", \"_\") for col in train_x.columns})\n",
    "# train_x = train_x.select(set(CONFIG.IMPT_COLS + [CONFIG.DATE_COL]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71253111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_correlation_sharpe(targets, predictions) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the rank correlation between predictions and target values,\n",
    "    and returns its Sharpe ratio (mean / standard deviation).\n",
    "\n",
    "    :param merged_df: DataFrame containing prediction columns (starting with 'prediction_')\n",
    "                      and target columns (starting with 'target_')\n",
    "    :return: Sharpe ratio of the rank correlation\n",
    "    :raises ZeroDivisionError: If the standard deviation is zero\n",
    "    \"\"\"\n",
    "    correlations = []\n",
    "\n",
    "    for i, (pred_row, target_row) in enumerate(zip(predictions, targets)):\n",
    "        # Find valid (non-NaN) assets for this timestep\n",
    "        valid_mask = ~np.isnan(target_row)\n",
    "        valid_pred = pred_row[valid_mask]\n",
    "        valid_target = target_row[valid_mask]\n",
    "\n",
    "        if np.std(pred_row) == 0 or np.std(target_row) == 0:\n",
    "            raise ZeroDivisionError(\"Zero standard deviation in a row.\")\n",
    "\n",
    "        rho = np.corrcoef(rankdata(valid_pred, method=\"average\"), rankdata(valid_target, method=\"average\"))[0, 1]\n",
    "        correlations.append(rho)\n",
    "\n",
    "    daily_rank_corrs = np.array(correlations)\n",
    "    std_dev = daily_rank_corrs.std(ddof=0)\n",
    "    if std_dev == 0:\n",
    "        raise ZeroDivisionError(\"Denominator is zero, unable to compute Sharpe ratio.\")\n",
    "\n",
    "    sharpe_ratio = daily_rank_corrs.mean() / std_dev\n",
    "    return float(sharpe_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d6675fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseFinancialDataset(Dataset, ABC):\n",
    "    \"\"\"Base class for financial datasets\"\"\"\n",
    "\n",
    "    def __init__(self, X: pl.DataFrame, y: pl.DataFrame, date_column: str = CONFIG.DATE_COL):\n",
    "        \"\"\"\n",
    "        Base initialization\n",
    "\n",
    "        Args:\n",
    "            data: Preprocessed DataFrame (scaling already done)\n",
    "            target_columns: List of target column names (424 targets)\n",
    "            feature_columns: List of feature column names\n",
    "            date_column: Name of date identifier column\n",
    "        \"\"\"\n",
    "        self.X = X.clone()\n",
    "        self.y = y.clone()\n",
    "        self.date_column = date_column\n",
    "\n",
    "        # Sort by date\n",
    "        self.X = self.X.sort(by=CONFIG.DATE_COL)\n",
    "        self.y = self.y.sort(by=CONFIG.DATE_COL)\n",
    "        self.unique_dates = sorted(self.X[self.date_column].unique())\n",
    "        self.device = torch.device(\"cpu\")\n",
    "\n",
    "        self._prepare_samples()\n",
    "\n",
    "    def _prepare_samples(self):\n",
    "        self.X = self.X.drop(CONFIG.DATE_COL).to_numpy()\n",
    "\n",
    "        self.num_features = self.X.shape[-1]\n",
    "        # Split continuous and categorical features\n",
    "        self.dates = torch.tensor(self.unique_dates, dtype=torch.int16)\n",
    "        self.continuous_data = torch.tensor(self.X, dtype=torch.float32)\n",
    "        self.continuous_data = torch.nan_to_num(self.continuous_data, 0)\n",
    "\n",
    "        if torch.isnan(self.continuous_data).any() or torch.isinf(self.continuous_data).any():\n",
    "            print(\"Input contains NaN or Inf values!\")\n",
    "\n",
    "        self.y = torch.tensor(self.y.drop(CONFIG.DATE_COL).to_numpy(), dtype=torch.float32)\n",
    "\n",
    "        self.unique_date, self.inverse_indices, self.counts = torch.unique(self.dates, return_inverse=True, return_counts=True)\n",
    "\n",
    "        self.n_unique_dates = len(self.unique_date)\n",
    "\n",
    "    @abstractmethod\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get item - implemented by subclasses\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed86feb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialDataset(BaseFinancialDataset):\n",
    "    \"\"\"Dataset for sequential models (LSTM, Transformers, CNN)\"\"\"\n",
    "\n",
    "    def __init__(self, X: pl.DataFrame, Y: pl.DataFrame, date_column: str = CONFIG.DATE_COL, prediction_horizon: int = 1):\n",
    "        \"\"\"\n",
    "        Sequential dataset for temporal models\n",
    "\n",
    "        Args:\n",
    "            data: Preprocessed DataFrame\n",
    "            target_columns: Target column names\n",
    "            feature_columns: Feature column names\n",
    "            date_column: Date identifier column\n",
    "            sequence_length: Number of time steps in sequence\n",
    "            prediction_horizon: Steps ahead to predict (usually 1)\n",
    "        \"\"\"\n",
    "        self.sequence_length = CONFIG.SEQ_LEN\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "\n",
    "        super().__init__(X, Y)\n",
    "\n",
    "        self._generate_sequence()\n",
    "\n",
    "    def _generate_sequence(self):\n",
    "        self.sequence_x = []\n",
    "        self.sequence_y = []\n",
    "\n",
    "        for date in range(self.sequence_length, self.n_unique_dates):\n",
    "            self.sequence_x.append(self.continuous_data[date - self.sequence_length : date])\n",
    "            self.sequence_y.append(self.y[date - 1])\n",
    "        self.sequence_x = torch.stack(self.sequence_x)\n",
    "        self.sequence_y = torch.stack(self.sequence_y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_unique_dates - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get sequence, target, and date_id\"\"\"\n",
    "        continuous_seq = self.sequence_x[idx]  # (seq_len, N_FEATURES)\n",
    "        target = self.sequence_y[idx]  # (424,)\n",
    "\n",
    "        return (\n",
    "            continuous_seq,\n",
    "            target,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee180e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossSectionalDataset(BaseFinancialDataset):\n",
    "    \"\"\"Dataset for cross-sectional models (MLP, XGBoost, Linear)\"\"\"\n",
    "\n",
    "    def __init__(self, X: pl.DataFrame, Y: pl.DataFrame, date_column: str = CONFIG.DATE_COL):\n",
    "        \"\"\"\n",
    "        Cross-sectional dataset for non-temporal models\n",
    "\n",
    "        Args:\n",
    "            data: Preprocessed DataFrame\n",
    "            target_columns: Target column names\n",
    "            feature_columns: Feature column names\n",
    "            date_column: Date identifier column\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(X, Y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_unique_dates - CONFIG.SEQ_LEN\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get features, target, and date_id\"\"\"\n",
    "        features = self.continuous_data[idx].unsqueeze(0)  # (n_features,)\n",
    "        target = self.y[idx]  # (n_targets,)\n",
    "\n",
    "        return (\n",
    "            features,\n",
    "            target,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82fe237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_collate_fn(batch: list) -> dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Collate function for DataLoader to flatten the batch.\n",
    "\n",
    "    Args:\n",
    "        batch (list): List of tuples containing tensors.\n",
    "\n",
    "        tuple[torch.Tensor]: Flattened tensors (type, instr, X, y).\n",
    "    \"\"\"\n",
    "    X, curr_y = zip(*batch)\n",
    "    continuous_batch = torch.stack(X)\n",
    "    curr_y = torch.stack(curr_y)\n",
    "\n",
    "    continuous_batch = torch.nan_to_num(continuous_batch)\n",
    "\n",
    "    return {\"continuous\": continuous_batch, \"current\": curr_y}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad522920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_val_dataset = SequentialDataset(df_valid, df_valid_current_y, df_valid_true_delta)\n",
    "# seq_val_dataloader = DataLoader(\n",
    "#     seq_val_dataset,\n",
    "#     batch_size=1,\n",
    "#     shuffle=False,\n",
    "#     collate_fn=flatten_collate_fn,\n",
    "#     pin_memory=True,\n",
    "#     # num_workers=6,\n",
    "#     # persistent_workers=True,\n",
    "#     # prefetch_factor=2,\n",
    "#     drop_last=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf1a5a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENSEMBLE_NN(nn.Module):\n",
    "    \"\"\"Ensemble of multiple architectures\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # seq models\n",
    "        self.cnn_transformer = CNNTransformerModel(input_dim, hidden_dim, output_dim, CONFIG.SEQ_LEN)\n",
    "        self.gru_model = GRUModel(input_dim, hidden_dim, output_dim)\n",
    "        self.lstm_model = LSTMModel(input_dim, hidden_dim, output_dim)\n",
    "        self.pure_transformer = PureTransformerModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "        # cross sectional models\n",
    "        # self.mlp = DeepMLPModel(input_dim, [64, 32], output_dim)\n",
    "        # self.linear = LinearModel(input_dim, output_dim)\n",
    "        # self.residual = ResidualMLPModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "        # Ensemble weights (learnable)\n",
    "        self.ensemble_weights = nn.Parameter(torch.ones(4) / 4)\n",
    "\n",
    "        self.ensemble_dropout = nn.Dropout(0.1)\n",
    "        self.prediction_dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_seq,\n",
    "    ):\n",
    "        # Get predictions from all models\n",
    "        x_cs = x_seq[:, -1, :]\n",
    "        out1 = self.cnn_transformer(x_seq)\n",
    "        out2 = self.gru_model(x_seq)\n",
    "        out3 = self.lstm_model(x_seq)\n",
    "        out4 = self.pure_transformer(x_seq)\n",
    "        # out5 = self.mlp(x_cs)\n",
    "        # out6 = self.linear(x_cs)\n",
    "        # out7 = self.residual(x_cs)\n",
    "\n",
    "        individual_outputs = [out1, out2, out3, out4]  # out5, out6, out7\n",
    "        individual_outputs = [self.prediction_dropout(out) for out in individual_outputs]\n",
    "\n",
    "        dropped_weights = self.ensemble_dropout(self.ensemble_weights)\n",
    "        weights = F.softmax(dropped_weights, dim=0)\n",
    "\n",
    "        ensemble_output = torch.zeros_like(individual_outputs[0])  # (batch_size, 424)\n",
    "        for w, out in zip(weights, individual_outputs):\n",
    "            ensemble_output += w * out\n",
    "\n",
    "        return ensemble_output\n",
    "        # return out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ab79cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        lr: float = 0.001,\n",
    "        batch_size: int = 1,\n",
    "        epochs: int = 100,\n",
    "        early_stopping_patience: int = 10,\n",
    "        early_stopping: bool = True,\n",
    "        lr_patience: int = 2,\n",
    "        lr_factor: float = 0.5,\n",
    "        lr_refit: float = 0.001,\n",
    "        random_seed: int = CONFIG.RANDOM_STATE,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.early_stopping = early_stopping\n",
    "        self.lr_patience = lr_patience\n",
    "        self.lr_factor = lr_factor\n",
    "        self.lr_refit = lr_refit\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        self.criterion = CombinedICIRLoss()\n",
    "\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = model.to(self.device)\n",
    "        self.optimizer = None\n",
    "        self.refit_optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr_refit, weight_decay=0.01)\n",
    "        self.best_epoch = None\n",
    "        self.features = None\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def fit(self, train_set: tuple, val_set: tuple, retrain_set: tuple, verbose: bool = False) -> None:\n",
    "        \"\"\"Fit the model on the training set and validate on the validation set.\n",
    "\n",
    "        Args:\n",
    "            train_set (tuple): A tuple containing input data, targets for training.\n",
    "            val_set (tuple): A tuple containing input data, targets for validation.\n",
    "            verbose (bool, optional): If True, prints training progress. Defaults to False.\n",
    "        \"\"\"\n",
    "        torch.manual_seed(self.random_seed)\n",
    "\n",
    "        seq_train_dataset = SequentialDataset(*train_set)\n",
    "        seq_train_dataloader = DataLoader(\n",
    "            seq_train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=flatten_collate_fn,\n",
    "            pin_memory=True,\n",
    "            # num_workers=2,\n",
    "            # persistent_workers=True,\n",
    "            # prefetch_factor=2,\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "        seq_val_dataset = SequentialDataset(*val_set)\n",
    "        seq_val_dataloader = DataLoader(\n",
    "            seq_val_dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            collate_fn=flatten_collate_fn,\n",
    "            pin_memory=True,\n",
    "            # num_workers=6,\n",
    "            # persistent_workers=True,\n",
    "            # prefetch_factor=2,\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "        retrain_val_dataset = SequentialDataset(*retrain_set)\n",
    "        retrain_val_dataloader = DataLoader(\n",
    "            retrain_val_dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            collate_fn=flatten_collate_fn,\n",
    "            pin_memory=True,\n",
    "            # num_workers=6,\n",
    "            # persistent_workers=True,\n",
    "            # prefetch_factor=2,\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=0.01)\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optimizer, T_0=5, T_mult=2, eta_min=1e-6)\n",
    "\n",
    "        train_sharpes, val_sharpes = [], []\n",
    "        if verbose:\n",
    "            print(f\"Device: {self.device}\")\n",
    "            print(\n",
    "                f\"{'Epoch':^5} | {'Train Loss':^10} | {'Train ICIR Loss':^15} | {'Train MSE Loss':^14} | {'Train Ranking Loss':^17} | {'Val Loss':^8} | {'Val ICIR Loss':^13} | {'Val MSE Loss':^12} | {'Val Ranking Loss':^16} | {'Train sharpe':^9} | {'Val sharpe':^7} | {'LR':^7}\"\n",
    "            )\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "        min_val_sharpe = -np.inf\n",
    "        best_epoch = 0\n",
    "        no_improvement = 0\n",
    "        best_model = None\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss, train_sharpe, train_icir_loss, train_mse_loss, train_ranking_loss = self.train_one_epoch(seq_train_dataloader, verbose)\n",
    "            val_loss, val_sharpe, val_icir_loss, val_mse_loss, val_ranking_loss = self.validate_one_epoch(\n",
    "                seq_val_dataloader, retrain_val_dataloader, verbose\n",
    "            )\n",
    "\n",
    "            self.scheduler.step()\n",
    "            lr_last = self.optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "            train_sharpes.append(train_sharpe)\n",
    "            val_sharpes.append(val_sharpe)\n",
    "\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"{epoch + 1:^5} | {train_loss:^10.4f} | {train_icir_loss:^15.4f} | {train_mse_loss:^14.4f} | {train_ranking_loss:^17.4f} | {val_loss:^8.4f} | {val_icir_loss:^13.4f} | {val_mse_loss:^12.4f} | {val_ranking_loss:^16.4f} | {train_sharpe:^9.4f} | {val_sharpe:^7.4f} | {lr_last:^7.5f}\"\n",
    "                )\n",
    "\n",
    "            if val_sharpe > min_val_sharpe:\n",
    "                min_val_sharpe = val_sharpe\n",
    "                best_model = copy.deepcopy(self.model.state_dict())\n",
    "                no_improvement = 0\n",
    "                best_epoch = epoch\n",
    "            else:\n",
    "                no_improvement += 1\n",
    "\n",
    "            if self.early_stopping:\n",
    "                if no_improvement >= self.early_stopping_patience + 1:\n",
    "                    self.best_epoch = best_epoch + 1\n",
    "                    if verbose:\n",
    "                        print(f\"Early stopping on epoch {best_epoch + 1}. Best score: {min_val_sharpe:.4f}\")\n",
    "                    break\n",
    "\n",
    "        # Load the best model\n",
    "        if self.early_stopping:\n",
    "            self.model.load_state_dict(best_model)\n",
    "\n",
    "    def train_one_epoch(self, seq_train_dataloader: DataLoader, verbose: bool) -> tuple:\n",
    "        \"\"\"Train the model for one epoch.\n",
    "\n",
    "        Args:\n",
    "            train_dataloader (DataLoader): DataLoader for the training set.\n",
    "            verbose (bool): If True, shows progress using tqdm.\n",
    "\n",
    "        Returns:\n",
    "            tuple[float, float]: A tuple containing:\n",
    "                - Train loss (float).\n",
    "                - Spearman Sharpe for the training set (float).\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        total_icir_loss = 0.0\n",
    "        total_mse_loss = 0.0\n",
    "        total_ranking_loss = 0.0\n",
    "\n",
    "        y_total, preds_total = [], []\n",
    "\n",
    "        for seq_batch in seq_train_dataloader:\n",
    "            seq_batch = {key: value.to(self.device) for key, value in seq_batch.items()}\n",
    "            seq_x_batch = seq_batch[\"continuous\"]\n",
    "\n",
    "            true_y = seq_batch[\"current\"]\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            with torch.autocast(device_type=\"cuda\"):\n",
    "                pred_y = self.model(seq_x_batch)\n",
    "                loss, icir_loss, mse_loss, ranking_loss, _ = self.criterion(pred_y, true_y).values()\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_icir_loss += icir_loss.item()\n",
    "            total_mse_loss += mse_loss.item()\n",
    "            total_ranking_loss += ranking_loss.item()\n",
    "\n",
    "            y_total.append(true_y)\n",
    "            preds_total.append((pred_y).detach())\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        y_total = torch.cat(y_total).cpu().numpy().astype(np.float64)\n",
    "        preds_total = torch.cat(preds_total).cpu().numpy().astype(np.float64)\n",
    "\n",
    "        train_sharpe = rank_correlation_sharpe(y_total, preds_total)\n",
    "        train_loss = total_loss / len(seq_train_dataloader)\n",
    "        train_icir_loss = total_icir_loss / len(seq_train_dataloader)\n",
    "        train_mse_loss = total_mse_loss / len(seq_train_dataloader)\n",
    "        train_ranking_loss = total_ranking_loss / len(seq_train_dataloader)\n",
    "\n",
    "        return train_loss, train_sharpe, train_icir_loss, train_mse_loss, train_ranking_loss\n",
    "\n",
    "    @timer\n",
    "    def validate_one_epoch(self, seq_val_dataloader: DataLoader, retrain_val_dataloader: DataLoader, verbose=False) -> tuple:\n",
    "        \"\"\"Validate the model on the validation set.\n",
    "\n",
    "        Args:\n",
    "            val_dataloader (DataLoader): DataLoader for the validation set.\n",
    "            verbose (bool, optional): If True, shows progress using tqdm. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            tuple[float, float]: A tuple containing:\n",
    "                - Validation loss (float).\n",
    "                - Spearman Sharpe for the validation set (float).\n",
    "        \"\"\"\n",
    "        model = copy.deepcopy(self.model).to(\"cpu\")\n",
    "\n",
    "        losses, icir_losses, mse_losses, ranking_losses, all_y, all_preds = [], [], [], [], [], []\n",
    "\n",
    "        for seq_batch in seq_val_dataloader:\n",
    "            # seq_batch = {key: value.to(self.device) for key, value in seq_batch.items()}\n",
    "            # cs_batch = {key: value.to(self.device) for key, value in cs_batch.items()}\n",
    "            seq_x_batch = seq_batch[\"continuous\"]\n",
    "\n",
    "            true_y = seq_batch[\"current\"]\n",
    "\n",
    "            # Predict\n",
    "            model.eval()\n",
    "            with torch.inference_mode():\n",
    "                pred_y = model(seq_x_batch)\n",
    "                pred_y = torch.nan_to_num(pred_y)\n",
    "\n",
    "                loss, icir_loss, mse_loss, ranking_loss, _ = self.criterion(pred_y, true_y).values()\n",
    "                losses.append(loss.cpu().numpy())\n",
    "                icir_losses.append(icir_loss.cpu().numpy())\n",
    "                mse_losses.append(mse_loss.cpu().numpy())\n",
    "                ranking_losses.append(ranking_loss.cpu().numpy())\n",
    "\n",
    "                all_y.append(true_y)\n",
    "                all_preds.append(pred_y)\n",
    "\n",
    "            # Update weights\n",
    "            if self.lr_refit > 0:\n",
    "                for retrain_batch in retrain_val_dataloader:\n",
    "                    retrain_seq_x_batch = retrain_batch[\"continuous\"]\n",
    "                    retrain_true_y = retrain_batch[\"current\"]\n",
    "                optimizer = torch.optim.AdamW(model.parameters(), lr=self.lr_refit, weight_decay=0.01)\n",
    "                optimizer.zero_grad()\n",
    "                model.train()\n",
    "                pred_y = model(retrain_seq_x_batch)\n",
    "                loss = self.criterion(pred_y, retrain_true_y)[\"total_loss\"]\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "        all_y = torch.cat(all_y).numpy().astype(np.float64)\n",
    "        all_preds = torch.cat(all_preds).numpy().astype(np.float64)\n",
    "        loss = np.mean(losses)\n",
    "        val_icir_loss = np.mean(icir_losses)\n",
    "        val_mse_loss = np.mean(mse_losses)\n",
    "        val_ranking_loss = np.mean(ranking_losses)\n",
    "\n",
    "        sharpe = rank_correlation_sharpe(all_y, all_preds)\n",
    "\n",
    "        return loss, sharpe, val_icir_loss, val_mse_loss, val_ranking_loss\n",
    "\n",
    "    def update(self, seq_X: np.array, true_y: np.array):\n",
    "        \"\"\"Update the model with new data.\n",
    "\n",
    "        Args:\n",
    "            X (np.array): Input data.\n",
    "            y (np.array): Target variable.\n",
    "            n_times (int): Number of time steps.\n",
    "        \"\"\"\n",
    "        torch.manual_seed(self.random_seed)\n",
    "        if self.lr_refit == 0.0:\n",
    "            return\n",
    "\n",
    "        seq_continuous_data = torch.tensor(np.nan_to_num(seq_X, nan=0.0), dtype=torch.float32, device=self.device)\n",
    "\n",
    "        true_y = torch.tensor(np.nan_to_num(true_y, nan=0.0), dtype=torch.float32, device=self.device)\n",
    "        self.model.train()\n",
    "\n",
    "        self.refit_optimizer.zero_grad()\n",
    "        with torch.autocast(device_type=\"cuda\"):\n",
    "            pred_y = self.model(seq_continuous_data.unsqueeze(0))\n",
    "            loss = self.criterion(pred_y, true_y)[\"total_loss\"]\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "        self.refit_optimizer.step()\n",
    "\n",
    "    def predict(self, seq_X: np.array) -> tuple[np.array, torch.Tensor | list]:\n",
    "        \"\"\"Predict the target variable for the given input data.\n",
    "\n",
    "        Args:\n",
    "            X (np.array): Input data.\n",
    "\n",
    "        Returns:\n",
    "            tuple[np.array, torch.Tensor or list]: A tuple containing:\n",
    "                - Predictions (np.array).\n",
    "                - Hidden state (torch.Tensor or list).\n",
    "        \"\"\"\n",
    "        torch.manual_seed(self.random_seed)\n",
    "        seq_continuous_data = torch.tensor(np.nan_to_num(seq_X, nan=0.0), dtype=torch.float32, device=self.device)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.inference_mode():\n",
    "            preds = self.model(seq_continuous_data.unsqueeze(0))\n",
    "            preds = torch.nan_to_num(preds)\n",
    "\n",
    "        return preds.cpu().numpy().astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cce0fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare DataLoader ---\n",
    "# Create the dataset\n",
    "\n",
    "train_x = pl.scan_csv(CONFIG.TRAIN_X_PATH)\n",
    "train_x = PREPROCESSOR(df=train_x)\n",
    "train_x = train_x.clean()\n",
    "\n",
    "features = FEATURE_ENGINEERING(df=train_x)\n",
    "train_x: pl.DataFrame = features.create_market_features()\n",
    "\n",
    "train_y = pl.scan_csv(CONFIG.TRAIN_Y_PATH)\n",
    "\n",
    "# retrain_y = (\n",
    "#     train_y.with_columns([pl.col(CONFIG.LAGS[f\"lag{i}\"]).exclude(CONFIG.DATE_COL).shift(i + 1) for i in range(1, 5)])\n",
    "#     # .with_columns(pl.all().exclude(CONFIG.DATE_COL).shift())\n",
    "#     .filter((pl.col(CONFIG.DATE_COL).is_in(train_x.select(CONFIG.DATE_COL).to_series())))\n",
    "#     .collect()\n",
    "# )\n",
    "train_y = train_y.filter((pl.col(CONFIG.DATE_COL).is_in(train_x.select(CONFIG.DATE_COL).to_series()))).collect()\n",
    "train_x = (\n",
    "    train_x.with_columns([pl.when(pl.col(col).is_infinite()).then(0.0).otherwise(pl.col(col)).alias(col) for col in train_x.columns])\n",
    "    .with_columns(pl.all().shrink_dtype())\n",
    "    .filter(pl.col(CONFIG.DATE_COL).is_in(train_y.select(CONFIG.DATE_COL).to_series()))\n",
    "    .with_columns(pl.col(CONFIG.DATE_COL).cast(pl.Int64))\n",
    "    .select(([CONFIG.DATE_COL] + CONFIG.IMPT_COLS))\n",
    ")\n",
    "\n",
    "retrain_x = train_x.with_columns(pl.all().exclude(CONFIG.DATE_COL).shift(5))\n",
    "retrain_y = train_y.filter((pl.col(CONFIG.DATE_COL).is_in(train_x.select(CONFIG.DATE_COL).to_series()))).with_columns(\n",
    "    pl.all().exclude(CONFIG.DATE_COL).shift(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39ef4cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model = NN(\n",
    "    model=ENSEMBLE_NN(input_dim=len(train_x.columns) - 1, hidden_dim=128, output_dim=CONFIG.NUM_TARGET_COLUMNS),\n",
    "    batch_size=CONFIG.BATCH_SIZE,\n",
    "    lr=0.0005,\n",
    "    lr_refit=0.0001,\n",
    "    epochs=200,\n",
    "    early_stopping_patience=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "797745ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Fold 4--------------------\n",
      "Train dates from 1 to 1522\n",
      "Valid dates from 1523 to 1826\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "793dfb7b8abf40ba8593537035e1b1cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAL Sharpe: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\edmund\\Lib\\site-packages\\numpy\\core\\_methods.py:206: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\edmund\\Lib\\site-packages\\numpy\\core\\_methods.py:163: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\edmund\\Lib\\site-packages\\numpy\\core\\_methods.py:198: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_51580\\2344204298.py:30: RuntimeWarning: Mean of empty slice.\n",
      "  sharpe_ratio = daily_rank_corrs.mean() / std_dev\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\edmund\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# test_size = (\n",
    "#     TEST_SIZE\n",
    "#     if len(dates_unique) > TEST_SIZE * (n_splits + 1)\n",
    "#     else len(dates_unique) // (n_splits + 1)\n",
    "# )  # For testing purposes on small samples\n",
    "\n",
    "dates_unique = train_x.filter(pl.col(CONFIG.DATE_COL) <= CONFIG.MAX_TRAIN_DATE).select(pl.col(CONFIG.DATE_COL).unique().sort()).to_series().to_numpy()\n",
    "real_dates_unique = (\n",
    "    train_x.filter(pl.col(CONFIG.DATE_COL) > CONFIG.MAX_TRAIN_DATE).select(pl.col(CONFIG.DATE_COL).unique().sort()).to_series().to_numpy()\n",
    ")\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=CONFIG.N_FOLDS)\n",
    "cv_split = cv.split(dates_unique)\n",
    "\n",
    "scores = []\n",
    "models = []\n",
    "for fold, (train_idx, valid_idx) in enumerate(cv_split):\n",
    "    if fold <= 3:\n",
    "        continue\n",
    "    if CONFIG.VERBOSE:\n",
    "        print(\"-\" * 20 + f\"Fold {fold}\" + \"-\" * 20)\n",
    "        print(f\"Train dates from {dates_unique[train_idx].min()} to {dates_unique[train_idx].max()}\")\n",
    "        print(f\"Valid dates from {dates_unique[valid_idx].min()} to {dates_unique[valid_idx].max()}\")\n",
    "\n",
    "    dates_train = dates_unique[train_idx]\n",
    "    dates_valid = dates_unique[valid_idx]\n",
    "\n",
    "    df_train = train_x.filter(pl.col(CONFIG.DATE_COL).is_in(dates_train))\n",
    "    true_y = train_y.filter(pl.col(CONFIG.DATE_COL).is_in(dates_train))\n",
    "\n",
    "    df_valid = train_x.filter(pl.col(CONFIG.DATE_COL).is_in(dates_valid))\n",
    "    df_valid_current_y = train_y.filter(pl.col(CONFIG.DATE_COL).is_in(dates_valid))\n",
    "\n",
    "    df_valid_retrain = retrain_x.filter(pl.col(CONFIG.DATE_COL).is_in(dates_valid))\n",
    "    df_valid_current_y_retrain = retrain_y.filter(pl.col(CONFIG.DATE_COL).is_in(dates_valid))\n",
    "\n",
    "    model_fold = copy.deepcopy(NN_model)\n",
    "\n",
    "    # model_fold.fit(\n",
    "    #     train_set=(df_train, true_y),\n",
    "    #     val_set=(df_valid, df_valid_current_y),\n",
    "    #     retrain_set=(df_valid_retrain, df_valid_current_y_retrain),\n",
    "    #     verbose=CONFIG.VERBOSE,\n",
    "    # )\n",
    "\n",
    "    # models.append(model_fold)\n",
    "\n",
    "    # torch.save(\n",
    "    #     model_fold.model.state_dict(),\n",
    "    #     f\"C:/Users/Admin/Desktop/Personal-Projects/Kaggle/MITSUI&CO. Commodity Prediction Challenge/ensemble_{fold}.pth\",\n",
    "    # )\n",
    "\n",
    "    # preds = []\n",
    "    # cnt_dates = 0\n",
    "    # model_save = copy.deepcopy(model_fold)\n",
    "\n",
    "    model_fold.model.load_state_dict(\n",
    "        torch.load(\n",
    "            f\"C:/Users/Admin/Desktop/Personal-Projects/Kaggle/MITSUI&CO. Commodity Prediction Challenge/ensemble_{fold}.pth\",\n",
    "            map_location=torch.device(\"cuda\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # for date_id in tqdm(dates_valid):\n",
    "    #     df_valid_date = (\n",
    "    #         train_x.filter(pl.col(CONFIG.DATE_COL).is_in(range(date_id - CONFIG.SEQ_LEN + 1, date_id + 1)))\n",
    "    #         .drop(CONFIG.DATE_COL)\n",
    "    #         .to_numpy()\n",
    "    #         .astype(np.float64)\n",
    "    #     )\n",
    "\n",
    "    #     if model_fold.lr_refit and (cnt_dates > 0):\n",
    "    #         seq_period = range((date_id - CONFIG.SEQ_LEN), date_id)\n",
    "\n",
    "    #         df_upd = train_x.filter(pl.col(CONFIG.DATE_COL).is_in(seq_period)).drop(CONFIG.DATE_COL).to_numpy()\n",
    "\n",
    "    #         df_upd_current_y = train_y.filter(pl.col(CONFIG.DATE_COL).is_in(date_id)).drop(CONFIG.DATE_COL).to_numpy()\n",
    "\n",
    "    #         if len(df_upd) > 0:\n",
    "    #             model_save.update(df_upd, df_upd_current_y)\n",
    "\n",
    "    #     preds_i = model_save.predict(df_valid_date)\n",
    "\n",
    "    #     preds += list(preds_i[-1].reshape(-1, CONFIG.NUM_TARGET_COLUMNS))\n",
    "\n",
    "    #     cnt_dates += 1\n",
    "\n",
    "    # preds = np.array(preds)\n",
    "\n",
    "    # score = rank_correlation_sharpe(\n",
    "    #     df_valid_current_y.drop(CONFIG.DATE_COL).to_numpy().astype(np.float64),\n",
    "    #     preds,\n",
    "    # )\n",
    "    # scores.append(score)\n",
    "\n",
    "    # print(f\"LAST VALIDIDATION Sharpe: {score:.5f}\")\n",
    "\n",
    "    model_real = copy.deepcopy(model_fold)\n",
    "    preds = []\n",
    "    cnt_dates = 0\n",
    "    for date_id in tqdm(real_dates_unique):\n",
    "        # print(date_id)\n",
    "        df_valid_date = (\n",
    "            train_x.filter(pl.col(CONFIG.DATE_COL).is_in(range(date_id - CONFIG.SEQ_LEN + 1, date_id + 1)))\n",
    "            .drop(CONFIG.DATE_COL)\n",
    "            .to_numpy()\n",
    "            .astype(np.float64)\n",
    "        )\n",
    "        break\n",
    "\n",
    "        if model_fold.lr_refit and (cnt_dates > 0):\n",
    "            seq_period = range((date_id - CONFIG.SEQ_LEN), date_id)\n",
    "\n",
    "            df_upd = retrain_x.filter(pl.col(CONFIG.DATE_COL).is_in(seq_period)).drop(CONFIG.DATE_COL).to_numpy()\n",
    "            df_upd_current_y = retrain_y.filter(pl.col(CONFIG.DATE_COL).is_in(date_id)).drop(CONFIG.DATE_COL).to_numpy()\n",
    "\n",
    "            if len(df_upd) > 0:\n",
    "                model_real.update(df_upd, df_upd_current_y)\n",
    "\n",
    "            # print(df_upd[:, 0])\n",
    "            # print(df_upd_current_y[:, -1])\n",
    "            # print(df_upd_true_delta[:, -1])\n",
    "        preds_i = model_real.predict(df_valid_date)\n",
    "\n",
    "        preds += list(preds_i[-1].reshape(-1, CONFIG.NUM_TARGET_COLUMNS))\n",
    "        # print(preds_i[-1].reshape(-1, CONFIG.NUM_TARGET_COLUMNS))\n",
    "\n",
    "        cnt_dates += 1\n",
    "        # if cnt_dates == 2:\n",
    "        #     break\n",
    "\n",
    "    preds = np.array(preds)\n",
    "\n",
    "    score = rank_correlation_sharpe(\n",
    "        df_valid_current_y.drop(CONFIG.DATE_COL).to_numpy().astype(np.float64),\n",
    "        preds,\n",
    "    )\n",
    "    scores.append(score)\n",
    "    print(f\"REAL Sharpe: {score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5054189",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample = X_sample.clone().detach().requires_grad_(True).to(device)\n",
    "output = model(X_sample)\n",
    "output.mean().backward()\n",
    "\n",
    "grads = X_sample.grad.detach().cpu().numpy()\n",
    "importance = np.abs(grads).mean(axis=0)  # Average across samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99951219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\edmund\\Lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py:99: UserWarning: Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\n",
      "  warnings.warn(\"Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "<class '__main__.NN'> is not currently a supported model type!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m shap\u001b[38;5;241m.\u001b[39mDeepExplainer(model_real,df_valid_date)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\edmund\\Lib\\site-packages\\shap\\explainers\\_deep\\__init__.py:90\u001b[0m, in \u001b[0;36mDeepExplainer.__init__\u001b[1;34m(self, model, data, session, learning_phase_flags)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(model, masker)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 90\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplainer \u001b[38;5;241m=\u001b[39m TFDeep(model, data, session, learning_phase_flags)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m framework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpytorch\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplainer \u001b[38;5;241m=\u001b[39m PyTorchDeep(model, data)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\edmund\\Lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py:102\u001b[0m, in \u001b[0;36mTFDeep.__init__\u001b[1;34m(self, model, data, session, learning_phase_flags)\u001b[0m\n\u001b[0;32m     99\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# determine the model inputs and outputs\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_inputs \u001b[38;5;241m=\u001b[39m _get_model_inputs(model)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_output \u001b[38;5;241m=\u001b[39m _get_model_output(model)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_output, \u001b[38;5;28mlist\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model output to be explained must be a single tensor!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\edmund\\Lib\\site-packages\\shap\\explainers\\tf_utils.py:73\u001b[0m, in \u001b[0;36m_get_model_inputs\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     72\u001b[0m emsg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not currently a supported model type!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(emsg)\n",
      "\u001b[1;31mValueError\u001b[0m: <class '__main__.NN'> is not currently a supported model type!"
     ]
    }
   ],
   "source": [
    "shap.DeepExplainer(model_real,df_valid_date)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edmund",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
