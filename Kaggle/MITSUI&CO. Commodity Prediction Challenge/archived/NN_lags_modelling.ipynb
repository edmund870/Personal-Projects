{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09d651b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional\n",
    "import copy\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "from abc import ABC, abstractmethod\n",
    "import math\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from numba import njit, prange\n",
    "from scipy.stats import spearmanr, rankdata\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, KFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "# from torch.profiler import profile, ProfilerActivity, record_function\n",
    "\n",
    "from CONFIG import CONFIG\n",
    "from PREPROCESSOR_V2 import PREPROCESSOR\n",
    "from FEATURE_ENGINEERING_V2 import FEATURE_ENGINEERING\n",
    "from SEQUENTIAL_NN_MODEL import CNNTransformerModel, GRUModel, LSTMModel, PureTransformerModel\n",
    "from CROSS_SECTIONAL_NN_MODEL import DeepMLPModel, LinearModel, ResidualMLPModel\n",
    "from LOSS import CombinedICIRLoss\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        print(f\"{func.__name__} took {end - start:.4f} seconds\")\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bc2efde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Prepare DataLoader ---\n",
    "# # Create the dataset\n",
    "\n",
    "# train_x = pl.scan_csv(CONFIG.TRAIN_X_PATH).filter(pl.col(\"date_id\") <= CONFIG.MAX_TRAIN_DATE)\n",
    "# train_y = pl.scan_csv(CONFIG.TRAIN_Y_PATH).filter(pl.col(\"date_id\") <= CONFIG.MAX_TRAIN_DATE).fill_null(0).collect()\n",
    "\n",
    "# train_x = PREPROCESSOR(df=train_x)\n",
    "# train_x.clean()\n",
    "# train_x = train_x.transform().lazy()\n",
    "\n",
    "# train_x = FEATURE_ENGINEERING(df=train_x)\n",
    "# train_x = train_x.create_all_features().collect().pivot(index=CONFIG.DATE_COL, on=[\"type\", \"instr\"])\n",
    "# train_x = train_x.rename({col: re.sub(r'[{\",}]', \"\", col).replace(\" \", \"_\").replace(\",\", \"_\") for col in train_x.columns})\n",
    "# train_x = train_x.select(set(CONFIG.IMPT_COLS + [CONFIG.DATE_COL]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71253111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_correlation_sharpe(targets, predictions) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the rank correlation between predictions and target values,\n",
    "    and returns its Sharpe ratio (mean / standard deviation).\n",
    "\n",
    "    :param merged_df: DataFrame containing prediction columns (starting with 'prediction_')\n",
    "                      and target columns (starting with 'target_')\n",
    "    :return: Sharpe ratio of the rank correlation\n",
    "    :raises ZeroDivisionError: If the standard deviation is zero\n",
    "    \"\"\"\n",
    "    correlations = []\n",
    "\n",
    "    for i, (pred_row, target_row) in enumerate(zip(predictions, targets)):\n",
    "        # Find valid (non-NaN) assets for this timestep\n",
    "        valid_mask = ~np.isnan(target_row)\n",
    "        valid_pred = pred_row[valid_mask]\n",
    "        valid_target = target_row[valid_mask]\n",
    "\n",
    "        if np.std(pred_row) == 0 or np.std(target_row) == 0:\n",
    "            raise ZeroDivisionError(\"Zero standard deviation in a row.\")\n",
    "\n",
    "        rho = np.corrcoef(rankdata(valid_pred, method=\"average\"), rankdata(valid_target, method=\"average\"))[0, 1]\n",
    "        correlations.append(rho)\n",
    "\n",
    "    daily_rank_corrs = np.array(correlations)\n",
    "    std_dev = daily_rank_corrs.std(ddof=0)\n",
    "    if std_dev == 0:\n",
    "        raise ZeroDivisionError(\"Denominator is zero, unable to compute Sharpe ratio.\")\n",
    "\n",
    "    sharpe_ratio = daily_rank_corrs.mean() / std_dev\n",
    "    return float(sharpe_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d6675fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseFinancialDataset(Dataset, ABC):\n",
    "    \"\"\"Base class for financial datasets\"\"\"\n",
    "\n",
    "    def __init__(self, X: pl.DataFrame, y: pl.DataFrame, date_column: str = CONFIG.DATE_COL):\n",
    "        \"\"\"\n",
    "        Base initialization\n",
    "\n",
    "        Args:\n",
    "            data: Preprocessed DataFrame (scaling already done)\n",
    "            target_columns: List of target column names (424 targets)\n",
    "            feature_columns: List of feature column names\n",
    "            date_column: Name of date identifier column\n",
    "        \"\"\"\n",
    "        self.X = X.clone()\n",
    "        self.y = y.clone()\n",
    "        self.date_column = date_column\n",
    "\n",
    "        # Sort by date\n",
    "        self.X = self.X.sort(by=CONFIG.DATE_COL)\n",
    "        self.y = self.y.sort(by=CONFIG.DATE_COL)\n",
    "        self.unique_dates = sorted(self.X[self.date_column].unique())\n",
    "        self.device = torch.device(\"cpu\")\n",
    "\n",
    "        self.lag_featues = CONFIG.LAG_FEATURES\n",
    "\n",
    "        self._prepare_samples()\n",
    "\n",
    "    def _prepare_samples(self):\n",
    "        self.samples = {}\n",
    "        for lag, features in self.lag_featues.items():\n",
    "            X = self.X.select(features).to_numpy()\n",
    "\n",
    "            # self.num_features = self.X.shape[-1]\n",
    "            # Split continuous and categorical features\n",
    "            continuous_data = torch.tensor(X, dtype=torch.float32)\n",
    "            continuous_data = torch.nan_to_num(continuous_data, 0)\n",
    "            self.samples[lag] = continuous_data\n",
    "\n",
    "        self.dates = torch.tensor(self.unique_dates, dtype=torch.int16)\n",
    "        self.y = torch.tensor(self.y.drop(CONFIG.DATE_COL).to_numpy(), dtype=torch.float32)\n",
    "\n",
    "        self.unique_date, self.inverse_indices, self.counts = torch.unique(self.dates, return_inverse=True, return_counts=True)\n",
    "\n",
    "        self.n_unique_dates = len(self.unique_date)\n",
    "\n",
    "    @abstractmethod\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get item - implemented by subclasses\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed86feb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialDataset(BaseFinancialDataset):\n",
    "    \"\"\"Dataset for sequential models (LSTM, Transformers, CNN)\"\"\"\n",
    "\n",
    "    def __init__(self, X: pl.DataFrame, Y: pl.DataFrame, date_column: str = CONFIG.DATE_COL, prediction_horizon: int = 1):\n",
    "        \"\"\"\n",
    "        Sequential dataset for temporal models\n",
    "\n",
    "        Args:\n",
    "            data: Preprocessed DataFrame\n",
    "            target_columns: Target column names\n",
    "            feature_columns: Feature column names\n",
    "            date_column: Date identifier column\n",
    "            sequence_length: Number of time steps in sequence\n",
    "            prediction_horizon: Steps ahead to predict (usually 1)\n",
    "        \"\"\"\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "\n",
    "        super().__init__(X, Y)\n",
    "\n",
    "        self._generate_sequence()\n",
    "\n",
    "    def _generate_sequence(self):\n",
    "        self.sequence_x = {1: [], 2: [], 3: [], 4: []}\n",
    "        self.sequence_y = []\n",
    "\n",
    "        for date in range(max(CONFIG.LAG_SEQ_LEN.values()), self.n_unique_dates):\n",
    "            for lag, seq in CONFIG.LAG_SEQ_LEN.items():\n",
    "                sample = self.samples[lag]\n",
    "                self.sequence_x[lag].append(sample[date - seq : date])\n",
    "\n",
    "            self.sequence_y.append(self.y[date - 1])\n",
    "        for lag, seq in self.sequence_x.items():\n",
    "            self.sequence_x[lag] = torch.stack(seq)\n",
    "        self.sequence_y = torch.stack(self.sequence_y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_unique_dates - max(CONFIG.LAG_SEQ_LEN.values())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get sequence, target, and date_id\"\"\"\n",
    "        continuous_seq = {lag: self.sequence_x[lag][idx] for lag in self.sequence_x.keys()}  # (seq_len, N_FEATURES)\n",
    "        target = self.sequence_y[idx]  # (424,)\n",
    "\n",
    "        return continuous_seq, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82fe237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_collate_fn(batch: list) -> dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Collate function for DataLoader to flatten the batch.\n",
    "\n",
    "    Args:\n",
    "        batch (list): List of tuples containing tensors.\n",
    "\n",
    "        tuple[torch.Tensor]: Flattened tensors (type, instr, X, y).\n",
    "    \"\"\"\n",
    "    X, curr_y = zip(*batch)\n",
    "    lag_keys = X[0].keys()\n",
    "\n",
    "    # Stack each lag sequence across the batch\n",
    "    continuous_batch = {\n",
    "        lag: torch.stack([x_dict[lag] for x_dict in X])  # shape: [B, seq_len, num_features]\n",
    "        for lag in lag_keys\n",
    "    }\n",
    "\n",
    "    curr_y = torch.stack(curr_y)\n",
    "\n",
    "    return {\"continuous\": continuous_batch, \"current\": curr_y}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad522920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_val_dataset = SequentialDataset(df_valid, df_valid_current_y)\n",
    "# seq_val_dataloader = DataLoader(\n",
    "#     seq_val_dataset,\n",
    "#     batch_size=6,\n",
    "#     shuffle=False,\n",
    "#     collate_fn=flatten_collate_fn,\n",
    "#     pin_memory=True,\n",
    "#     # num_workers=6,\n",
    "#     # persistent_workers=True,\n",
    "#     # prefetch_factor=2,\n",
    "#     drop_last=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf1a5a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LagSpecificEnsemble(nn.Module):\n",
    "    \"\"\"Ensemble of multiple architectures for each lag\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # seq models\n",
    "        self.cnn_transformer = CNNTransformerModel(input_dim, hidden_dim, output_dim, CONFIG.SEQ_LEN)\n",
    "        self.gru_model = GRUModel(input_dim, hidden_dim, output_dim)\n",
    "        self.lstm_model = LSTMModel(input_dim, hidden_dim, output_dim, num_layers=2)\n",
    "        self.pure_transformer = PureTransformerModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "        # cross sectional models\n",
    "        self.mlp = DeepMLPModel(input_dim, [64, 32], output_dim)\n",
    "        self.linear = LinearModel(input_dim, output_dim)\n",
    "        self.residual = ResidualMLPModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "        # Ensemble weights (learnable)\n",
    "        self.ensemble_weights = nn.Parameter(torch.ones(3) / 3)\n",
    "\n",
    "        self.ensemble_dropout = nn.Dropout(0.3)\n",
    "        self.prediction_dropout = nn.Dropout(0.3)\n",
    "        self.input_dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_seq,\n",
    "    ):\n",
    "        x_seq = self.input_dropout(x_seq)\n",
    "        # Get predictions from all models\n",
    "        x_cs = x_seq[:, -1, :]\n",
    "        # out1 = self.cnn_transformer(x_seq)\n",
    "        out2 = self.gru_model(x_seq)\n",
    "        # out3 = self.lstm_model(x_seq)\n",
    "        # out4 = self.pure_transformer(x_seq)\n",
    "        out5 = self.mlp(x_cs)\n",
    "        # out6 = self.linear(x_cs)\n",
    "        # out7 = self.residual(x_cs)\n",
    "\n",
    "        individual_outputs = [out2, out5]  # out1, out2, out3, out4, out5, out6, out7\n",
    "        individual_outputs = [self.prediction_dropout(out) for out in individual_outputs]\n",
    "\n",
    "        dropped_weights = self.ensemble_dropout(self.ensemble_weights)\n",
    "        weights = F.softmax(dropped_weights, dim=0)\n",
    "\n",
    "        ensemble_output = torch.zeros_like(individual_outputs[0])  # (batch_size, 424)\n",
    "        for w, out in zip(weights, individual_outputs):\n",
    "            ensemble_output += w * out\n",
    "\n",
    "        entropy_reg = self.entropy_regularization(weights)\n",
    "        return ensemble_output + 0.01 * entropy_reg\n",
    "\n",
    "    def entropy_regularization(self, weights):\n",
    "        \"\"\"Penalize overly confident ensemble weights\"\"\"\n",
    "        return -torch.sum(weights * torch.log(weights + 1e-8))  # Entropy term\n",
    "\n",
    "\n",
    "class HierarchicalModel(nn.Module):\n",
    "    \"\"\"Complete hierarchical model with variable targets per lag\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: Dict[int, int], lag_target_sizes: Dict[int, int] = CONFIG.LAGS_TARGET):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lag_target_sizes = lag_target_sizes\n",
    "        self.num_lags = len(lag_target_sizes)\n",
    "\n",
    "        # Lag-specific encoders with different configurations (ensemble models for each lag)\n",
    "        self.lag_encoders = nn.ModuleList(\n",
    "            [\n",
    "                LagSpecificEnsemble(\n",
    "                    input_dim=input_dim[lag],\n",
    "                    hidden_dim=64,  # More capacity for longer lags\n",
    "                    output_dim=lag_target_sizes[lag],\n",
    "                )\n",
    "                for lag in lag_target_sizes.keys()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs: Dict[str, torch.Tensor]):\n",
    "        \"\"\"\n",
    "        inputs: Dict with keys 'lag_1', 'lag_2', etc.\n",
    "        Each tensor shape: (batch, seq_len, input_dim)\n",
    "        \"\"\"\n",
    "        lag_features = []\n",
    "\n",
    "        # Process each lag using the ensemble models\n",
    "        for i, (lag, encoder) in enumerate(zip(self.lag_target_sizes.keys(), self.lag_encoders)):\n",
    "            lag_key = lag\n",
    "            if lag_key in inputs:\n",
    "                features = encoder(inputs[lag_key])\n",
    "                lag_features.append(features)\n",
    "            else:\n",
    "                print(\"missing\")\n",
    "                # Handle missing lags (e.g., during inference)\n",
    "                batch_size = list(inputs.values())[0].size(0)\n",
    "                dummy_features = torch.zeros(batch_size, 64, device=next(encoder.parameters()).device)\n",
    "                lag_features.append(dummy_features)\n",
    "\n",
    "        # Return all individual lag predictions directly (no meta combiner)\n",
    "        return {\n",
    "            \"final_prediction\": torch.stack(lag_features).reshape(-1, CONFIG.NUM_TARGET_COLUMNS),  # List of tensors with different sizes for each lag\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ab79cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: HierarchicalModel,\n",
    "        lr: float = 0.001,\n",
    "        batch_size: int = 1,\n",
    "        epochs: int = 100,\n",
    "        early_stopping_patience: int = 10,\n",
    "        early_stopping: bool = True,\n",
    "        lr_patience: int = 2,\n",
    "        lr_factor: float = 0.5,\n",
    "        lr_refit: float = 0.001,\n",
    "        random_seed: int = CONFIG.RANDOM_STATE,\n",
    "        refit: bool = True,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.early_stopping = early_stopping\n",
    "        self.lr_patience = lr_patience\n",
    "        self.lr_factor = lr_factor\n",
    "        self.lr_refit = lr_refit\n",
    "        self.random_seed = random_seed\n",
    "        self.refit = refit\n",
    "\n",
    "        self.criterion = CombinedICIRLoss()\n",
    "\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = model.to(self.device)\n",
    "        # Different optimizers for different components\n",
    "        # self.lag_params = list(self.model.lag_encoders.parameters()) + list(self.model.prediction_heads.parameters())\n",
    "        # self.meta_params = list(self.model.meta_combiner.parameters()) + list(self.model.cross_attention.parameters())\n",
    "\n",
    "        # self.refit_lag_optimizer = torch.optim.AdamW(self.lag_params, lr=self.lr_refit[\"lag_optimizer\"], weight_decay=0.01)\n",
    "        # self.refit_meta_optimizer = torch.optim.AdamW(self.meta_params, lr=self.lr_refit[\"meta_optimizer\"], weight_decay=0.01)\n",
    "        # self.refit_optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr_refit[\"lag_optimizer\"], weight_decay=0.01)\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=0.01)\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optimizer, T_0=5, T_mult=2, eta_min=1e-6)\n",
    "\n",
    "        self.refit_optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr_refit, weight_decay=0.01)\n",
    "\n",
    "        self.best_epoch = None\n",
    "        self.features = None\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def fit(self, train_set: tuple, val_set: tuple, retrain_set: tuple, verbose: bool = False) -> None:\n",
    "        \"\"\"Fit the model on the training set and validate on the validation set.\n",
    "\n",
    "        Args:\n",
    "            train_set (tuple): A tuple containing input data, targets for training.\n",
    "            val_set (tuple): A tuple containing input data, targets for validation.\n",
    "            verbose (bool, optional): If True, prints training progress. Defaults to False.\n",
    "        \"\"\"\n",
    "        torch.manual_seed(self.random_seed)\n",
    "\n",
    "        seq_train_dataset = SequentialDataset(*train_set)\n",
    "        seq_train_dataloader = DataLoader(\n",
    "            seq_train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=flatten_collate_fn,\n",
    "            pin_memory=True,\n",
    "            # num_workers=2,\n",
    "            # persistent_workers=True,\n",
    "            # prefetch_factor=2,\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "        seq_val_dataset = SequentialDataset(*val_set)\n",
    "        seq_val_dataloader = DataLoader(\n",
    "            seq_val_dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            collate_fn=flatten_collate_fn,\n",
    "            pin_memory=True,\n",
    "            # num_workers=6,\n",
    "            # persistent_workers=True,\n",
    "            # prefetch_factor=2,\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "        retrain_val_dataset = SequentialDataset(*retrain_set)\n",
    "        retrain_val_dataloader = DataLoader(\n",
    "            retrain_val_dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            collate_fn=flatten_collate_fn,\n",
    "            pin_memory=True,\n",
    "            # num_workers=6,\n",
    "            # persistent_workers=True,\n",
    "            # prefetch_factor=2,\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "        train_sharpes, val_sharpes = [], []\n",
    "        if verbose:\n",
    "            print(f\"Device: {self.device}\")\n",
    "            print(\n",
    "                f\"{'Epoch':^5} | {'Train Loss':^10} | {'Train ICIR Loss':^15} | {'Train MSE Loss':^14} | {'Train Ranking Loss':^17} | {'Val Loss':^8} | {'Val ICIR Loss':^13} | {'Val MSE Loss':^12} | {'Val Ranking Loss':^16} | {'Train sharpe':^9} | {'Val sharpe':^7} | {'LR':^7}\"\n",
    "            )\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "        min_val_sharpe = -np.inf\n",
    "        best_epoch = 0\n",
    "        no_improvement = 0\n",
    "        best_model = None\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss, train_sharpe, train_icir_loss, train_mse_loss, train_ranking_loss = self.train_one_epoch(seq_train_dataloader, verbose)\n",
    "            val_loss, val_sharpe, val_icir_loss, val_mse_loss, val_ranking_loss = self.validate_one_epoch(\n",
    "                seq_val_dataloader, retrain_val_dataloader, verbose\n",
    "            )\n",
    "\n",
    "            self.scheduler.step()\n",
    "            lr_last = self.optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "            # self.lag_scheduler.step()\n",
    "            # self.meta_scheduler.step()\n",
    "            # lag_optimizer_lr_last = self.lag_optimizer.param_groups[0][\"lr\"]\n",
    "            # meta_optimizer_lr_last = self.meta_optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "            train_sharpes.append(train_sharpe)\n",
    "            val_sharpes.append(val_sharpe)\n",
    "\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"{epoch + 1:^5} | {train_loss:^10.4f} | {train_icir_loss:^15.4f} | {train_mse_loss:^14.4f} | {train_ranking_loss:^17.4f} | {val_loss:^8.4f} | {val_icir_loss:^13.4f} | {val_mse_loss:^12.4f} | {val_ranking_loss:^16.4f} | {train_sharpe:^9.4f} | {val_sharpe:^7.4f} | {lr_last:^7.5f}\"\n",
    "                )\n",
    "\n",
    "            if val_sharpe > min_val_sharpe:\n",
    "                min_val_sharpe = val_sharpe\n",
    "                best_model = copy.deepcopy(self.model.state_dict())\n",
    "                no_improvement = 0\n",
    "                best_epoch = epoch\n",
    "            else:\n",
    "                no_improvement += 1\n",
    "\n",
    "            if self.early_stopping:\n",
    "                if no_improvement >= self.early_stopping_patience + 1:\n",
    "                    self.best_epoch = best_epoch + 1\n",
    "                    if verbose:\n",
    "                        print(f\"Early stopping on epoch {best_epoch + 1}. Best score: {min_val_sharpe:.4f}\")\n",
    "                    break\n",
    "\n",
    "        # Load the best model\n",
    "        if self.early_stopping:\n",
    "            self.model.load_state_dict(best_model)\n",
    "\n",
    "    def train_one_epoch(self, seq_train_dataloader: DataLoader, verbose: bool) -> tuple:\n",
    "        \"\"\"Train the model for one epoch.\n",
    "\n",
    "        Args:\n",
    "            train_dataloader (DataLoader): DataLoader for the training set.\n",
    "            verbose (bool): If True, shows progress using tqdm.\n",
    "\n",
    "        Returns:\n",
    "            tuple[float, float]: A tuple containing:\n",
    "                - Train loss (float).\n",
    "                - Spearman Sharpe for the training set (float).\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        total_icir_loss = 0.0\n",
    "        total_mse_loss = 0.0\n",
    "        total_ranking_loss = 0.0\n",
    "\n",
    "        y_total, preds_total = [], []\n",
    "\n",
    "        for seq_batch in seq_train_dataloader:\n",
    "            seq_x_batch = seq_batch[\"continuous\"]\n",
    "            seq_x_batch = {key: value.to(self.device) for key, value in seq_x_batch.items()}\n",
    "\n",
    "            true_y = seq_batch[\"current\"].to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            # self.lag_optimizer.zero_grad()\n",
    "            # self.meta_optimizer.zero_grad()\n",
    "            with torch.autocast(device_type=\"cuda\"):\n",
    "                pred_y = self.model(seq_x_batch)\n",
    "                final_preds = pred_y[\"final_prediction\"]\n",
    "                loss, icir_loss, mse_loss, ranking_loss, _ = self.criterion(final_preds, true_y).values()\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)\n",
    "\n",
    "            self.optimizer.step()\n",
    "            # self.lag_optimizer.step()\n",
    "            # self.meta_optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_icir_loss += icir_loss.item()\n",
    "            total_mse_loss += mse_loss.item()\n",
    "            total_ranking_loss += ranking_loss.item()\n",
    "\n",
    "            y_total.append(true_y)\n",
    "            preds_total.append((final_preds).detach())\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        y_total = torch.cat(y_total).cpu().numpy().astype(np.float64)\n",
    "        preds_total = torch.cat(preds_total).cpu().numpy().astype(np.float64)\n",
    "\n",
    "        train_sharpe = rank_correlation_sharpe(y_total, preds_total)\n",
    "        train_loss = total_loss / len(seq_train_dataloader)\n",
    "        train_icir_loss = total_icir_loss / len(seq_train_dataloader)\n",
    "        train_mse_loss = total_mse_loss / len(seq_train_dataloader)\n",
    "        train_ranking_loss = total_ranking_loss / len(seq_train_dataloader)\n",
    "\n",
    "        return train_loss, train_sharpe, train_icir_loss, train_mse_loss, train_ranking_loss\n",
    "\n",
    "    @timer\n",
    "    def validate_one_epoch(self, seq_val_dataloader: DataLoader, retrain_val_dataloader: DataLoader, verbose=False) -> tuple:\n",
    "        \"\"\"Validate the model on the validation set.\n",
    "\n",
    "        Args:\n",
    "            val_dataloader (DataLoader): DataLoader for the validation set.\n",
    "            verbose (bool, optional): If True, shows progress using tqdm. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            tuple[float, float]: A tuple containing:\n",
    "                - Validation loss (float).\n",
    "                - Spearman Sharpe for the validation set (float).\n",
    "        \"\"\"\n",
    "        model = copy.deepcopy(self.model).to(\"cpu\")\n",
    "\n",
    "        losses, icir_losses, mse_losses, ranking_losses, all_y, all_preds = [], [], [], [], [], []\n",
    "\n",
    "        for seq_batch, retrain_batch in zip(seq_val_dataloader, retrain_val_dataloader):\n",
    "            # seq_batch = {key: value.to(self.device) for key, value in seq_batch.items()}\n",
    "            # cs_batch = {key: value.to(self.device) for key, value in cs_batch.items()}\n",
    "            seq_x_batch = seq_batch[\"continuous\"]\n",
    "            true_y = seq_batch[\"current\"]\n",
    "\n",
    "            # Predict\n",
    "            model.eval()\n",
    "            with torch.inference_mode():\n",
    "                pred_y = model(seq_x_batch)\n",
    "                final_preds = pred_y[\"final_prediction\"]\n",
    "                final_preds = torch.nan_to_num(final_preds)\n",
    "\n",
    "                loss, icir_loss, mse_loss, ranking_loss, _ = self.criterion(final_preds, true_y).values()\n",
    "                losses.append(loss.cpu().numpy())\n",
    "                icir_losses.append(icir_loss.cpu().numpy())\n",
    "                mse_losses.append(mse_loss.cpu().numpy())\n",
    "                ranking_losses.append(ranking_loss.cpu().numpy())\n",
    "\n",
    "                all_y.append(true_y)\n",
    "                all_preds.append(final_preds)\n",
    "\n",
    "            # Update weights\n",
    "            if self.refit:\n",
    "                retrain_seq_x_batch = retrain_batch[\"continuous\"]\n",
    "                retrain_true_y = retrain_batch[\"current\"]\n",
    "\n",
    "                self.refit_optimizer.zero_grad()\n",
    "                # self.refit_lag_optimizer.zero_grad()\n",
    "                # self.refit_meta_optimizer.zero_grad()\n",
    "\n",
    "                model.train()\n",
    "                pred_y = model(retrain_seq_x_batch)\n",
    "                final_preds = pred_y[\"final_prediction\"]\n",
    "\n",
    "                loss = self.criterion(final_preds, retrain_true_y)[\"total_loss\"]\n",
    "                loss.backward()\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "\n",
    "                self.refit_optimizer.step()\n",
    "                # self.refit_lag_optimizer.step()\n",
    "                # self.refit_meta_optimizer.step()\n",
    "\n",
    "        all_y = torch.cat(all_y).numpy().astype(np.float64)\n",
    "        all_preds = torch.cat(all_preds).numpy().astype(np.float64)\n",
    "        loss = np.mean(losses)\n",
    "        val_icir_loss = np.mean(icir_losses)\n",
    "        val_mse_loss = np.mean(mse_losses)\n",
    "        val_ranking_loss = np.mean(ranking_losses)\n",
    "\n",
    "        sharpe = rank_correlation_sharpe(all_y, all_preds)\n",
    "\n",
    "        return loss, sharpe, val_icir_loss, val_mse_loss, val_ranking_loss\n",
    "\n",
    "    def update(self, seq_X: Dict[int, np.array], true_y: np.array):\n",
    "        \"\"\"Update the model with new data.\n",
    "\n",
    "        Args:\n",
    "            X (np.array): Input data.\n",
    "            y (np.array): Target variable.\n",
    "            n_times (int): Number of time steps.\n",
    "        \"\"\"\n",
    "        torch.manual_seed(self.random_seed)\n",
    "        if self.lr_refit == 0.0:\n",
    "            return\n",
    "\n",
    "        seq_continuous_data = {\n",
    "            lag: torch.tensor(np.nan_to_num(X, nan=0.0), dtype=torch.float32, device=self.device).unsqueeze(0) for lag, X in seq_X.items()\n",
    "        }\n",
    "\n",
    "        true_y = torch.tensor(np.nan_to_num(true_y, nan=0.0), dtype=torch.float32, device=self.device)\n",
    "        self.model.train()\n",
    "\n",
    "        self.refit_optimizer.zero_grad()\n",
    "        # self.refit_lag_optimizer.zero_grad()\n",
    "        # self.refit_meta_optimizer.zero_grad()\n",
    "        with torch.autocast(device_type=\"cuda\"):\n",
    "            pred_y = self.model(seq_continuous_data)\n",
    "            loss = self.criterion(pred_y[\"final_prediction\"], true_y)[\"total_loss\"]\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)\n",
    "        self.refit_optimizer.step()\n",
    "        # self.refit_lag_optimizer.step()\n",
    "        # self.refit_meta_optimizer.step()\n",
    "\n",
    "    def predict(self, seq_X: Dict[int, np.array]) -> tuple[np.array, torch.Tensor | list]:\n",
    "        \"\"\"Predict the target variable for the given input data.\n",
    "\n",
    "        Args:\n",
    "            X (np.array): Input data.\n",
    "\n",
    "        Returns:\n",
    "            tuple[np.array, torch.Tensor or list]: A tuple containing:\n",
    "                - Predictions (np.array).\n",
    "                - Hidden state (torch.Tensor or list).\n",
    "        \"\"\"\n",
    "        torch.manual_seed(self.random_seed)\n",
    "        seq_continuous_data = {\n",
    "            lag: torch.tensor(np.nan_to_num(X, nan=0.0), dtype=torch.float32, device=self.device).unsqueeze(0) for lag, X in seq_X.items()\n",
    "        }\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.inference_mode():\n",
    "            preds = self.model(seq_continuous_data)\n",
    "            preds = torch.nan_to_num(preds[\"final_prediction\"])\n",
    "\n",
    "        return preds.cpu().numpy().astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cce0fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_compute_lag_returns took 0.0125 seconds\n",
      "_compute_autocorr_torch took 0.3736 seconds\n",
      "_compute_obv took 0.0095 seconds\n",
      "_compute_return_skew took 0.0025 seconds\n",
      "_compute_volume_z took 0.0416 seconds\n",
      "_compute_market_stats took 0.1341 seconds\n",
      "_compute_atr took 0.0075 seconds\n",
      "_compute_rolling took 0.0133 seconds\n",
      "create_market_features took 2.1133 seconds\n",
      "_compute_lag_returns took 0.0084 seconds\n",
      "_compute_market_stats took 0.0115 seconds\n",
      "_compute_return_skew took 0.0030 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_47708\\3323296322.py:59: RuntimeWarning: Mean of empty slice\n",
      "  (retrain_y_arr - np.nanmean(retrain_y_arr, axis=1).reshape(retrain_y_arr.shape[0], -1))\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\edmund\\Lib\\site-packages\\numpy\\lib\\nanfunctions.py:1879: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    }
   ],
   "source": [
    "# --- Prepare DataLoader ---\n",
    "# Create the dataset\n",
    "\n",
    "train_x = pl.scan_csv(CONFIG.TRAIN_X_PATH)\n",
    "train_x = PREPROCESSOR(df=train_x)\n",
    "train_x = train_x.clean()\n",
    "\n",
    "features = FEATURE_ENGINEERING(df=train_x)\n",
    "train_x: pl.DataFrame = features.create_market_features()\n",
    "\n",
    "train_y = pl.scan_csv(CONFIG.TRAIN_Y_PATH)\n",
    "\n",
    "curr_y = (\n",
    "    train_y.with_columns([pl.col(CONFIG.LAGS[f\"lag{i}\"]).exclude(CONFIG.DATE_COL).shift(i + 1) for i in range(1, 5)])\n",
    "    .with_columns(pl.all().exclude(CONFIG.DATE_COL).shift())\n",
    "    .filter((pl.col(CONFIG.DATE_COL).is_in(train_x.select(CONFIG.DATE_COL).to_series())))\n",
    "    .collect()\n",
    "    .fill_null(0)\n",
    "    .lazy()\n",
    ")\n",
    "\n",
    "y_feat = FEATURE_ENGINEERING(df=curr_y)\n",
    "lags = y_feat._compute_lag_returns(df=curr_y)\n",
    "market = y_feat._compute_market_stats(df=curr_y)\n",
    "skew = y_feat._compute_return_skew(df=curr_y)\n",
    "\n",
    "train_x = (\n",
    "    train_x.join(curr_y.collect(), on=CONFIG.DATE_COL)\n",
    "    .join(lags.collect(), on=CONFIG.DATE_COL)\n",
    "    .join(market.collect(), on=CONFIG.DATE_COL)\n",
    "    .join(skew.collect(), on=CONFIG.DATE_COL)\n",
    ")\n",
    "\n",
    "\n",
    "train_y = train_y.filter((pl.col(CONFIG.DATE_COL).is_in(train_x.select(CONFIG.DATE_COL).to_series()))).collect()\n",
    "train_x = (\n",
    "    train_x.with_columns([pl.when(pl.col(col).is_infinite()).then(0.0).otherwise(pl.col(col)).alias(col) for col in train_x.columns])\n",
    "    .with_columns(pl.all().shrink_dtype())\n",
    "    .filter(pl.col(CONFIG.DATE_COL).is_in(train_y.select(CONFIG.DATE_COL).to_series()))\n",
    "    .with_columns(pl.col(CONFIG.DATE_COL).cast(pl.Int64))\n",
    "    # .select([CONFIG.DATE_COL] + CONFIG.IMPT_COL)\n",
    ")\n",
    "\n",
    "retrain_x = train_x.with_columns(pl.all().exclude(CONFIG.DATE_COL).shift(5))\n",
    "retrain_y = train_y.filter((pl.col(CONFIG.DATE_COL).is_in(train_x.select(CONFIG.DATE_COL).to_series()))).with_columns(\n",
    "    pl.all().exclude(CONFIG.DATE_COL).shift(5)\n",
    ")\n",
    "\n",
    "train_y_arr = train_y.drop(CONFIG.DATE_COL).to_numpy()\n",
    "\n",
    "train_y = pl.DataFrame(\n",
    "    (train_y_arr - np.nanmean(train_y_arr, axis=1).reshape(train_y_arr.shape[0], -1))\n",
    "    / np.nanstd(train_y_arr, axis=1).reshape(train_y_arr.shape[0], -1),\n",
    "    schema=train_y.drop(CONFIG.DATE_COL).columns,\n",
    ").insert_column(0, train_y.select(CONFIG.DATE_COL).to_series())\n",
    "\n",
    "retrain_y_arr = retrain_y.drop(CONFIG.DATE_COL).to_numpy()\n",
    "retrain_y = pl.DataFrame(\n",
    "    (retrain_y_arr - np.nanmean(retrain_y_arr, axis=1).reshape(retrain_y_arr.shape[0], -1))\n",
    "    / np.nanstd(retrain_y_arr, axis=1).reshape(retrain_y_arr.shape[0], -1),\n",
    "    schema=train_y.drop(CONFIG.DATE_COL).columns,\n",
    ").insert_column(0, train_y.select(CONFIG.DATE_COL).to_series())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39ef4cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model = NN(\n",
    "    model=HierarchicalModel(input_dim={lag: len(features) for lag, features in CONFIG.LAG_FEATURES.items()}, lag_target_sizes=CONFIG.LAGS_TARGET),\n",
    "    batch_size=CONFIG.BATCH_SIZE,\n",
    "    lr=0.0005,\n",
    "    lr_refit=0.0005,\n",
    "    epochs=200,\n",
    "    early_stopping_patience=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797745ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Fold 4--------------------\n",
      "Train dates from 2 to 1522\n",
      "Valid dates from 1523 to 1826\n",
      "Device: cuda:0\n",
      "Epoch | Train Loss | Train ICIR Loss | Train MSE Loss | Train Ranking Loss | Val Loss | Val ICIR Loss | Val MSE Loss | Val Ranking Loss | Train sharpe | Val sharpe |   LR   \n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# test_size = (\n",
    "#     TEST_SIZE\n",
    "#     if len(dates_unique) > TEST_SIZE * (n_splits + 1)\n",
    "#     else len(dates_unique) // (n_splits + 1)\n",
    "# )  # For testing purposes on small samples\n",
    "\n",
    "dates_unique = train_x.filter(pl.col(CONFIG.DATE_COL) <= CONFIG.MAX_TRAIN_DATE).select(pl.col(CONFIG.DATE_COL).unique().sort()).to_series().to_numpy()\n",
    "real_dates_unique = (\n",
    "    train_x.filter(pl.col(CONFIG.DATE_COL) > CONFIG.MAX_TRAIN_DATE).select(pl.col(CONFIG.DATE_COL).unique().sort()).to_series().to_numpy()\n",
    ")\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=CONFIG.N_FOLDS)\n",
    "cv_split = cv.split(dates_unique)\n",
    "\n",
    "scores = []\n",
    "models = []\n",
    "for fold, (train_idx, valid_idx) in enumerate(cv_split):\n",
    "    if fold <= 3:\n",
    "        continue\n",
    "    if CONFIG.VERBOSE:\n",
    "        print(\"-\" * 20 + f\"Fold {fold}\" + \"-\" * 20)\n",
    "        print(f\"Train dates from {dates_unique[train_idx].min()} to {dates_unique[train_idx].max()}\")\n",
    "        print(f\"Valid dates from {dates_unique[valid_idx].min()} to {dates_unique[valid_idx].max()}\")\n",
    "\n",
    "    dates_train = dates_unique[train_idx]\n",
    "    dates_valid = dates_unique[valid_idx]\n",
    "\n",
    "    df_train = train_x.filter(pl.col(CONFIG.DATE_COL).is_in(dates_train))\n",
    "    true_y = train_y.filter(pl.col(CONFIG.DATE_COL).is_in(dates_train))\n",
    "\n",
    "    valid_period = range(min(dates_valid) - max(CONFIG.LAG_SEQ_LEN.values()) + 1, max(dates_valid) + 1)\n",
    "    df_valid = train_x.filter(pl.col(CONFIG.DATE_COL).is_in(valid_period))\n",
    "    df_valid_current_y = train_y.filter(pl.col(CONFIG.DATE_COL).is_in(valid_period))\n",
    "\n",
    "    df_valid_retrain = retrain_x.filter(pl.col(CONFIG.DATE_COL).is_in(valid_period))\n",
    "    df_valid_current_y_retrain = retrain_y.filter(pl.col(CONFIG.DATE_COL).is_in(valid_period))\n",
    "\n",
    "    model_fold = copy.deepcopy(NN_model)\n",
    "\n",
    "    model_fold.fit(\n",
    "        train_set=(df_train, true_y),\n",
    "        val_set=(df_valid, df_valid_current_y),\n",
    "        retrain_set=(df_valid_retrain, df_valid_current_y_retrain),\n",
    "        verbose=CONFIG.VERBOSE,\n",
    "    )\n",
    "\n",
    "    models.append(model_fold)\n",
    "\n",
    "    torch.save(\n",
    "        model_fold.model.state_dict(),\n",
    "        f\"C:/Users/Admin/Desktop/Personal-Projects/Kaggle/MITSUI&CO. Commodity Prediction Challenge/ensemble_{fold}.pth\",\n",
    "    )\n",
    "\n",
    "    preds = []\n",
    "    cnt_dates = 0\n",
    "    model_save = copy.deepcopy(model_fold)\n",
    "\n",
    "    model_fold.model.load_state_dict(\n",
    "        torch.load(\n",
    "            f\"C:/Users/Admin/Desktop/Personal-Projects/Kaggle/MITSUI&CO. Commodity Prediction Challenge/ensemble_{fold}.pth\",\n",
    "            map_location=torch.device(\"cuda\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for date_id in tqdm(dates_valid):\n",
    "        period = range(date_id - max(CONFIG.LAG_SEQ_LEN.values()) + 1, date_id + 1)\n",
    "\n",
    "        df_valid_date = train_x.filter(pl.col(CONFIG.DATE_COL).is_in(period)).drop(CONFIG.DATE_COL)\n",
    "        valid_lags = {lag: df_valid_date.select(features).to_numpy().astype(np.float64) for lag, features in CONFIG.LAG_FEATURES.items()}\n",
    "        valid_lags = {lag: valid_lags[lag][-seq_len:] for lag, seq_len in CONFIG.LAG_SEQ_LEN.items()}\n",
    "\n",
    "        if model_fold.refit and (cnt_dates > 0):\n",
    "            df_upd = retrain_x.filter(pl.col(CONFIG.DATE_COL).is_in(period)).drop(CONFIG.DATE_COL)\n",
    "            df_upd_lags = {lag: df_upd.select(features).to_numpy().astype(np.float64) for lag, features in CONFIG.LAG_FEATURES.items()}\n",
    "            df_upd_lags = {lag: df_upd_lags[lag][-seq_len:] for lag, seq_len in CONFIG.LAG_SEQ_LEN.items()}\n",
    "\n",
    "            df_upd_current_y = retrain_y.filter(pl.col(CONFIG.DATE_COL).is_in(date_id)).drop(CONFIG.DATE_COL).to_numpy()\n",
    "\n",
    "            if len(df_upd) > 0:\n",
    "                model_save.update(df_upd_lags, df_upd_current_y)\n",
    "\n",
    "        preds_i = model_save.predict(valid_lags)\n",
    "\n",
    "        preds += list(preds_i[-1].reshape(-1, CONFIG.NUM_TARGET_COLUMNS))\n",
    "\n",
    "        cnt_dates += 1\n",
    "\n",
    "    preds = np.array(preds)\n",
    "\n",
    "    score = rank_correlation_sharpe(\n",
    "        df_valid_current_y.drop(CONFIG.DATE_COL).to_numpy().astype(np.float64),\n",
    "        preds,\n",
    "    )\n",
    "    scores.append(score)\n",
    "\n",
    "    print(f\"LAST VALIDIDATION Sharpe: {score:.5f}\")\n",
    "\n",
    "    model_real = copy.deepcopy(model_save)\n",
    "    preds = []\n",
    "    cnt_dates = 0\n",
    "    for date_id in tqdm(real_dates_unique):\n",
    "        # print(date_id)\n",
    "        period = range(date_id - max(CONFIG.LAG_SEQ_LEN.values()) + 1, date_id + 1)\n",
    "        df_valid_date = train_x.filter(pl.col(CONFIG.DATE_COL).is_in(period)).drop(CONFIG.DATE_COL)\n",
    "\n",
    "        valid_lags = {lag: df_valid_date.select(features).to_numpy().astype(np.float64) for lag, features in CONFIG.LAG_FEATURES.items()}\n",
    "        valid_lags = {lag: valid_lags[lag][-seq_len:] for lag, seq_len in CONFIG.LAG_SEQ_LEN.items()}\n",
    "\n",
    "        if model_fold.refit and (cnt_dates > 0):\n",
    "            df_upd = retrain_x.filter(pl.col(CONFIG.DATE_COL).is_in(period)).drop(CONFIG.DATE_COL)\n",
    "            df_upd_lags = {lag: df_upd.select(features).to_numpy().astype(np.float64) for lag, features in CONFIG.LAG_FEATURES.items()}\n",
    "            df_upd_lags = {lag: df_upd_lags[lag][-seq_len:] for lag, seq_len in CONFIG.LAG_SEQ_LEN.items()}\n",
    "\n",
    "            df_upd_current_y = retrain_y.filter(pl.col(CONFIG.DATE_COL).is_in(date_id)).drop(CONFIG.DATE_COL).to_numpy()\n",
    "\n",
    "            if len(df_upd) > 0:\n",
    "                model_real.update(df_upd_lags, df_upd_current_y)\n",
    "\n",
    "        # print(df_upd[:, 0])\n",
    "        # print(df_upd_current_y[:, -1])\n",
    "        # print(df_upd_true_delta[:, -1])\n",
    "        preds_i = model_real.predict(valid_lags)\n",
    "\n",
    "        preds += list(preds_i[-1].reshape(-1, CONFIG.NUM_TARGET_COLUMNS))\n",
    "        # print(preds_i[-1].reshape(-1, CONFIG.NUM_TARGET_COLUMNS))\n",
    "\n",
    "        cnt_dates += 1\n",
    "        # if cnt_dates == 2:\n",
    "        #     break\n",
    "\n",
    "    preds = np.array(preds)\n",
    "\n",
    "    score = rank_correlation_sharpe(\n",
    "        train_y.filter(pl.col(CONFIG.DATE_COL).is_in(real_dates_unique)).drop(CONFIG.DATE_COL).to_numpy().astype(np.float64),\n",
    "        preds,\n",
    "    )\n",
    "    scores.append(score)\n",
    "    print(f\"REAL Sharpe: {score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e676e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (32, 100)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>column_0</th><th>column_1</th><th>column_2</th><th>column_3</th><th>column_4</th><th>column_5</th><th>column_6</th><th>column_7</th><th>column_8</th><th>column_9</th><th>column_10</th><th>column_11</th><th>column_12</th><th>column_13</th><th>column_14</th><th>column_15</th><th>column_16</th><th>column_17</th><th>column_18</th><th>column_19</th><th>column_20</th><th>column_21</th><th>column_22</th><th>column_23</th><th>column_24</th><th>column_25</th><th>column_26</th><th>column_27</th><th>column_28</th><th>column_29</th><th>column_30</th><th>column_31</th><th>column_32</th><th>column_33</th><th>column_34</th><th>column_35</th><th>column_36</th><th>&hellip;</th><th>column_63</th><th>column_64</th><th>column_65</th><th>column_66</th><th>column_67</th><th>column_68</th><th>column_69</th><th>column_70</th><th>column_71</th><th>column_72</th><th>column_73</th><th>column_74</th><th>column_75</th><th>column_76</th><th>column_77</th><th>column_78</th><th>column_79</th><th>column_80</th><th>column_81</th><th>column_82</th><th>column_83</th><th>column_84</th><th>column_85</th><th>column_86</th><th>column_87</th><th>column_88</th><th>column_89</th><th>column_90</th><th>column_91</th><th>column_92</th><th>column_93</th><th>column_94</th><th>column_95</th><th>column_96</th><th>column_97</th><th>column_98</th><th>column_99</th></tr><tr><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>&hellip;</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td></tr></thead><tbody><tr><td>-0.600796</td><td>-0.914647</td><td>0.030829</td><td>-0.708012</td><td>-0.054668</td><td>-0.948001</td><td>-1.359432</td><td>-0.146292</td><td>-0.137402</td><td>-0.850851</td><td>0.078772</td><td>-0.123519</td><td>0.000983</td><td>-0.884438</td><td>1.125735</td><td>1.143567</td><td>1.159554</td><td>-0.505677</td><td>1.106106</td><td>-0.168141</td><td>-1.12163</td><td>-0.141626</td><td>-0.418873</td><td>-2.22735</td><td>-0.018628</td><td>0.876863</td><td>-0.263154</td><td>-0.659306</td><td>-0.949431</td><td>-1.160557</td><td>-0.547408</td><td>-0.144907</td><td>-0.32823</td><td>0.011029</td><td>0.008649</td><td>-1.469505</td><td>-0.192384</td><td>&hellip;</td><td>0.416911</td><td>0.49945</td><td>0.415239</td><td>0.01134</td><td>0.505069</td><td>0.011571</td><td>0.011571</td><td>-0.065728</td><td>0.428339</td><td>0.427753</td><td>-1.43725</td><td>-1.056561</td><td>-0.017627</td><td>0.110134</td><td>0.15424</td><td>0.503649</td><td>0.620564</td><td>0.06471</td><td>0.399062</td><td>0.604878</td><td>0.691247</td><td>-0.02008</td><td>0.693985</td><td>0.638001</td><td>-1.114075</td><td>0.182135</td><td>0.662117</td><td>0.721626</td><td>-0.987616</td><td>0.682037</td><td>-0.065506</td><td>1.024815</td><td>-0.410165</td><td>1.093586</td><td>-0.052384</td><td>0.030002</td><td>0.118704</td></tr><tr><td>-0.331305</td><td>-0.799951</td><td>0.04221</td><td>-0.609153</td><td>0.222382</td><td>-1.083803</td><td>-1.329361</td><td>-0.524006</td><td>-0.866288</td><td>-0.78775</td><td>-0.705297</td><td>0.011299</td><td>-0.016925</td><td>-0.681306</td><td>1.074381</td><td>1.087834</td><td>1.111351</td><td>-0.04803</td><td>1.053658</td><td>-0.819396</td><td>-0.831125</td><td>-0.809522</td><td>-0.274026</td><td>-2.369183</td><td>-0.068554</td><td>0.866089</td><td>-1.459333</td><td>-0.513191</td><td>-0.818227</td><td>-1.076879</td><td>-0.287044</td><td>-1.180784</td><td>-0.213305</td><td>-0.002656</td><td>-0.586793</td><td>-1.471487</td><td>-1.138588</td><td>&hellip;</td><td>0.343683</td><td>0.436692</td><td>0.33969</td><td>-0.005089</td><td>0.441262</td><td>-0.024865</td><td>-0.024865</td><td>-0.018148</td><td>0.358543</td><td>0.355994</td><td>-1.439224</td><td>-0.91452</td><td>0.011029</td><td>-0.09682</td><td>-1.293827</td><td>0.296467</td><td>0.540513</td><td>-0.136164</td><td>-0.716517</td><td>0.52334</td><td>0.608413</td><td>0.168394</td><td>0.620309</td><td>0.637208</td><td>-1.226812</td><td>0.063022</td><td>0.663466</td><td>0.724069</td><td>-0.763981</td><td>0.68042</td><td>-0.068423</td><td>0.949595</td><td>-1.30037</td><td>1.074553</td><td>-0.389326</td><td>0.056125</td><td>-0.120663</td></tr><tr><td>-0.672513</td><td>-0.636656</td><td>0.046717</td><td>-0.600888</td><td>0.41067</td><td>-0.007206</td><td>-1.275968</td><td>-0.859563</td><td>-1.660442</td><td>-0.744369</td><td>-0.602834</td><td>0.000385</td><td>0.01921</td><td>-0.459605</td><td>1.024923</td><td>1.03614</td><td>1.055555</td><td>0.60446</td><td>1.007326</td><td>-1.579592</td><td>-0.377803</td><td>-1.605468</td><td>-0.090251</td><td>2.471409</td><td>0.191179</td><td>0.707784</td><td>-1.358112</td><td>-0.466072</td><td>-0.894953</td><td>-0.814633</td><td>-0.734001</td><td>-0.546881</td><td>-0.124805</td><td>0.021448</td><td>-0.777764</td><td>-1.104306</td><td>-0.532989</td><td>&hellip;</td><td>0.26818</td><td>0.357569</td><td>0.263673</td><td>-0.026372</td><td>0.358072</td><td>-0.02419</td><td>-0.02419</td><td>-0.026771</td><td>0.275264</td><td>0.273948</td><td>-1.451764</td><td>-0.862717</td><td>-0.002656</td><td>-0.346436</td><td>0.475959</td><td>0.182605</td><td>0.4661</td><td>-0.360702</td><td>0.214843</td><td>0.454266</td><td>0.53341</td><td>0.190971</td><td>0.535629</td><td>0.640734</td><td>-1.364923</td><td>0.327149</td><td>0.667453</td><td>0.727474</td><td>-0.545925</td><td>0.684033</td><td>-0.044396</td><td>0.847165</td><td>-1.70006</td><td>1.060013</td><td>-0.580176</td><td>0.024639</td><td>-0.258854</td></tr><tr><td>-0.669441</td><td>-0.958308</td><td>0.041761</td><td>-0.694853</td><td>0.48666</td><td>-0.399988</td><td>-0.731586</td><td>-0.852627</td><td>-2.475681</td><td>-0.705136</td><td>-0.313858</td><td>0.07329</td><td>-0.023334</td><td>-0.469034</td><td>0.978591</td><td>0.98603</td><td>1.011364</td><td>0.021094</td><td>0.956699</td><td>-2.387139</td><td>-0.162747</td><td>-2.413514</td><td>0.981804</td><td>2.242544</td><td>0.101565</td><td>0.63733</td><td>-1.347286</td><td>-0.552309</td><td>-0.894854</td><td>-0.839749</td><td>-1.0248</td><td>-1.514108</td><td>-0.186853</td><td>0.007691</td><td>-0.70138</td><td>-0.926617</td><td>-1.330135</td><td>&hellip;</td><td>0.214499</td><td>0.285683</td><td>0.209884</td><td>-0.039141</td><td>0.285819</td><td>0.022193</td><td>0.022193</td><td>0.014136</td><td>0.230739</td><td>0.231165</td><td>-1.503957</td><td>-1.070332</td><td>0.021448</td><td>-0.551869</td><td>0.168374</td><td>0.124773</td><td>0.392771</td><td>-0.570181</td><td>-0.43412</td><td>0.375534</td><td>0.457342</td><td>0.102894</td><td>0.465667</td><td>0.642076</td><td>-1.394</td><td>0.275905</td><td>0.663425</td><td>0.725267</td><td>-0.496921</td><td>0.686843</td><td>-0.002049</td><td>0.731776</td><td>-1.717889</td><td>1.030099</td><td>-0.786002</td><td>-0.005382</td><td>-0.331825</td></tr><tr><td>0.514794</td><td>-1.189119</td><td>0.022192</td><td>-0.588277</td><td>-0.064362</td><td>0.077836</td><td>-0.398386</td><td>-0.778215</td><td>-1.876163</td><td>-0.639785</td><td>0.000009</td><td>0.172795</td><td>0.027207</td><td>-0.403682</td><td>0.921649</td><td>0.928434</td><td>0.955571</td><td>-0.079817</td><td>0.901019</td><td>-1.822546</td><td>-0.215668</td><td>-1.818873</td><td>1.469027</td><td>2.26922</td><td>0.238014</td><td>0.374133</td><td>0.139196</td><td>-0.685932</td><td>-2.453737</td><td>-0.124573</td><td>-1.139299</td><td>0.327368</td><td>-0.422237</td><td>0.056293</td><td>-0.277295</td><td>-1.232394</td><td>0.412006</td><td>&hellip;</td><td>0.175401</td><td>0.234107</td><td>0.171287</td><td>-0.052108</td><td>0.235389</td><td>-0.001036</td><td>-0.001036</td><td>-0.050332</td><td>0.180022</td><td>0.179639</td><td>-1.502217</td><td>-1.165954</td><td>0.007691</td><td>-0.839695</td><td>0.691321</td><td>-0.00693</td><td>0.302254</td><td>-0.867949</td><td>0.295432</td><td>0.281222</td><td>0.363977</td><td>0.029722</td><td>0.379238</td><td>0.651947</td><td>-1.258663</td><td>1.451046</td><td>0.673701</td><td>0.73948</td><td>-0.279643</td><td>0.698398</td><td>0.066064</td><td>0.660564</td><td>-1.259132</td><td>0.981152</td><td>-1.028614</td><td>-0.045712</td><td>-0.475671</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>-1.96814</td><td>-1.115693</td><td>0.033142</td><td>-1.344751</td><td>-0.543341</td><td>-0.076323</td><td>-0.211837</td><td>-0.478293</td><td>-0.217636</td><td>0.145765</td><td>-1.331024</td><td>0.119196</td><td>0.002533</td><td>-0.561447</td><td>0.05448</td><td>0.00422</td><td>0.019327</td><td>0.790045</td><td>0.015917</td><td>-0.228357</td><td>-0.329444</td><td>-0.169107</td><td>-2.771951</td><td>-1.062265</td><td>-0.246845</td><td>-0.21535</td><td>-0.480388</td><td>-1.460805</td><td>1.496517</td><td>-0.998931</td><td>-0.992053</td><td>1.257294</td><td>-0.334431</td><td>-0.008477</td><td>0.021521</td><td>-2.449189</td><td>1.093991</td><td>&hellip;</td><td>-0.283008</td><td>-0.295676</td><td>-0.26737</td><td>0.043099</td><td>-0.28509</td><td>-0.004691</td><td>-0.004691</td><td>-0.00335</td><td>-0.242449</td><td>-0.2158</td><td>-1.54215</td><td>-1.05603</td><td>-0.024786</td><td>0.509546</td><td>1.476047</td><td>-1.482851</td><td>-1.464848</td><td>0.488219</td><td>1.545948</td><td>-1.46654</td><td>-1.427171</td><td>-0.709411</td><td>-1.470181</td><td>1.328351</td><td>0.459814</td><td>1.925516</td><td>1.338632</td><td>1.381834</td><td>-0.543535</td><td>1.348985</td><td>-0.03714</td><td>-1.139946</td><td>1.362018</td><td>1.432309</td><td>0.455582</td><td>0.029711</td><td>-1.85691</td></tr><tr><td>-1.70836</td><td>-0.869532</td><td>-0.015545</td><td>-1.241043</td><td>-0.206872</td><td>-0.400984</td><td>-0.207813</td><td>1.166677</td><td>-0.335576</td><td>0.141556</td><td>-1.170128</td><td>0.162678</td><td>0.023578</td><td>-0.152517</td><td>0.080209</td><td>0.048139</td><td>0.057744</td><td>2.32708</td><td>0.040351</td><td>-0.323255</td><td>0.204012</td><td>-0.284087</td><td>-1.718454</td><td>0.374419</td><td>-0.225106</td><td>-0.316583</td><td>2.1017</td><td>-1.446151</td><td>1.390184</td><td>-0.346507</td><td>0.030289</td><td>0.818277</td><td>-0.4362</td><td>0.02143</td><td>0.340718</td><td>-2.046823</td><td>0.699786</td><td>&hellip;</td><td>-0.170766</td><td>-0.166887</td><td>-0.154509</td><td>0.018265</td><td>-0.156789</td><td>0.024597</td><td>0.024597</td><td>0.01073</td><td>-0.110283</td><td>-0.082167</td><td>-1.492885</td><td>-0.789841</td><td>-0.008477</td><td>0.531188</td><td>4.144293</td><td>-1.123518</td><td>-1.453697</td><td>0.503802</td><td>3.791299</td><td>-1.456301</td><td>-1.395617</td><td>-0.68776</td><td>-1.446778</td><td>1.359627</td><td>0.333251</td><td>2.365141</td><td>1.369433</td><td>1.410887</td><td>0.361639</td><td>1.378128</td><td>-0.006543</td><td>-1.049528</td><td>2.475384</td><td>1.524135</td><td>0.308651</td><td>0.00552</td><td>-1.507814</td></tr><tr><td>-1.547518</td><td>-0.963822</td><td>0.020784</td><td>-1.080271</td><td>0.105217</td><td>-0.067506</td><td>-0.218394</td><td>1.493208</td><td>-1.039451</td><td>0.141422</td><td>-0.624688</td><td>0.13518</td><td>0.049707</td><td>0.037228</td><td>0.100481</td><td>0.07362</td><td>0.09134</td><td>1.947745</td><td>0.056703</td><td>-0.951019</td><td>0.508251</td><td>-0.974819</td><td>-1.768716</td><td>1.074763</td><td>-0.027022</td><td>-0.351408</td><td>1.895022</td><td>-1.420101</td><td>0.613906</td><td>0.266544</td><td>-0.332149</td><td>0.489066</td><td>-0.445044</td><td>0.027328</td><td>0.35058</td><td>-2.403541</td><td>0.431322</td><td>&hellip;</td><td>-0.05274</td><td>-0.050915</td><td>-0.035895</td><td>-0.04864</td><td>-0.042202</td><td>0.0</td><td>0.0</td><td>0.042943</td><td>0.013415</td><td>0.039093</td><td>-1.458768</td><td>-0.892023</td><td>0.02143</td><td>0.357499</td><td>3.792241</td><td>-0.7788</td><td>-1.451631</td><td>0.326401</td><td>2.929815</td><td>-1.465609</td><td>-1.391209</td><td>-0.679369</td><td>-1.426131</td><td>1.390869</td><td>0.390423</td><td>2.136362</td><td>1.398071</td><td>1.439649</td><td>0.580624</td><td>1.409553</td><td>-0.046906</td><td>-0.96927</td><td>2.26241</td><td>1.567311</td><td>-0.050363</td><td>0.027104</td><td>-1.175144</td></tr><tr><td>-1.447757</td><td>-1.476177</td><td>-0.049767</td><td>-1.001616</td><td>0.269588</td><td>0.445854</td><td>-0.304137</td><td>1.586509</td><td>-1.46205</td><td>0.14727</td><td>-0.728888</td><td>-0.023395</td><td>0.053843</td><td>0.317731</td><td>0.106147</td><td>0.084324</td><td>0.102986</td><td>1.501789</td><td>0.065393</td><td>-1.430824</td><td>1.146053</td><td>-1.434283</td><td>-1.65314</td><td>0.931555</td><td>-0.115491</td><td>-0.330579</td><td>1.271166</td><td>-1.414736</td><td>0.108802</td><td>0.021706</td><td>-0.035344</td><td>-0.439081</td><td>-0.440282</td><td>0.026983</td><td>0.48243</td><td>-2.462531</td><td>-0.378354</td><td>&hellip;</td><td>0.02136</td><td>0.041744</td><td>0.036118</td><td>0.0</td><td>0.050104</td><td>0.000707</td><td>0.000707</td><td>0.030855</td><td>0.04767</td><td>0.075274</td><td>-1.416233</td><td>-1.141376</td><td>0.027328</td><td>0.020897</td><td>2.598391</td><td>-0.548398</td><td>-1.481715</td><td>-0.015513</td><td>1.763104</td><td>-1.489823</td><td>-1.412691</td><td>-0.798053</td><td>-1.442874</td><td>1.419367</td><td>0.590982</td><td>0.947132</td><td>1.426985</td><td>1.468707</td><td>0.813362</td><td>1.436473</td><td>0.008007</td><td>-0.89995</td><td>0.95295</td><td>1.55111</td><td>-0.109264</td><td>-0.022066</td><td>-0.952475</td></tr><tr><td>-1.121106</td><td>-1.202997</td><td>-0.05389</td><td>-0.972412</td><td>-0.826888</td><td>0.650817</td><td>-0.465301</td><td>1.576089</td><td>-1.461391</td><td>0.15231</td><td>-0.905234</td><td>-0.190578</td><td>-0.04147</td><td>0.707635</td><td>0.111567</td><td>0.090581</td><td>0.10964</td><td>0.462027</td><td>0.069267</td><td>-1.436085</td><td>0.828896</td><td>-1.445251</td><td>-1.536763</td><td>0.72931</td><td>0.076886</td><td>-0.556108</td><td>0.638186</td><td>-1.422123</td><td>0.026034</td><td>0.095548</td><td>-0.391523</td><td>-1.475849</td><td>-0.253499</td><td>0.02355</td><td>0.556443</td><td>-2.380152</td><td>-1.388063</td><td>&hellip;</td><td>0.054427</td><td>0.075225</td><td>0.068792</td><td>-0.050118</td><td>0.082702</td><td>0.0</td><td>0.0</td><td>0.023381</td><td>0.07268</td><td>0.101087</td><td>-1.391373</td><td>-1.118125</td><td>0.026983</td><td>-0.040407</td><td>-0.022851</td><td>-0.384021</td><td>-1.506174</td><td>-0.081978</td><td>0.370864</td><td>-1.51846</td><td>-1.437231</td><td>-0.867022</td><td>-1.466035</td><td>1.44833</td><td>0.850331</td><td>-1.196419</td><td>1.452338</td><td>1.494605</td><td>0.020149</td><td>1.465752</td><td>0.001378</td><td>-0.846894</td><td>0.688942</td><td>1.49789</td><td>-0.198327</td><td>0.001819</td><td>-0.801319</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (32, 100)\n",
       "\n",
       " column_0   column_1   column_2   column_3     column_96  column_97  column_98  column_9 \n",
       " ---        ---        ---        ---           ---        ---        ---        9        \n",
       " f32        f32        f32        f32           f32        f32        f32        ---      \n",
       "                                                                                 f32      \n",
       "\n",
       " -0.600796  -0.914647  0.030829   -0.708012    1.093586   -0.052384  0.030002   0.118704 \n",
       " -0.331305  -0.799951  0.04221    -0.609153    1.074553   -0.389326  0.056125   -0.12066 \n",
       "                                                                                 3        \n",
       " -0.672513  -0.636656  0.046717   -0.600888    1.060013   -0.580176  0.024639   -0.25885 \n",
       "                                                                                 4        \n",
       " -0.669441  -0.958308  0.041761   -0.694853    1.030099   -0.786002  -0.005382  -0.33182 \n",
       "                                                                                 5        \n",
       " 0.514794   -1.189119  0.022192   -0.588277    0.981152   -1.028614  -0.045712  -0.47567 \n",
       "                                                                                 1        \n",
       "                                                                                 \n",
       " -1.96814   -1.115693  0.033142   -1.344751    1.432309   0.455582   0.029711   -1.85691 \n",
       " -1.70836   -0.869532  -0.015545  -1.241043    1.524135   0.308651   0.00552    -1.50781 \n",
       "                                                                                 4        \n",
       " -1.547518  -0.963822  0.020784   -1.080271    1.567311   -0.050363  0.027104   -1.17514 \n",
       "                                                                                 4        \n",
       " -1.447757  -1.476177  -0.049767  -1.001616    1.55111    -0.109264  -0.022066  -0.95247 \n",
       "                                                                                 5        \n",
       " -1.121106  -1.202997  -0.05389   -0.972412    1.49789    -0.198327  0.001819   -0.80131 \n",
       "                                                                                 9        \n",
       ""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrain_ds = SequentialDataset(df_valid_retrain, df_valid_current_y_retrain)\n",
    "pl.DataFrame(next(iter(retrain_ds))[0][4].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c17a086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (32, 101)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date_id</th><th>LME_CA_Close_vol_20</th><th>US_Stock_SPTL_adj_close_log_ret_return_lag_3</th><th>target_334</th><th>target_334_return_lag_0</th><th>FX_AUDUSD_log_ret_return_lag_3</th><th>US_Stock_VGLT_adj_close_log_ret_return_lag_3</th><th>FX_USDCHF_log_ret_return_lag_4</th><th>US_Stock_CCJ_adj_high_log_ret</th><th>US_Stock_CCJ_adj_high_log_ret_return_lag_0</th><th>US_Stock_VGK_adj_high_log_ret_return_lag_4</th><th>US_Stock_URA_adj_high_log_ret_return_lag_1</th><th>US_Stock_IGSB_adj_high_log_ret_return_lag_4</th><th>US_Stock_OKE_adj_close_vol_5</th><th>target_353</th><th>target_353_return_lag_0</th><th>US_Stock_VEA_adj_high_log_ret_return_lag_4</th><th>FX_GBPAUD_log_ret_return_lag_2</th><th>target_352_return_lag_5</th><th>US_Stock_EWY_adj_high_log_ret_return_lag_4</th><th>US_Stock_VALE_adj_high_log_ret_return_lag_4</th><th>target_342_return_lag_4</th><th>US_Stock_EFA_adj_high_log_ret_return_lag_4</th><th>US_Stock_GLD_adj_high_log_ret_return_lag_1</th><th>target_282_return_lag_4</th><th>US_Stock_EWJ_adj_high_log_ret_return_lag_4</th><th>US_Stock_RSP_adj_low_log_ret_return_lag_4</th><th>US_Stock_BNDX_adj_close_log_ret_return_lag_5</th><th>target_391_return_lag_5</th><th>US_Stock_EWY_adj_open_log_ret_return_lag_4</th><th>US_Stock_VALE_adj_open_log_ret_return_lag_4</th><th>US_Stock_BNDX_adj_low_vol_5</th><th>FX_USDJPY_log_ret_return_lag_3</th><th>US_Stock_IGSB_adj_high_log_ret_return_lag_2</th><th>US_Stock_SPTL_adj_low_log_ret_return_lag_3</th><th>LME_PB_Close_log_ret_return_lag_3</th><th>target_293_return_lag_3</th><th>&hellip;</th><th>US_Stock_YINN_adj_open_log_ret_return_lag_4</th><th>US_Stock_VCSH_adj_low_log_ret_return_lag_4</th><th>US_Stock_KMI_adj_low_log_ret_return_lag_1</th><th>US_Stock_BKR_adj_close_log_ret</th><th>US_Stock_BKR_adj_close_log_ret_return_lag_0</th><th>US_Stock_BP_adj_high_log_ret_return_lag_4</th><th>US_Stock_BKR_adj_close_log_ret_return_lag_4</th><th>US_Stock_CVX_adj_low_log_ret_return_lag_4</th><th>US_Stock_SPIB_adj_open_log_ret_return_lag_5</th><th>US_Stock_SHY_adj_low_log_ret_return_lag_3</th><th>US_Stock_YINN_adj_low_log_ret_return_lag_4</th><th>US_Stock_BSV_adj_low_log_ret_return_lag_3</th><th>US_Stock_CCJ_adj_close_log_ret_return_lag_4</th><th>target_355_return_lag_2</th><th>US_Stock_FXI_adj_low_log_ret_return_lag_4</th><th>US_Stock_BNDX_adj_low_log_ret_return_lag_3</th><th>JPX_Gold_Standard_Futures_Low_log_ret</th><th>JPX_Gold_Standard_Futures_Low_log_ret_return_lag_0</th><th>target_396_return_lag_5</th><th>US_Stock_EWJ_adj_low_log_ret_return_lag_4</th><th>JPX_Platinum_Standard_Futures_Close_log_ret_return_lag_4</th><th>JPX_Gold_Mini_Futures_Low_log_ret</th><th>JPX_Gold_Mini_Futures_Low_log_ret_return_lag_0</th><th>US_Stock_VGLT_adj_low_log_ret_return_lag_3</th><th>US_Stock_VWO_adj_close_log_ret</th><th>US_Stock_VWO_adj_close_log_ret_return_lag_0</th><th>FX_USDJPY_sma_5_ratio</th><th>US_Stock_MPC_adj_close_vol_20</th><th>JPX_Platinum_Standard_Futures_Low_log_ret</th><th>JPX_Platinum_Standard_Futures_Low_log_ret_return_lag_0</th><th>US_Stock_TECK_adj_open_log_ret</th><th>US_Stock_TECK_adj_open_log_ret_return_lag_0</th><th>US_Stock_LYB_adj_low_log_ret_return_lag_4</th><th>FX_AUDCAD_log_ret_return_lag_2</th><th>US_Stock_EWZ_adj_low_log_ret</th><th>US_Stock_EWZ_adj_low_log_ret_return_lag_0</th><th>US_Stock_FXI_adj_open_log_ret_return_lag_5</th></tr><tr><td>i64</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>&hellip;</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td></tr></thead><tbody><tr><td>1492</td><td>-0.511044</td><td>0.173562</td><td>-0.033112</td><td>-0.033112</td><td>1.533998</td><td>0.234326</td><td>1.023351</td><td>-2.188488</td><td>-2.188488</td><td>-0.663893</td><td>-2.809797</td><td>-0.074308</td><td>1.073938</td><td>-0.022895</td><td>-0.022895</td><td>-0.519893</td><td>-0.154931</td><td>-0.023216</td><td>-0.240476</td><td>-0.428015</td><td>0.023171</td><td>-0.455075</td><td>-2.550884</td><td>-0.03502</td><td>0.215218</td><td>-0.611564</td><td>-0.224083</td><td>0.027589</td><td>-0.173789</td><td>-0.15977</td><td>-0.008468</td><td>-0.157877</td><td>0.954141</td><td>-1.089345</td><td>1.486385</td><td>-0.037555</td><td>&hellip;</td><td>0.161582</td><td>-1.27943</td><td>-2.127097</td><td>-0.125517</td><td>-0.125517</td><td>0.738528</td><td>0.674825</td><td>0.727557</td><td>0.234615</td><td>0.581767</td><td>-0.088414</td><td>-0.000459</td><td>-0.356188</td><td>0.032037</td><td>-0.118028</td><td>-1.095543</td><td>-2.022053</td><td>-2.022053</td><td>0.026556</td><td>0.024484</td><td>0.329002</td><td>-2.035179</td><td>-2.035179</td><td>-1.097548</td><td>-1.284433</td><td>-1.284433</td><td>-0.23474</td><td>-0.97524</td><td>-2.99021</td><td>-2.99021</td><td>-1.378905</td><td>-1.378905</td><td>0.077251</td><td>1.309706</td><td>-1.638625</td><td>-1.638625</td><td>-0.276408</td></tr><tr><td>1493</td><td>-0.312962</td><td>0.101284</td><td>-0.016007</td><td>-0.016007</td><td>0.02741</td><td>0.097307</td><td>-1.096954</td><td>-1.957091</td><td>-1.957091</td><td>0.558081</td><td>-1.654699</td><td>-0.45129</td><td>1.165789</td><td>-0.038005</td><td>-0.038005</td><td>0.381586</td><td>0.147775</td><td>0.013702</td><td>0.254698</td><td>0.281916</td><td>-0.008606</td><td>0.401456</td><td>-0.312567</td><td>0.010238</td><td>-0.371636</td><td>0.65835</td><td>-0.746884</td><td>0.027614</td><td>-0.65199</td><td>-0.364345</td><td>0.081926</td><td>0.015751</td><td>-1.643458</td><td>1.254481</td><td>-0.893886</td><td>0.039143</td><td>&hellip;</td><td>-0.678046</td><td>0.356886</td><td>-0.603849</td><td>-2.20992</td><td>-2.20992</td><td>0.192488</td><td>0.377309</td><td>0.222367</td><td>-0.321926</td><td>0.819286</td><td>-0.350581</td><td>1.449974</td><td>0.765745</td><td>0.063368</td><td>-0.373034</td><td>1.393187</td><td>-1.040454</td><td>-1.040454</td><td>0.032931</td><td>-0.255789</td><td>-1.269722</td><td>-1.061661</td><td>-1.061661</td><td>1.242034</td><td>-0.444676</td><td>-0.444676</td><td>-0.329176</td><td>-0.468101</td><td>-1.172184</td><td>-1.172184</td><td>-0.616882</td><td>-0.616882</td><td>0.241764</td><td>-0.687277</td><td>-0.346604</td><td>-0.346604</td><td>0.125275</td></tr><tr><td>1494</td><td>-0.080944</td><td>-1.401107</td><td>0.009703</td><td>0.009703</td><td>-1.289793</td><td>-1.485501</td><td>0.17002</td><td>0.772106</td><td>0.772106</td><td>0.922436</td><td>-1.991366</td><td>0.954141</td><td>-0.212533</td><td>-0.039579</td><td>-0.039579</td><td>0.578321</td><td>1.378222</td><td>0.057261</td><td>0.453239</td><td>0.766982</td><td>-0.005012</td><td>0.561487</td><td>-0.524425</td><td>-0.015106</td><td>-0.730568</td><td>0.091347</td><td>-0.320084</td><td>0.009189</td><td>1.373382</td><td>1.248753</td><td>0.08233</td><td>0.421902</td><td>-0.549668</td><td>-1.361923</td><td>-0.915021</td><td>0.047038</td><td>&hellip;</td><td>1.210833</td><td>0.913825</td><td>-0.450888</td><td>0.25656</td><td>0.25656</td><td>-0.375223</td><td>-1.910836</td><td>-0.696194</td><td>-1.614106</td><td>-0.785505</td><td>0.702767</td><td>-1.214816</td><td>-1.681274</td><td>0.041355</td><td>0.708703</td><td>-1.167691</td><td>0.024832</td><td>0.024832</td><td>0.034368</td><td>-1.076575</td><td>1.646769</td><td>0.064427</td><td>0.064427</td><td>-1.371962</td><td>0.376309</td><td>0.376309</td><td>-0.597941</td><td>-0.037599</td><td>-0.070476</td><td>-0.070476</td><td>-0.706681</td><td>-0.706681</td><td>-0.04385</td><td>-1.06743</td><td>-0.217693</td><td>-0.217693</td><td>-0.675665</td></tr><tr><td>1495</td><td>-0.023508</td><td>-1.782801</td><td>0.014661</td><td>0.014661</td><td>-1.058782</td><td>-1.810959</td><td>0.550438</td><td>0.951175</td><td>0.951175</td><td>-1.918621</td><td>1.034759</td><td>-1.643458</td><td>-0.15478</td><td>0.006229</td><td>0.006229</td><td>-1.997166</td><td>0.264006</td><td>0.046996</td><td>-1.335938</td><td>-0.496724</td><td>0.001405</td><td>-1.952236</td><td>-0.335142</td><td>-0.047527</td><td>-1.464685</td><td>-1.4316</td><td>0.367832</td><td>0.004801</td><td>-1.476968</td><td>-0.339328</td><td>-0.725367</td><td>-0.601002</td><td>-0.075723</td><td>-1.991876</td><td>-1.027569</td><td>0.079335</td><td>&hellip;</td><td>-0.56788</td><td>-1.111427</td><td>0.032094</td><td>0.518414</td><td>0.518414</td><td>-1.231643</td><td>-1.203543</td><td>-1.104401</td><td>1.425645</td><td>-0.469598</td><td>-0.243584</td><td>-0.84612</td><td>-2.57197</td><td>0.04096</td><td>-0.284574</td><td>-1.158817</td><td>-0.937016</td><td>-0.937016</td><td>0.036215</td><td>-1.057066</td><td>-0.526809</td><td>-0.938397</td><td>-0.938397</td><td>-1.98672</td><td>1.300727</td><td>1.300727</td><td>0.067641</td><td>0.207813</td><td>-0.981562</td><td>-0.981562</td><td>1.040375</td><td>1.040375</td><td>-1.239508</td><td>0.998291</td><td>-0.73244</td><td>-0.73244</td><td>1.204968</td></tr><tr><td>1496</td><td>-0.087871</td><td>1.149808</td><td>0.03664</td><td>0.03664</td><td>0.386462</td><td>1.17987</td><td>0.652076</td><td>-0.445462</td><td>-0.445462</td><td>-1.495579</td><td>1.244723</td><td>-0.549668</td><td>1.883954</td><td>0.024519</td><td>0.024519</td><td>-1.797999</td><td>-0.453268</td><td>0.01164</td><td>-0.660344</td><td>-0.620099</td><td>-0.016771</td><td>-1.679115</td><td>0.833929</td><td>-0.060649</td><td>-1.061548</td><td>-1.184204</td><td>-1.078687</td><td>0.054209</td><td>-0.969331</td><td>-1.025861</td><td>-0.669347</td><td>-0.206991</td><td>0.767765</td><td>0.563411</td><td>-0.029653</td><td>-0.011485</td><td>&hellip;</td><td>-1.334306</td><td>-1.70101</td><td>0.590575</td><td>1.586124</td><td>1.586124</td><td>-1.752598</td><td>-0.125517</td><td>0.091868</td><td>-1.071626</td><td>0.379826</td><td>-1.151713</td><td>0.225303</td><td>-0.697418</td><td>-0.012123</td><td>-1.184028</td><td>-0.032004</td><td>-0.067485</td><td>-0.067485</td><td>0.097434</td><td>-2.147739</td><td>-2.552266</td><td>-0.067455</td><td>-0.067455</td><td>0.593217</td><td>-0.649562</td><td>-0.649562</td><td>-0.423256</td><td>0.270992</td><td>0.005253</td><td>0.005253</td><td>-0.059475</td><td>-0.059475</td><td>0.415142</td><td>0.624903</td><td>1.460085</td><td>1.460085</td><td>-0.57029</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>1519</td><td>-0.960542</td><td>-0.804647</td><td>-0.092706</td><td>-0.092706</td><td>-0.459747</td><td>-0.776192</td><td>-1.49708</td><td>1.472992</td><td>1.472992</td><td>1.215112</td><td>-0.260884</td><td>1.932424</td><td>1.12496</td><td>-0.04171</td><td>-0.04171</td><td>1.758383</td><td>0.87107</td><td>-0.046063</td><td>2.226214</td><td>0.373327</td><td>-0.016667</td><td>1.730929</td><td>-0.455415</td><td>0.032896</td><td>2.10888</td><td>2.74635</td><td>1.170535</td><td>0.062301</td><td>2.11611</td><td>0.43695</td><td>-0.091291</td><td>0.567891</td><td>0.328829</td><td>-1.097286</td><td>0.347239</td><td>-0.002168</td><td>&hellip;</td><td>1.041632</td><td>1.615778</td><td>-0.450138</td><td>0.258001</td><td>0.258001</td><td>-0.860806</td><td>-0.226697</td><td>1.13054</td><td>2.63622</td><td>-0.476386</td><td>1.224654</td><td>-0.849864</td><td>-1.087795</td><td>-0.004142</td><td>1.255055</td><td>-1.095059</td><td>-0.434664</td><td>-0.434664</td><td>0.030135</td><td>1.969524</td><td>0.00129</td><td>-0.417545</td><td>-0.417545</td><td>-1.124315</td><td>-0.994871</td><td>-0.994871</td><td>0.820444</td><td>-0.314741</td><td>-1.865203</td><td>-1.865203</td><td>-0.468029</td><td>-0.468029</td><td>1.419654</td><td>-0.695311</td><td>-0.387257</td><td>-0.387257</td><td>0.790494</td></tr><tr><td>1520</td><td>-1.009707</td><td>1.275387</td><td>-0.086301</td><td>-0.086301</td><td>-1.20894</td><td>1.294288</td><td>0.116636</td><td>-0.003635</td><td>-0.003635</td><td>-0.293308</td><td>0.560836</td><td>-1.368154</td><td>0.325644</td><td>-0.023029</td><td>-0.023029</td><td>-0.009204</td><td>0.591108</td><td>0.011156</td><td>3.997054</td><td>0.021724</td><td>0.000381</td><td>-0.653924</td><td>0.091066</td><td>0.043951</td><td>-1.373731</td><td>-0.306983</td><td>1.229914</td><td>0.001028</td><td>4.169611</td><td>0.363435</td><td>-0.084726</td><td>0.340282</td><td>-0.072727</td><td>1.213365</td><td>0.303273</td><td>0.009394</td><td>&hellip;</td><td>0.971451</td><td>-1.060379</td><td>-0.717461</td><td>0.658274</td><td>0.658274</td><td>0.346922</td><td>-0.441287</td><td>0.298863</td><td>2.205608</td><td>0.009822</td><td>0.495286</td><td>0.144293</td><td>-1.397655</td><td>-0.024666</td><td>0.521689</td><td>1.254137</td><td>-0.378165</td><td>-0.378165</td><td>-0.029611</td><td>-0.815667</td><td>-0.597778</td><td>-0.388318</td><td>-0.388318</td><td>1.198703</td><td>0.732307</td><td>0.732307</td><td>0.576604</td><td>-0.243254</td><td>-1.013675</td><td>-1.013675</td><td>-0.052795</td><td>-0.052795</td><td>-0.617059</td><td>-0.370447</td><td>0.851918</td><td>0.851918</td><td>0.949652</td></tr><tr><td>1521</td><td>-1.011257</td><td>1.426783</td><td>-0.044674</td><td>-0.044674</td><td>-0.513088</td><td>1.401085</td><td>0.173089</td><td>1.623886</td><td>1.623886</td><td>-0.772582</td><td>-0.254333</td><td>0.328829</td><td>-0.814267</td><td>0.013741</td><td>0.013741</td><td>-1.378117</td><td>0.199196</td><td>0.017206</td><td>-1.731424</td><td>-0.352703</td><td>0.02319</td><td>-1.056797</td><td>-1.222856</td><td>-0.001765</td><td>-1.382764</td><td>-0.254804</td><td>-1.169921</td><td>0.000309</td><td>-2.278234</td><td>-0.138944</td><td>-0.806531</td><td>0.436513</td><td>-0.2722</td><td>0.937505</td><td>0.071482</td><td>0.056605</td><td>&hellip;</td><td>-0.794188</td><td>0.042823</td><td>0.285481</td><td>0.169348</td><td>0.169348</td><td>-1.453235</td><td>-1.799926</td><td>-1.542321</td><td>-1.098614</td><td>0.170228</td><td>-0.71455</td><td>0.23943</td><td>0.299185</td><td>0.044893</td><td>-0.748403</td><td>1.051235</td><td>-0.431901</td><td>-0.431901</td><td>-0.038768</td><td>-1.475391</td><td>-1.42638</td><td>-0.423974</td><td>-0.423974</td><td>0.943803</td><td>0.175352</td><td>0.175352</td><td>0.37805</td><td>-0.228861</td><td>-1.26094</td><td>-1.26094</td><td>0.808815</td><td>0.808815</td><td>-0.731644</td><td>-1.005815</td><td>-0.202861</td><td>-0.202861</td><td>0.991861</td></tr><tr><td>1522</td><td>-0.971642</td><td>-1.937158</td><td>-0.01281</td><td>-0.01281</td><td>-0.907734</td><td>-1.965644</td><td>-0.135633</td><td>-0.325053</td><td>-0.325053</td><td>0.516323</td><td>1.927278</td><td>-0.072727</td><td>-0.118373</td><td>0.032001</td><td>0.032001</td><td>-0.06372</td><td>0.08499</td><td>-0.016568</td><td>-1.374882</td><td>-0.389102</td><td>0.025895</td><td>0.089888</td><td>0.049997</td><td>-0.028327</td><td>-1.309477</td><td>-0.272417</td><td>1.553362</td><td>-0.036827</td><td>-0.863802</td><td>-0.601661</td><td>0.167284</td><td>0.442838</td><td>-0.372716</td><td>-1.892661</td><td>-0.033468</td><td>0.065226</td><td>&hellip;</td><td>-0.118559</td><td>0.442015</td><td>0.651406</td><td>0.331279</td><td>0.331279</td><td>-0.617521</td><td>-0.238593</td><td>-0.410269</td><td>-0.127812</td><td>-1.052224</td><td>0.029804</td><td>-0.987344</td><td>-0.917916</td><td>0.042114</td><td>0.004822</td><td>-0.964702</td><td>-0.255183</td><td>-0.255183</td><td>-0.000566</td><td>-1.452903</td><td>-0.797426</td><td>-0.228269</td><td>-0.228269</td><td>-1.868637</td><td>2.466483</td><td>2.466483</td><td>-0.752396</td><td>-0.239898</td><td>0.131002</td><td>0.131002</td><td>3.295977</td><td>3.295977</td><td>0.00009</td><td>-0.162136</td><td>2.252719</td><td>2.252719</td><td>-0.816642</td></tr><tr><td>1523</td><td>-0.922929</td><td>0.433636</td><td>0.049016</td><td>0.049016</td><td>0.00168</td><td>0.497715</td><td>0.924426</td><td>-0.032955</td><td>-0.032955</td><td>0.79922</td><td>0.446941</td><td>-0.2722</td><td>0.141341</td><td>0.063113</td><td>0.063113</td><td>0.739945</td><td>0.303056</td><td>-0.028534</td><td>0.076154</td><td>0.120696</td><td>0.039051</td><td>0.76831</td><td>1.33247</td><td>-0.038351</td><td>0.962099</td><td>-0.555465</td><td>0.803042</td><td>-0.000578</td><td>0.080885</td><td>-0.037665</td><td>0.54333</td><td>0.127965</td><td>-0.36693</td><td>1.154178</td><td>-0.403463</td><td>0.039382</td><td>&hellip;</td><td>-0.226693</td><td>-1.103711</td><td>1.311952</td><td>-0.115139</td><td>-0.115139</td><td>0.135005</td><td>0.258001</td><td>-0.172701</td><td>0.453807</td><td>-0.234558</td><td>-0.834158</td><td>-0.177022</td><td>1.900266</td><td>-0.013813</td><td>-0.861969</td><td>0.108819</td><td>1.051589</td><td>1.051589</td><td>0.010187</td><td>0.606042</td><td>-1.440397</td><td>1.071974</td><td>1.071974</td><td>1.158091</td><td>0.71458</td><td>0.71458</td><td>-0.106039</td><td>-0.328504</td><td>2.349288</td><td>2.349288</td><td>-2.324606</td><td>-2.324606</td><td>0.558654</td><td>0.577771</td><td>0.232377</td><td>0.232377</td><td>-0.158632</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (32, 101)\n",
       "\n",
       " date_id  LME_CA_Clo  US_Stock_  target_33    FX_AUDCAD  US_Stock_  US_Stock_  US_Stock_ \n",
       " ---      se_vol_20   SPTL_adj_  4             _log_ret_  EWZ_adj_l  EWZ_adj_l  FXI_adj_o \n",
       " i64      ---         close_log  ---           return_la  ow_log_re  ow_log_re  pen_log_r \n",
       "          f32         _re       f32           g_2        t          t_r       et_      \n",
       "                      ---                      ---        ---        ---        ---       \n",
       "                      f32                      f32        f32        f32        f32       \n",
       "\n",
       " 1492     -0.511044   0.173562   -0.033112    1.309706   -1.638625  -1.638625  -0.276408 \n",
       " 1493     -0.312962   0.101284   -0.016007    -0.687277  -0.346604  -0.346604  0.125275  \n",
       " 1494     -0.080944   -1.401107  0.009703     -1.06743   -0.217693  -0.217693  -0.675665 \n",
       " 1495     -0.023508   -1.782801  0.014661     0.998291   -0.73244   -0.73244   1.204968  \n",
       " 1496     -0.087871   1.149808   0.03664      0.624903   1.460085   1.460085   -0.57029  \n",
       "                                                                                 \n",
       " 1519     -0.960542   -0.804647  -0.092706    -0.695311  -0.387257  -0.387257  0.790494  \n",
       " 1520     -1.009707   1.275387   -0.086301    -0.370447  0.851918   0.851918   0.949652  \n",
       " 1521     -1.011257   1.426783   -0.044674    -1.005815  -0.202861  -0.202861  0.991861  \n",
       " 1522     -0.971642   -1.937158  -0.01281     -0.162136  2.252719   2.252719   -0.816642 \n",
       " 1523     -0.922929   0.433636   0.049016     0.577771   0.232377   0.232377   -0.158632 \n",
       ""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.filter(pl.col(CONFIG.DATE_COL).is_in(range(date_id - max(CONFIG.LAG_SEQ_LEN.values()) + 1, date_id + 1))).select(\n",
    "    [CONFIG.DATE_COL] + CONFIG.LAG_FEATURES[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29acf208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6851f123b7c8453aa110bc26554ee068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/304 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for date_id in tqdm(dates_valid):\n",
    "    df_valid_date = train_x.filter(pl.col(CONFIG.DATE_COL).is_in(range(date_id - max(CONFIG.LAG_SEQ_LEN.values()) + 1, date_id + 1))).drop(\n",
    "        CONFIG.DATE_COL\n",
    "    )\n",
    "    valid_lags = {lag: df_valid_date.select(features).to_numpy().astype(np.float64) for lag, features in CONFIG.LAG_FEATURES.items()}\n",
    "    valid_lags = {lag: valid_lags[lag][-seq_len:] for lag, seq_len in CONFIG.LAG_SEQ_LEN.items()}\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5054189",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 2831456 into shape (8,173)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Step 1: Prepare the numpy input\u001b[39;00m\n\u001b[0;32m      2\u001b[0m X_np \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      3\u001b[0m     df_valid\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[0;32m      4\u001b[0m         CONFIG\u001b[38;5;241m.\u001b[39mDATE_COL,\n\u001b[0;32m      5\u001b[0m     )\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m173\u001b[39m)\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Step 2: Create leaf tensor on CUDA with requires_grad=True\u001b[39;00m\n\u001b[0;32m     11\u001b[0m X_sample \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_np, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m#  single step\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 2831456 into shape (8,173)"
     ]
    }
   ],
   "source": [
    "# Step 1: Prepare the numpy input\n",
    "X_np = (\n",
    "    df_valid.drop(\n",
    "        CONFIG.DATE_COL,\n",
    "    )\n",
    "    .to_numpy()\n",
    "    .reshape(-1, 8, 173)\n",
    ")\n",
    "\n",
    "# Step 2: Create leaf tensor on CUDA with requires_grad=True\n",
    "X_sample = torch.tensor(X_np, dtype=torch.float32, device=\"cuda\", requires_grad=True)  #  single step\n",
    "\n",
    "# Step 3: Put model into train mode to support CuDNN RNN backward\n",
    "model_real.model.train()\n",
    "\n",
    "# Step 4: Forward and backward pass\n",
    "output = model_real.model(X_sample)  # (B, ...)\n",
    "loss = output.mean()\n",
    "loss.backward()\n",
    "\n",
    "# Step 5: Retrieve input gradients  now this will work without warnings\n",
    "grads = X_sample.grad.detach().cpu().numpy()  # shape: (B, T, F)\n",
    "importance = np.abs(grads).mean(axis=(0, 1))  # shape: (F,)\n",
    "importance /= importance.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd26ec87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (173, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>column_0</th><th>feats</th></tr><tr><td>f32</td><td>str</td></tr></thead><tbody><tr><td>0.011635</td><td>&quot;US_Stock_GLD_adj_low_log_ret_r</td></tr><tr><td>0.011528</td><td>&quot;US_Stock_NUGT_adj_close_log_re</td></tr><tr><td>0.010101</td><td>&quot;US_Stock_IGSB_adj_open_log_ret</td></tr><tr><td>0.00998</td><td>&quot;US_Stock_RY_adj_high_log_ret&quot;</td></tr><tr><td>0.009936</td><td>&quot;US_Stock_MPC_adj_low&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>0.003558</td><td>&quot;US_Stock_VCIT_adj_open_log_ret</td></tr><tr><td>0.003554</td><td>&quot;US_Stock_AMP_adj_low_log_ret&quot;</td></tr><tr><td>0.003419</td><td>&quot;US_Stock_LYB_adj_close_log_ret</td></tr><tr><td>0.003284</td><td>&quot;US_Stock_VGK_adj_close_log_ret&quot;</td></tr><tr><td>0.003199</td><td>&quot;US_Stock_MPC_adj_volume&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (173, 2)\n",
       "\n",
       " column_0  feats                           \n",
       " ---       ---                             \n",
       " f32       str                             \n",
       "\n",
       " 0.011635  US_Stock_GLD_adj_low_log_ret_r \n",
       " 0.011528  US_Stock_NUGT_adj_close_log_re \n",
       " 0.010101  US_Stock_IGSB_adj_open_log_ret \n",
       " 0.00998   US_Stock_RY_adj_high_log_ret    \n",
       " 0.009936  US_Stock_MPC_adj_low            \n",
       "                                         \n",
       " 0.003558  US_Stock_VCIT_adj_open_log_ret \n",
       " 0.003554  US_Stock_AMP_adj_low_log_ret    \n",
       " 0.003419  US_Stock_LYB_adj_close_log_ret \n",
       " 0.003284  US_Stock_VGK_adj_close_log_ret  \n",
       " 0.003199  US_Stock_MPC_adj_volume         \n",
       ""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.DataFrame(importance).with_columns(pl.Series(name=\"feats\", values=df_train.drop(CONFIG.DATE_COL).columns)).sort(by=\"column_0\", descending=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edmund",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
