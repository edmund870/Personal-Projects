{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09d651b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "import copy\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from numba import njit, prange\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.profiler import profile, ProfilerActivity, record_function\n",
    "\n",
    "from CONFIG import CONFIG\n",
    "from PREPROCESSOR import PREPROCESSOR\n",
    "from FEATURE_ENGINEERING import FEATURE_ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60095aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Desktop\\Personal-Projects\\Kaggle\\MITSUI&CO. Commodity Prediction Challenge\\FEATURE_ENGINEERING.py:619: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  for col in all.columns\n"
     ]
    }
   ],
   "source": [
    "train_x = pl.scan_csv(CONFIG.TRAIN_X_PATH).filter(pl.col(\"date_id\") <= CONFIG.MAX_TRAIN_DATE)\n",
    "\n",
    "train_x = PREPROCESSOR(df=train_x)\n",
    "train_x.clean()\n",
    "train_x = train_x.transform().lazy()\n",
    "\n",
    "\n",
    "train_x = FEATURE_ENGINEERING(df=train_x)\n",
    "train_x = train_x.create_all_features().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f580b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_correlation_sharpe(targets, predictions) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the rank correlation between predictions and target values,\n",
    "    and returns its Sharpe ratio (mean / standard deviation).\n",
    "\n",
    "    :param merged_df: DataFrame containing prediction columns (starting with 'prediction_')\n",
    "                      and target columns (starting with 'target_')\n",
    "    :return: Sharpe ratio of the rank correlation\n",
    "    :raises ZeroDivisionError: If the standard deviation is zero\n",
    "    \"\"\"\n",
    "    correlations = []\n",
    "\n",
    "    for pred_row, target_row in zip(predictions, targets):\n",
    "        if np.std(pred_row) == 0 or np.std(target_row) == 0:\n",
    "            raise ZeroDivisionError(\"Zero standard deviation in a row.\")\n",
    "\n",
    "        rho = spearmanr(pred_row, target_row).correlation\n",
    "        correlations.append(rho)\n",
    "\n",
    "    daily_rank_corrs = np.array(correlations)\n",
    "    std_dev = daily_rank_corrs.std(ddof=0)\n",
    "    if std_dev == 0:\n",
    "        raise ZeroDivisionError(\"Denominator is zero, unable to compute Sharpe ratio.\")\n",
    "\n",
    "    sharpe_ratio = daily_rank_corrs.mean() / std_dev\n",
    "    return float(sharpe_ratio)\n",
    "\n",
    "\n",
    "class RankCorrelationSharpeRatioLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch Loss Function for Rank Correlation Sharpe Ratio\n",
    "\n",
    "    Loss = -Sharpe_Ratio = -(mean_daily_rank_correlation / std_daily_rank_correlation)\n",
    "\n",
    "    Goal: Minimize loss = Maximize Sharpe ratio of daily rank correlations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, regularization_eps=1e-8, ranking_temperature=1.0):\n",
    "        super(RankCorrelationSharpeRatioLoss, self).__init__()\n",
    "        self.regularization_eps = regularization_eps\n",
    "        self.ranking_temperature = ranking_temperature\n",
    "\n",
    "    def forward(self, predictions, targets, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: (batch_size, n_features) - daily predictions for each security\n",
    "            targets: (batch_size, n_features) - daily targets for each security\n",
    "            mask: (batch_size, n_features) - optional mask for valid securities (1=valid, 0=invalid)\n",
    "                  Use this to handle trading halts, holidays, delistings\n",
    "\n",
    "        Returns:\n",
    "            loss: scalar tensor = -sharpe_ratio (negative for minimization)\n",
    "        \"\"\"\n",
    "        batch_size, n_features = predictions.shape\n",
    "\n",
    "        if batch_size == 1:\n",
    "            # Cannot compute Sharpe ratio with single sample\n",
    "            return torch.tensor(0.0, device=predictions.device, requires_grad=True)\n",
    "\n",
    "        # Apply mask if provided (handle missing/invalid securities)\n",
    "        if mask is not None:\n",
    "            predictions = predictions * mask\n",
    "            targets = targets * mask\n",
    "\n",
    "        # Compute daily rank correlations for each sample in batch\n",
    "        daily_rank_correlations = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            pred_day = predictions[i]  # (n_features,)\n",
    "            target_day = targets[i]  # (n_features,)\n",
    "\n",
    "            if mask is not None:\n",
    "                # Only consider valid securities for this day\n",
    "                valid_mask = mask[i] > 0\n",
    "                if valid_mask.sum() < 2:  # Need at least 2 for correlation\n",
    "                    continue\n",
    "                pred_day = pred_day[valid_mask]\n",
    "                target_day = target_day[valid_mask]\n",
    "\n",
    "            # Check for zero variance (all predictions/targets are the same)\n",
    "            if self._has_zero_variance(pred_day) or self._has_zero_variance(target_day):\n",
    "                continue  # Skip this day\n",
    "\n",
    "            # Compute differentiable rank correlation\n",
    "            rank_corr = self._differentiable_rank_correlation(pred_day, target_day)\n",
    "            daily_rank_correlations.append(rank_corr)\n",
    "\n",
    "        if len(daily_rank_correlations) == 0:\n",
    "            # No valid correlations computed\n",
    "            return torch.tensor(0.0, device=predictions.device, requires_grad=True)\n",
    "\n",
    "        # Stack daily correlations\n",
    "        daily_correlations = torch.stack(daily_rank_correlations)\n",
    "\n",
    "        # Compute Sharpe ratio = mean / std\n",
    "        mean_corr = daily_correlations.mean()\n",
    "        std_corr = daily_correlations.std() + self.regularization_eps\n",
    "\n",
    "        sharpe_ratio = mean_corr / std_corr\n",
    "\n",
    "        # Return negative Sharpe ratio (minimize loss = maximize Sharpe ratio)\n",
    "        return -sharpe_ratio\n",
    "\n",
    "    def _has_zero_variance(self, x):\n",
    "        \"\"\"Check if tensor has zero variance (all values are the same)\"\"\"\n",
    "        return (x.max() - x.min()) < self.regularization_eps\n",
    "\n",
    "    def _differentiable_rank_correlation(self, x, y):\n",
    "        \"\"\"\n",
    "        Compute differentiable rank correlation between two 1D tensors\n",
    "        Uses Pearson correlation of differentiable ranks\n",
    "        \"\"\"\n",
    "        # Get differentiable ranks\n",
    "        x_ranks = self._differentiable_ranking(x)\n",
    "        y_ranks = self._differentiable_ranking(y)\n",
    "\n",
    "        # Compute Pearson correlation of ranks\n",
    "        return self._pearson_correlation(x_ranks, y_ranks)\n",
    "\n",
    "    def _differentiable_ranking(self, x):\n",
    "        \"\"\"\n",
    "        Convert values to differentiable ranks using sigmoid approximation\n",
    "        \"\"\"\n",
    "        n = x.shape[0]\n",
    "\n",
    "        # Create pairwise comparison matrix\n",
    "        x_expanded = x.unsqueeze(1)  # (n, 1)\n",
    "        x_transposed = x.unsqueeze(0)  # (1, n)\n",
    "\n",
    "        # Pairwise differences\n",
    "        pairwise_diff = x_expanded - x_transposed  # (n, n)\n",
    "\n",
    "        # Sigmoid approximation of step function\n",
    "        sigmoid_comparisons = torch.sigmoid(pairwise_diff / self.ranking_temperature)\n",
    "\n",
    "        # Sum to get ranks (how many elements this element is greater than)\n",
    "        ranks = sigmoid_comparisons.sum(dim=1)\n",
    "\n",
    "        return ranks\n",
    "\n",
    "    def _pearson_correlation(self, x, y):\n",
    "        \"\"\"Compute Pearson correlation coefficient\"\"\"\n",
    "        # Center the data\n",
    "        x_centered = x - x.mean()\n",
    "        y_centered = y - y.mean()\n",
    "\n",
    "        # Compute correlation\n",
    "        numerator = (x_centered * y_centered).sum()\n",
    "\n",
    "        # Compute standard deviations with regularization\n",
    "        x_std = torch.sqrt((x_centered**2).sum() + self.regularization_eps)\n",
    "        y_std = torch.sqrt((y_centered**2).sum() + self.regularization_eps)\n",
    "\n",
    "        correlation = numerator / (x_std * y_std)\n",
    "\n",
    "        # Clamp to valid range\n",
    "        return torch.clamp(correlation, -1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74425038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_collate_fn(batch: list) -> dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Collate function for DataLoader to flatten the batch.\n",
    "\n",
    "    Args:\n",
    "        batch (list): List of tuples containing tensors.\n",
    "\n",
    "        tuple[torch.Tensor]: Flattened tensors (type, instr, X, y).\n",
    "    \"\"\"\n",
    "    continuous_batch = torch.stack([item[\"continuous\"] for item in batch])\n",
    "    categorical_batch = torch.stack([item[\"categorical\"] for item in batch])\n",
    "    target_batch = torch.stack([item[\"target\"] for item in batch])\n",
    "\n",
    "    return {\n",
    "        \"continuous\": continuous_batch,\n",
    "        \"categorical\": categorical_batch,\n",
    "        \"target\": target_batch,\n",
    "    }\n",
    "\n",
    "\n",
    "class CustomTensorDataset(Dataset):\n",
    "    def __init__(self, date_id, x, type, instr, y, sequence_length=10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: DataFrame or numpy array of shape (275561, 70)\n",
    "                  First 68 columns: continuous features\n",
    "                  Last 2 columns: categorical features (should be integers)\n",
    "            target: DataFrame or numpy array of shape (1927, 424)\n",
    "                    Target values for each date\n",
    "            sequence_length: Number of time steps to use for prediction\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_features = x.shape[-1]\n",
    "        # Split continuous and categorical features\n",
    "        self.dates = torch.tensor(date_id, dtype=torch.int16)\n",
    "        self.continuous_data = torch.tensor(x, dtype=torch.float16)\n",
    "        self.continuous_data = torch.nan_to_num(self.continuous_data, 0)\n",
    "        self.continuous_data = (self.continuous_data - self.continuous_data.mean(dim=(0, 1), keepdim=True)) / self.continuous_data.std(\n",
    "            dim=(0, 1), keepdim=True\n",
    "        )\n",
    "        self.continuous_data = torch.nan_to_num(self.continuous_data, 0)\n",
    "        if torch.isnan(self.continuous_data).any() or torch.isinf(self.continuous_data).any():\n",
    "            print(\"Input contains NaN or Inf values!\")\n",
    "        self.type = torch.tensor(type, dtype=torch.long)\n",
    "        self.instr = torch.tensor(instr, dtype=torch.long)\n",
    "        self.categorical_data = torch.stack((self.type, self.instr), dim=-1)\n",
    "        self.y = torch.tensor(y, dtype=torch.float16)\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "        self.n_tickers = CONFIG.N_INSTR\n",
    "        self.unique_date, self.inverse_indices, self.counts = torch.unique(self.dates, return_inverse=True, return_counts=True)\n",
    "\n",
    "        self.n_unique_dates = self.counts.sum().item()\n",
    "\n",
    "        self.n_categorical_features = 2\n",
    "\n",
    "        # Reshape data to (unique_date, n_tickers, n_features)\n",
    "        self.reshaped_continuous = self.continuous_data.view(self.n_unique_dates, self.n_tickers, self.num_features)\n",
    "        self.reshaped_categorical = self.categorical_data.view(self.n_unique_dates, self.n_tickers, self.n_categorical_features)\n",
    "\n",
    "    def __len__(self):\n",
    "        # We can create sequences from date indices, leaving room for sequence_length\n",
    "        return max(0, self.n_unique_dates - self.sequence_length)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get sequence of dates starting from idx\n",
    "        continuous_seq = self.reshaped_continuous[idx : idx + self.sequence_length]  # (seq_len, 143, 68)\n",
    "        categorical_seq = self.reshaped_categorical[idx : idx + self.sequence_length]  # (seq_len, 143, 2)\n",
    "        # Target is the 424-dimensional vector for the next date\n",
    "        if idx + self.sequence_length < self.n_unique_dates:\n",
    "            target = self.y[idx + self.sequence_length]  # (424,)\n",
    "        else:\n",
    "            # For the last sequence, use the last available target\n",
    "            target = self.y[-1]  # (424,)\n",
    "\n",
    "        return {\n",
    "            \"continuous\": continuous_seq,\n",
    "            \"categorical\": categorical_seq,\n",
    "            \"target\": target,\n",
    "        }\n",
    "\n",
    "\n",
    "# --- Model Definition ---\n",
    "class TIMESERIES_NN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_numerical_features: int,\n",
    "        unique_cats=[\n",
    "            CONFIG.N_TYPES,\n",
    "            CONFIG.N_INSTR,\n",
    "        ],\n",
    "        embedding_dims=[\n",
    "            min(50, max(CONFIG.N_TYPES // 2, 4)),\n",
    "            min(50, max(CONFIG.N_INSTR // 2, 4)),\n",
    "        ],\n",
    "        n_tickers: int = 143,\n",
    "        sequence_length=30,\n",
    "        hidden_size=128,\n",
    "        rnn_layers=2,\n",
    "        dropout=0.2,\n",
    "        output_dim=424,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Store these for use in forward method\n",
    "        self.n_tickers = n_tickers\n",
    "        self.seq_len = sequence_length\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Create embedding layers for each categorical feature using ModuleDict\n",
    "        self.embeddings = nn.ModuleList(\n",
    "            [nn.Embedding(vocab_size, embed_dim, padding_idx=0) for vocab_size, embed_dim in zip(unique_cats, embedding_dims)]\n",
    "        )\n",
    "\n",
    "        self.input_dim = num_numerical_features + sum(embedding_dims)\n",
    "        self.input_projection = nn.Linear(self.input_dim, hidden_size)\n",
    "\n",
    "        # GRU layers for temporal modeling\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=rnn_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if rnn_layers > 1 else 0,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "\n",
    "        # Attention mechanism for ticker aggregation\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=8, dropout=dropout, batch_first=True)\n",
    "\n",
    "        # Layer normalization\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        # Output projection layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, output_dim),\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.GRU):\n",
    "                for name, param in module.named_parameters():\n",
    "                    if \"weight_ih\" in name:\n",
    "                        nn.init.xavier_uniform_(param.data)\n",
    "                    elif \"weight_hh\" in name:\n",
    "                        nn.init.orthogonal_(param.data)\n",
    "                    elif \"bias\" in name:\n",
    "                        nn.init.zeros_(param.data)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Extract continuous and categorical data\n",
    "        continuous = batch[\"continuous\"]  # (batch_size, seq_len, n_tickers, n_continuous_features)\n",
    "        categorical = batch[\"categorical\"]  # (batch_size, seq_len, n_tickers, n_categorical_features)\n",
    "\n",
    "        batch_size, seq_len, num_tickers, input_size = continuous.shape\n",
    "\n",
    "        # Process embeddings if categorical features are provided\n",
    "        embedded_features = []\n",
    "\n",
    "        for i in range(len(CONFIG.CAT_COLS)):\n",
    "            # Extract i-th categorical feature: (batch_size, seq_len, num_tickers)\n",
    "            cat_feature = categorical[:, :, :, i]\n",
    "            # Apply embedding: (batch_size, seq_len, num_tickers, embed_dim)\n",
    "            embedded = self.embeddings[i](cat_feature)\n",
    "            embedded_features.append(embedded)\n",
    "\n",
    "        # Concatenate all embedded features along the last dimension\n",
    "        embedded_concat = torch.cat(embedded_features, dim=-1)\n",
    "        # Concatenate with continuous features\n",
    "        x_combined = torch.cat([continuous, embedded_concat], dim=-1)\n",
    "\n",
    "        # Reshape to process all tickers across all time steps\n",
    "        # (batch_size * seq_len, num_tickers, total_input_size)\n",
    "        x_reshaped = x_combined.view(batch_size * seq_len, num_tickers, self.input_dim)\n",
    "\n",
    "        # Project input features\n",
    "        # (batch_size * seq_len, num_tickers, hidden_size)\n",
    "        x_proj = self.input_projection(x_reshaped)\n",
    "        x_proj = torch.relu(x_proj)\n",
    "\n",
    "        # Apply attention across tickers for each time step\n",
    "        # This aggregates information across all tickers at each time point\n",
    "        attn_output, _ = self.attention(x_proj, x_proj, x_proj)\n",
    "        x_proj = self.layer_norm1(x_proj + attn_output)\n",
    "\n",
    "        # Aggregate across tickers (mean pooling)\n",
    "        # (batch_size * seq_len, hidden_size)\n",
    "        x_agg = torch.mean(x_proj, dim=1)\n",
    "\n",
    "        # Reshape back to sequence format for GRU\n",
    "        # (batch_size, seq_len, hidden_size)\n",
    "        x_seq = x_agg.view(batch_size, seq_len, self.hidden_size)\n",
    "\n",
    "        # Apply GRU for temporal modeling\n",
    "        self.gru.flatten_parameters()\n",
    "        gru_out, hidden = self.gru(x_seq)\n",
    "\n",
    "        # Use the last output from the sequence\n",
    "        # (batch_size, hidden_size)\n",
    "        last_output = gru_out[:, -1, :]\n",
    "        last_output = self.layer_norm2(last_output)\n",
    "\n",
    "        # Generate final prediction\n",
    "        # (batch_size, output_size)\n",
    "        output = self.fc_layers(last_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ab79cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr: float = 0.001,\n",
    "        batch_size: int = 1,\n",
    "        epochs: int = 100,\n",
    "        early_stopping_patience: int = 10,\n",
    "        early_stopping: bool = True,\n",
    "        lr_patience: int = 2,\n",
    "        lr_factor: float = 0.5,\n",
    "        lr_refit: float = 0.001,\n",
    "        random_seed: int = CONFIG.RANDOM_STATE,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.early_stopping = early_stopping\n",
    "        self.lr_patience = lr_patience\n",
    "        self.lr_factor = lr_factor\n",
    "        self.lr_refit = lr_refit\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = None\n",
    "        self.optimizer = None\n",
    "        self.best_epoch = None\n",
    "        self.features = None\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def fit(self, train_set: tuple, val_set: tuple, verbose: bool = False) -> None:\n",
    "        \"\"\"Fit the model on the training set and validate on the validation set.\n",
    "\n",
    "        Args:\n",
    "            train_set (tuple): A tuple containing input data, targets for training.\n",
    "            val_set (tuple): A tuple containing input data, targets for validation.\n",
    "            verbose (bool, optional): If True, prints training progress. Defaults to False.\n",
    "        \"\"\"\n",
    "        torch.manual_seed(self.random_seed)\n",
    "\n",
    "        train_dataset = CustomTensorDataset(*train_set)\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=flatten_collate_fn,\n",
    "            pin_memory=True,\n",
    "            # num_workers=2,\n",
    "            # persistent_workers=True,\n",
    "            # prefetch_factor=2,\n",
    "        )\n",
    "\n",
    "        val_dataset = CustomTensorDataset(*val_set)\n",
    "        val_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=flatten_collate_fn,\n",
    "            pin_memory=True,\n",
    "            # num_workers=6,\n",
    "            # persistent_workers=True,\n",
    "            # prefetch_factor=2,\n",
    "        )\n",
    "\n",
    "        self.model = TIMESERIES_NN(num_numerical_features=train_dataset.num_features, **self.kwargs).to(self.device)\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, weight_decay=0.01)\n",
    "\n",
    "        train_sharpes, val_sharpes = [], []\n",
    "        if verbose:\n",
    "            print(f\"Device: {self.device}\")\n",
    "            print(f\"{'Epoch':^5} | {'Train Loss':^10} | {'Val Loss':^8} | {'Train sharpe':^9} | {'Val sharpe':^7} | {'LR':^7}\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "        min_val_sharpe = -np.inf\n",
    "        best_epoch = 0\n",
    "        no_improvement = 0\n",
    "        best_model = None\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss, train_sharpe = self.train_one_epoch(train_dataloader, verbose)\n",
    "            val_loss, val_sharpe = self.validate_one_epoch(val_dataloader, verbose)\n",
    "            lr_last = self.optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "            train_sharpes.append(train_sharpe)\n",
    "            val_sharpes.append(val_sharpe)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"{epoch + 1:^5} | {train_loss:^10.4f} | {val_loss:^8.4f} | {train_sharpe:^9.4f} | {val_sharpe:^7.4f} | {lr_last:^7.5f}\")\n",
    "\n",
    "            if val_sharpe > min_val_sharpe:\n",
    "                min_val_sharpe = val_sharpe\n",
    "                best_model = copy.deepcopy(self.model.state_dict())\n",
    "                no_improvement = 0\n",
    "                best_epoch = epoch\n",
    "            else:\n",
    "                no_improvement += 1\n",
    "\n",
    "            if self.early_stopping:\n",
    "                if no_improvement >= self.early_stopping_patience + 1:\n",
    "                    self.best_epoch = best_epoch + 1\n",
    "                    if verbose:\n",
    "                        print(f\"Early stopping on epoch {best_epoch + 1}. Best score: {min_val_sharpe:.4f}\")\n",
    "                    break\n",
    "\n",
    "        # Load the best model\n",
    "        if self.early_stopping:\n",
    "            self.model.load_state_dict(best_model)\n",
    "\n",
    "    def train_one_epoch(self, train_dataloader: DataLoader, verbose: bool) -> None:\n",
    "        \"\"\"Train the model for one epoch.\n",
    "\n",
    "        Args:\n",
    "            train_dataloader (DataLoader): DataLoader for the training set.\n",
    "            verbose (bool): If True, shows progress using tqdm.\n",
    "\n",
    "        Returns:\n",
    "            tuple[float, float]: A tuple containing:\n",
    "                - Train loss (float).\n",
    "                - Spearman Sharpe for the training set (float).\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        y_total, preds_total = [], []\n",
    "\n",
    "        itr = train_dataloader\n",
    "\n",
    "        for batch in itr:\n",
    "            batch = {key: value.to(self.device) for key, value in batch.items()}\n",
    "            y_batch = batch[\"target\"]\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            with torch.autocast(device_type=\"cuda\"):\n",
    "                out_y = self.model(batch)\n",
    "                out_y = torch.nan_to_num(out_y)\n",
    "                loss = self.criterion(out_y, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            y_total.append(batch[\"target\"])\n",
    "            preds_total.append(out_y.detach())\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        y_total = torch.cat(y_total).cpu().numpy()\n",
    "        preds_total = torch.cat(preds_total).cpu().numpy()\n",
    "\n",
    "        train_sharpe = rank_correlation_sharpe(y_total, preds_total)\n",
    "        train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        return train_loss, train_sharpe\n",
    "\n",
    "    def validate_one_epoch(self, val_dataloader: DataLoader, verbose=False) -> None:\n",
    "        \"\"\"Validate the model on the validation set.\n",
    "\n",
    "        Args:\n",
    "            val_dataloader (DataLoader): DataLoader for the validation set.\n",
    "            verbose (bool, optional): If True, shows progress using tqdm. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            tuple[float, float]: A tuple containing:\n",
    "                - Validation loss (float).\n",
    "                - Spearman Sharpe for the validation set (float).\n",
    "        \"\"\"\n",
    "        model = copy.deepcopy(self.model)\n",
    "\n",
    "        losses, all_y, all_preds = [], [], []\n",
    "\n",
    "        itr = val_dataloader\n",
    "        for batch in itr:\n",
    "            batch = {key: value.to(self.device) for key, value in batch.items()}\n",
    "            y_batch = batch[\"target\"]\n",
    "\n",
    "            # Predict\n",
    "            with torch.inference_mode():\n",
    "                model.eval()\n",
    "                preds_batch = model(batch)\n",
    "                preds_batch = torch.nan_to_num(preds_batch)\n",
    "                loss = self.criterion(\n",
    "                    preds_batch,\n",
    "                    y_batch,\n",
    "                )\n",
    "                losses.append(loss.item())\n",
    "\n",
    "                all_y.append(y_batch)\n",
    "                all_preds.append(preds_batch)\n",
    "\n",
    "            # Update weights\n",
    "            if self.lr_refit > 0:\n",
    "                optimizer = torch.optim.AdamW(model.parameters(), lr=self.lr_refit, weight_decay=0.01)\n",
    "                optimizer.zero_grad()\n",
    "                model.train()\n",
    "                with torch.autocast(device_type=\"cuda\"):\n",
    "                    out_y = model(batch)\n",
    "                    out_y = torch.nan_to_num(out_y)\n",
    "                    loss = self.criterion(out_y, y_batch)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        all_y = torch.cat(all_y).cpu().numpy()\n",
    "        all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "        loss = np.mean(losses)\n",
    "        sharpe = rank_correlation_sharpe(all_y, all_preds)\n",
    "\n",
    "        return loss, sharpe\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        type_data: np.array,\n",
    "        instr_data: np.array,\n",
    "        X: np.array,\n",
    "        y: np.array,\n",
    "        n_times: int,\n",
    "    ):\n",
    "        \"\"\"Update the model with new data.\n",
    "\n",
    "        Args:\n",
    "            X (np.array): Input data.\n",
    "            y (np.array): Target variable.\n",
    "            n_times (int): Number of time steps.\n",
    "        \"\"\"\n",
    "        if self.lr_refit == 0.0:\n",
    "            return\n",
    "\n",
    "        N, K, num_features = X.shape\n",
    "        continuous_data = torch.tensor(X, dtype=torch.float16, device=self.device)\n",
    "        continuous_data = torch.nan_to_num(continuous_data, 0)\n",
    "        continuous_data = (continuous_data - continuous_data.mean(dim=(0, 1), keepdim=True)) / continuous_data.std(dim=(0, 1), keepdim=True)\n",
    "        continuous_data = torch.nan_to_num(continuous_data, 0)\n",
    "        type = torch.tensor(type_data, dtype=torch.long, device=self.device)\n",
    "        instr = torch.tensor(instr_data, dtype=torch.long, device=self.device)\n",
    "        categorical_data = torch.stack((type, instr), dim=-1)\n",
    "        y = torch.tensor(y, dtype=torch.float16, device=self.device)\n",
    "\n",
    "        # Reshape data to (unique_date, n_tickers, n_features)\n",
    "        reshaped_continuous = continuous_data.view(N, CONFIG.N_INSTR, num_features)\n",
    "        reshaped_categorical = categorical_data.view(N, CONFIG.N_INSTR, len(CONFIG.CAT_COLS))\n",
    "\n",
    "        batch = {\n",
    "            \"continuous\": reshaped_continuous.unsqueeze(0),\n",
    "            \"categorical\": reshaped_categorical.unsqueeze(0),\n",
    "        }\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr_refit, weight_decay=0.01)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.autocast(device_type=\"cuda\"):\n",
    "            out_y = self.model(batch)\n",
    "            out_y = torch.nan_to_num(out_y)\n",
    "            loss = self.criterion(out_y, y)\n",
    "            loss = torch.nan_to_num(loss, nan=-10.0)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        type_data: np.array,\n",
    "        instr_data: np.array,\n",
    "        X: np.array,\n",
    "        n_times: int = None,\n",
    "        hidden: torch.Tensor | list | None = None,\n",
    "    ) -> tuple[np.array, torch.Tensor | list]:\n",
    "        \"\"\"Predict the target variable for the given input data.\n",
    "\n",
    "        Args:\n",
    "            X (np.array): Input data.\n",
    "            n_times (int, optional): Number of time steps. Defaults to None.\n",
    "            hidden (torch.Tensor or list or None, optional): Initial hidden state. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            tuple[np.array, torch.Tensor or list]: A tuple containing:\n",
    "                - Predictions (np.array).\n",
    "                - Hidden state (torch.Tensor or list).\n",
    "        \"\"\"\n",
    "        N, K, num_features = X.shape\n",
    "        continuous_data = torch.tensor(X, dtype=torch.float16, device=self.device)\n",
    "        continuous_data = torch.nan_to_num(continuous_data, 0)\n",
    "        continuous_data = (continuous_data - continuous_data.mean(dim=(0, 1), keepdim=True)) / continuous_data.std(dim=(0, 1), keepdim=True)\n",
    "        continuous_data = torch.nan_to_num(continuous_data, 0)\n",
    "        type = torch.tensor(type_data, dtype=torch.long, device=self.device)\n",
    "        instr = torch.tensor(instr_data, dtype=torch.long, device=self.device)\n",
    "        categorical_data = torch.stack((type, instr), dim=-1)\n",
    "\n",
    "        # Reshape data to (unique_date, n_tickers, n_features)\n",
    "        reshaped_continuous = continuous_data.view(N, CONFIG.N_INSTR, num_features)\n",
    "        reshaped_categorical = categorical_data.view(N, CONFIG.N_INSTR, len(CONFIG.CAT_COLS))\n",
    "\n",
    "        batch = {\n",
    "            \"continuous\": reshaped_continuous.unsqueeze(0),\n",
    "            \"categorical\": reshaped_categorical.unsqueeze(0),\n",
    "        }\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.inference_mode():\n",
    "            preds = self.model(batch)\n",
    "            preds = torch.nan_to_num(preds)\n",
    "        return preds.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cce0fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Desktop\\Personal-Projects\\Kaggle\\MITSUI&CO. Commodity Prediction Challenge\\FEATURE_ENGINEERING.py:619: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  for col in all.columns\n"
     ]
    }
   ],
   "source": [
    "# --- Prepare DataLoader ---\n",
    "# Create the dataset\n",
    "\n",
    "train_x = pl.scan_csv(CONFIG.TRAIN_X_PATH).filter(pl.col(\"date_id\") <= CONFIG.MAX_TRAIN_DATE)\n",
    "train_y = pl.scan_csv(CONFIG.TRAIN_Y_PATH).filter(pl.col(\"date_id\") <= CONFIG.MAX_TRAIN_DATE).fill_null(0).collect()\n",
    "\n",
    "train_x = PREPROCESSOR(df=train_x)\n",
    "train_x.clean()\n",
    "train_x = train_x.transform().lazy()\n",
    "\n",
    "train_x = FEATURE_ENGINEERING(df=train_x)\n",
    "train_x = train_x.create_all_features().select(set(CONFIG.IMPT_COLS + CONFIG.CAT_COLS + [CONFIG.DATE_COL])).collect()\n",
    "\n",
    "cats = train_x.select(CONFIG.CAT_COLS)\n",
    "type_encoder = LabelEncoder().fit(train_x.select(CONFIG.CAT_COLS[0]).to_numpy().flatten())\n",
    "instr_encoder = LabelEncoder().fit(train_x.select(CONFIG.CAT_COLS[1]).to_numpy().flatten())\n",
    "\n",
    "NN_model = NN(\n",
    "    **CONFIG.NN_PARAMS,\n",
    "    batch_size=CONFIG.BATCH_SIZE,\n",
    "    lr=0.0001,\n",
    "    lr_refit=0.0005,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797745ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Fold 0--------------------\n",
      "Train dates from 367 to 1826\n",
      "Valid dates from 2 to 366\n",
      "Device: cuda:0\n",
      "Epoch | Train Loss | Val Loss | Train sharpe | Val sharpe |   LR   \n",
      "------------------------------------------------------------\n",
      "  1   |   0.0853   |  0.0020  |  -0.0093  | -0.1048 | 0.00010\n",
      "  2   |   0.0063   |  0.0018  |  -0.0079  | -0.0306 | 0.00010\n",
      "  3   |   0.0031   |  0.0017  |  0.0424   | -0.0838 | 0.00010\n",
      "  4   |   0.0022   |  0.0017  |  0.0045   | -0.0139 | 0.00010\n",
      "  5   |   0.0018   |  0.0017  |  0.0162   | 0.0567  | 0.00010\n",
      "  6   |   0.0016   |  0.0017  |  0.0182   | -0.0426 | 0.00010\n",
      "  7   |   0.0014   |  0.0017  |  -0.0178  | 0.0069  | 0.00010\n",
      "  8   |   0.0013   |  0.0017  |  0.0808   | 0.0193  | 0.00010\n",
      "  9   |   0.0012   |  0.0017  |  0.1019   | 0.0424  | 0.00010\n",
      " 10   |   0.0012   |  0.0017  |  0.1007   | 0.0288  | 0.00010\n",
      " 11   |   0.0012   |  0.0017  |  0.0680   | 0.0440  | 0.00010\n",
      " 12   |   0.0011   |  0.0017  |  0.0565   | 0.0677  | 0.00010\n",
      " 13   |   0.0011   |  0.0017  |  0.1482   | 0.0463  | 0.00010\n",
      " 14   |   0.0011   |  0.0017  |  0.1247   | 0.0613  | 0.00010\n",
      " 15   |   0.0011   |  0.0017  |  0.1428   | 0.0659  | 0.00010\n",
      " 16   |   0.0011   |  0.0017  |  0.1562   | 0.0807  | 0.00010\n",
      " 17   |   0.0011   |  0.0017  |  0.1277   | 0.0619  | 0.00010\n",
      " 18   |   0.0011   |  0.0017  |  0.1452   | 0.0633  | 0.00010\n",
      " 19   |   0.0011   |  0.0017  |  0.1581   | 0.0650  | 0.00010\n",
      " 20   |   0.0010   |  0.0017  |  0.1665   | 0.0610  | 0.00010\n",
      " 21   |   0.0010   |  0.0017  |  0.1746   | 0.0925  | 0.00010\n",
      " 22   |   0.0010   |  0.0017  |  0.1413   | 0.0809  | 0.00010\n",
      " 23   |   0.0010   |  0.0017  |  0.1561   | 0.0764  | 0.00010\n",
      " 24   |   0.0010   |  0.0017  |  0.1564   | 0.0916  | 0.00010\n",
      " 25   |   0.0010   |  0.0017  |  0.1616   | 0.0868  | 0.00010\n",
      " 26   |   0.0010   |  0.0017  |  0.1641   | 0.0863  | 0.00010\n",
      " 27   |   0.0010   |  0.0017  |  0.1400   | 0.0865  | 0.00010\n",
      " 28   |   0.0010   |  0.0017  |  0.1699   | 0.1016  | 0.00010\n",
      " 29   |   0.0010   |  0.0017  |  0.1498   | 0.0866  | 0.00010\n",
      " 30   |   0.0010   |  0.0017  |  0.1796   | 0.0683  | 0.00010\n",
      " 31   |   0.0010   |  0.0017  |  0.1567   | 0.0961  | 0.00010\n",
      " 32   |   0.0010   |  0.0017  |  0.1418   | 0.0839  | 0.00010\n",
      " 33   |   0.0010   |  0.0017  |  0.1803   | 0.0843  | 0.00010\n",
      " 34   |   0.0010   |  0.0017  |  0.1812   | 0.0769  | 0.00010\n",
      " 35   |   0.0010   |  0.0017  |  0.1615   | 0.0783  | 0.00010\n",
      " 36   |   0.0010   |  0.0017  |  0.1869   | 0.0798  | 0.00010\n",
      " 37   |   0.0010   |  0.0017  |  0.1838   | 0.0633  | 0.00010\n",
      " 38   |   0.0010   |  0.0017  |  0.1522   | 0.0730  | 0.00010\n",
      " 39   |   0.0010   |  0.0017  |  0.1733   | 0.0876  | 0.00010\n",
      "Early stopping on epoch 28. Best score: 0.1016\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e486855c65941ac99dce61cd7d126a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharpe: 0.15766\n",
      "--------------------Fold 1--------------------\n",
      "Train dates from 2 to 1826\n",
      "Valid dates from 367 to 731\n",
      "Device: cuda:0\n",
      "Epoch | Train Loss | Val Loss | Train sharpe | Val sharpe |   LR   \n",
      "------------------------------------------------------------\n",
      "  1   |   0.0847   |  0.0010  |  -0.0238  | -0.0386 | 0.00010\n",
      "  2   |   0.0061   |  0.0007  |  -0.0532  | 0.1570  | 0.00010\n",
      "  3   |   0.0029   |  0.0006  |  0.0093   | 0.0251  | 0.00010\n",
      "  4   |   0.0020   |  0.0006  |  0.0053   | 0.1483  | 0.00010\n",
      "  5   |   0.0016   |  0.0006  |  0.0385   | 0.0731  | 0.00010\n",
      "  6   |   0.0013   |  0.0006  |  0.0323   | 0.0739  | 0.00010\n",
      "  7   |   0.0012   |  0.0006  |  0.0254   | 0.1049  | 0.00010\n",
      "  8   |   0.0011   |  0.0006  |  0.0927   | 0.1071  | 0.00010\n",
      "  9   |   0.0010   |  0.0006  |  0.0895   | 0.1092  | 0.00010\n",
      " 10   |   0.0010   |  0.0006  |  0.1213   | 0.1128  | 0.00010\n",
      " 11   |   0.0009   |  0.0006  |  0.0722   | 0.0738  | 0.00010\n",
      " 12   |   0.0009   |  0.0006  |  0.1021   | 0.0973  | 0.00010\n",
      " 13   |   0.0009   |  0.0006  |  0.1526   | 0.0916  | 0.00010\n",
      "Early stopping on epoch 2. Best score: 0.1570\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89680dac775c40b694ba043316e7d4cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharpe: 0.12193\n",
      "--------------------Fold 2--------------------\n",
      "Train dates from 2 to 1826\n",
      "Valid dates from 732 to 1096\n",
      "Device: cuda:0\n",
      "Epoch | Train Loss | Val Loss | Train sharpe | Val sharpe |   LR   \n",
      "------------------------------------------------------------\n",
      "  1   |   0.0853   |  0.0009  |  -0.0594  | -0.0744 | 0.00010\n",
      "  2   |   0.0062   |  0.0007  |  -0.0431  | 0.1221  | 0.00010\n",
      "  3   |   0.0030   |  0.0006  |  -0.0142  | 0.0393  | 0.00010\n",
      "  4   |   0.0021   |  0.0006  |  0.0048   | 0.1152  | 0.00010\n",
      "  5   |   0.0017   |  0.0006  |  0.0220   | 0.0348  | 0.00010\n",
      "  6   |   0.0015   |  0.0006  |  0.0338   | 0.0895  | 0.00010\n",
      "  7   |   0.0013   |  0.0006  |  0.0328   | 0.1250  | 0.00010\n",
      "  8   |   0.0012   |  0.0006  |  0.0861   | 0.1244  | 0.00010\n",
      "  9   |   0.0012   |  0.0006  |  0.1041   | 0.1272  | 0.00010\n",
      " 10   |   0.0011   |  0.0006  |  0.1079   | 0.1135  | 0.00010\n",
      " 11   |   0.0011   |  0.0006  |  0.1182   | 0.0956  | 0.00010\n",
      " 12   |   0.0010   |  0.0006  |  0.1179   | 0.1144  | 0.00010\n",
      " 13   |   0.0010   |  0.0006  |  0.1650   | 0.1185  | 0.00010\n",
      " 14   |   0.0010   |  0.0006  |  0.1066   | 0.1255  | 0.00010\n",
      " 15   |   0.0010   |  0.0006  |  0.1810   | 0.1187  | 0.00010\n",
      " 16   |   0.0010   |  0.0006  |  0.1365   | 0.1247  | 0.00010\n",
      " 17   |   0.0010   |  0.0006  |  0.1420   | 0.1103  | 0.00010\n",
      " 18   |   0.0010   |  0.0006  |  0.1683   | 0.1192  | 0.00010\n",
      " 19   |   0.0010   |  0.0006  |  0.1688   | 0.1027  | 0.00010\n",
      " 20   |   0.0009   |  0.0006  |  0.1765   | 0.1154  | 0.00010\n",
      "Early stopping on epoch 9. Best score: 0.1272\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57fed218f7fa4672a5acefb5e84b2e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/365 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharpe: 0.00880\n",
      "--------------------Fold 3--------------------\n",
      "Train dates from 2 to 1826\n",
      "Valid dates from 1097 to 1461\n",
      "Device: cuda:0\n",
      "Epoch | Train Loss | Val Loss | Train sharpe | Val sharpe |   LR   \n",
      "------------------------------------------------------------\n",
      "  1   |   0.0854   |  0.0009  |  -0.0544  | -0.0560 | 0.00010\n",
      "  2   |   0.0062   |  0.0007  |  -0.0284  | 0.0245  | 0.00010\n",
      "  3   |   0.0031   |  0.0006  |  -0.0387  | 0.0399  | 0.00010\n"
     ]
    }
   ],
   "source": [
    "def make_data(df: pl.DataFrame, type_encoder: LabelEncoder, instr_encoder: LabelEncoder):\n",
    "    type = type_encoder.transform(df.select(CONFIG.CAT_COLS[0]).to_numpy().flatten())\n",
    "    instr = instr_encoder.transform(df.select(CONFIG.CAT_COLS[1]).to_numpy().flatten())\n",
    "    x = df.drop([CONFIG.DATE_COL] + CONFIG.CAT_COLS).to_numpy()\n",
    "\n",
    "    return (\n",
    "        type.reshape(-1, CONFIG.N_INSTR),\n",
    "        instr.reshape(-1, CONFIG.N_INSTR),\n",
    "        x.reshape(-1, CONFIG.N_INSTR, x.shape[1]),\n",
    "    )\n",
    "\n",
    "\n",
    "# test_size = (\n",
    "#     TEST_SIZE\n",
    "#     if len(dates_unique) > TEST_SIZE * (n_splits + 1)\n",
    "#     else len(dates_unique) // (n_splits + 1)\n",
    "# )  # For testing purposes on small samples\n",
    "\n",
    "dates_unique = train_x.select(pl.col(CONFIG.DATE_COL).unique().sort()).to_series().to_numpy()\n",
    "\n",
    "cv = KFold(n_splits=CONFIG.N_FOLDS, shuffle=False)\n",
    "cv_split = cv.split(dates_unique)\n",
    "\n",
    "scores = []\n",
    "models = []\n",
    "for fold, (train_idx, valid_idx) in enumerate(cv_split):\n",
    "    if CONFIG.VERBOSE:\n",
    "        print(\"-\" * 20 + f\"Fold {fold}\" + \"-\" * 20)\n",
    "        print(f\"Train dates from {dates_unique[train_idx].min()} to {dates_unique[train_idx].max()}\")\n",
    "        print(f\"Valid dates from {dates_unique[valid_idx].min()} to {dates_unique[valid_idx].max()}\")\n",
    "\n",
    "    dates_train = dates_unique[train_idx]\n",
    "    dates_valid = dates_unique[valid_idx]\n",
    "\n",
    "    df_train = train_x.filter(pl.col(CONFIG.DATE_COL).is_in(dates_train))\n",
    "\n",
    "    type_train, instr_train, df_train = make_data(df=df_train, type_encoder=type_encoder, instr_encoder=instr_encoder)\n",
    "\n",
    "    df_train_y = train_y.filter(pl.col(CONFIG.DATE_COL).is_in(dates_train)).drop(CONFIG.DATE_COL).to_numpy()\n",
    "\n",
    "    df_valid = train_x.filter(pl.col(CONFIG.DATE_COL).is_in(dates_valid))\n",
    "    type_valid, instr_valid, df_valid_arr = make_data(df=df_valid, type_encoder=type_encoder, instr_encoder=instr_encoder)\n",
    "    df_valid_y = train_y.filter(pl.col(CONFIG.DATE_COL).is_in(dates_train)).drop(CONFIG.DATE_COL).to_numpy()\n",
    "\n",
    "    model_fold = copy.deepcopy(NN_model)\n",
    "\n",
    "    model_fold.fit(\n",
    "        train_set=(\n",
    "            dates_train,\n",
    "            df_train,\n",
    "            type_train,\n",
    "            instr_train,\n",
    "            df_train_y,\n",
    "            CONFIG.SEQ_LEN,\n",
    "        ),\n",
    "        val_set=(\n",
    "            dates_valid,\n",
    "            df_valid_arr,\n",
    "            type_valid,\n",
    "            instr_valid,\n",
    "            df_valid_y,\n",
    "            CONFIG.SEQ_LEN,\n",
    "        ),\n",
    "        verbose=CONFIG.VERBOSE,\n",
    "    )\n",
    "\n",
    "    models.append(model_fold)\n",
    "\n",
    "    torch.save(\n",
    "        model_fold.model.state_dict(),\n",
    "        f\"C:/Users/Admin/Desktop/Personal-Projects/Kaggle/MITSUI&CO. Commodity Prediction Challenge/sample_{fold}.pth\",\n",
    "    )\n",
    "\n",
    "    preds = []\n",
    "    cnt_dates = 0\n",
    "    model_save = copy.deepcopy(model_fold)\n",
    "\n",
    "    for date_id in tqdm(dates_valid):\n",
    "        df_valid_date = df_valid.filter(pl.col(CONFIG.DATE_COL).is_in(range(date_id - CONFIG.SEQ_LEN, date_id)))\n",
    "        avail_dates = df_valid_date[\"date_id\"].unique().len()\n",
    "        if avail_dates < CONFIG.SEQ_LEN:\n",
    "            continue\n",
    "\n",
    "        type_valid_date, instr_valid_date, df_valid_date = make_data(df=df_valid_date, type_encoder=type_encoder, instr_encoder=instr_encoder)\n",
    "\n",
    "        if model_fold.lr_refit and (cnt_dates > 0):\n",
    "            period = range((date_id - 1 - CONFIG.BATCH_SIZE * CONFIG.SEQ_LEN), date_id - 1)\n",
    "            df_upd = train_x.filter(pl.col(CONFIG.DATE_COL).is_in(period))\n",
    "            df_upd_y = train_y.filter(pl.col(CONFIG.DATE_COL).is_in(date_id - 1)).drop(CONFIG.DATE_COL).to_numpy()\n",
    "            type_upd, instr_upd, df_upd = make_data(\n",
    "                df=df_upd,\n",
    "                type_encoder=type_encoder,\n",
    "                instr_encoder=instr_encoder,\n",
    "            )\n",
    "            if len(df_upd) > 0:\n",
    "                model_save.update(type_upd, instr_upd, df_upd, df_upd_y, 1)\n",
    "\n",
    "        preds_i = model_save.predict(type_valid_date, instr_valid_date, df_valid_date, n_times=1)\n",
    "        preds += list(preds_i[-1].reshape(-1, CONFIG.NUM_TARGET_COLUMNS))\n",
    "        cnt_dates += 1\n",
    "    preds = np.array(preds)\n",
    "\n",
    "    score = rank_correlation_sharpe(df_valid_y, preds)\n",
    "    scores.append(score)\n",
    "\n",
    "    print(f\"Sharpe: {score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc93b2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# --- Prepare DataLoader ---\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Create the dataset\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m train_x \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mscan_csv(CONFIG\u001b[38;5;241m.\u001b[39mTRAIN_X_PATH)\n\u001b[0;32m      5\u001b[0m train_x \u001b[38;5;241m=\u001b[39m PREPROCESSOR(df\u001b[38;5;241m=\u001b[39mtrain_x)\n\u001b[0;32m      6\u001b[0m train_x\u001b[38;5;241m.\u001b[39mclean()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pl' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Prepare DataLoader ---\n",
    "# Create the dataset\n",
    "\n",
    "train_x = pl.scan_csv(CONFIG.TRAIN_X_PATH)\n",
    "train_x = PREPROCESSOR(df=train_x)\n",
    "train_x.clean()\n",
    "train_x = train_x.transform().lazy()\n",
    "\n",
    "train_x = FEATURE_ENGINEERING(df=train_x)\n",
    "train_x = train_x.create_all_features().collect()\n",
    "\n",
    "train_y = pl.scan_csv(CONFIG.TRAIN_Y_PATH).fill_null(0).collect()\n",
    "\n",
    "test_x = train_x.filter(pl.col(\"date_id\") > CONFIG.MAX_TRAIN_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30b5b51",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for TIMESERIES_NN:\n\tsize mismatch for feature_extractor.0.weight: copying a param with shape torch.Size([14157, 14157]) from checkpoint, the shape in current model is torch.Size([43186, 43186]).\n\tsize mismatch for feature_extractor.0.bias: copying a param with shape torch.Size([14157]) from checkpoint, the shape in current model is torch.Size([43186]).\n\tsize mismatch for feature_extractor.1.weight: copying a param with shape torch.Size([14157]) from checkpoint, the shape in current model is torch.Size([43186]).\n\tsize mismatch for feature_extractor.1.bias: copying a param with shape torch.Size([14157]) from checkpoint, the shape in current model is torch.Size([43186]).\n\tsize mismatch for feature_extractor.4.weight: copying a param with shape torch.Size([7078, 14157]) from checkpoint, the shape in current model is torch.Size([21593, 43186]).\n\tsize mismatch for feature_extractor.4.bias: copying a param with shape torch.Size([7078]) from checkpoint, the shape in current model is torch.Size([21593]).\n\tsize mismatch for feature_extractor.5.weight: copying a param with shape torch.Size([7078]) from checkpoint, the shape in current model is torch.Size([21593]).\n\tsize mismatch for feature_extractor.5.bias: copying a param with shape torch.Size([7078]) from checkpoint, the shape in current model is torch.Size([21593]).\n\tsize mismatch for feature_extractor.8.weight: copying a param with shape torch.Size([512, 7078]) from checkpoint, the shape in current model is torch.Size([512, 21593]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m NN(\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mCONFIG\u001b[38;5;241m.\u001b[39mNN_PARAMS,\n\u001b[0;32m      3\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mCONFIG\u001b[38;5;241m.\u001b[39mBATCH_SIZE,\n\u001b[0;32m      4\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.002\u001b[39m,\n\u001b[0;32m      5\u001b[0m     lr_refit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m TIMESERIES_NN(num_numerical_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_x\u001b[38;5;241m.\u001b[39mdrop([CONFIG\u001b[38;5;241m.\u001b[39mDATE_COL] \u001b[38;5;241m+\u001b[39m CONFIG\u001b[38;5;241m.\u001b[39mCAT_COLS)\u001b[38;5;241m.\u001b[39mcolumns), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mCONFIG\u001b[38;5;241m.\u001b[39mNN_PARAMS)\n\u001b[1;32m----> 9\u001b[0m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_state_dict(\n\u001b[0;32m     10\u001b[0m     torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/Admin/Desktop/Personal-Projects/Kaggle/MITSUI&CO. Commodity Prediction Challenge/sample_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m0\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\edmund\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2593\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2585\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2586\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2587\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2588\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2589\u001b[0m             ),\n\u001b[0;32m   2590\u001b[0m         )\n\u001b[0;32m   2592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2593\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2594\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2595\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2596\u001b[0m         )\n\u001b[0;32m   2597\u001b[0m     )\n\u001b[0;32m   2598\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for TIMESERIES_NN:\n\tsize mismatch for feature_extractor.0.weight: copying a param with shape torch.Size([14157, 14157]) from checkpoint, the shape in current model is torch.Size([43186, 43186]).\n\tsize mismatch for feature_extractor.0.bias: copying a param with shape torch.Size([14157]) from checkpoint, the shape in current model is torch.Size([43186]).\n\tsize mismatch for feature_extractor.1.weight: copying a param with shape torch.Size([14157]) from checkpoint, the shape in current model is torch.Size([43186]).\n\tsize mismatch for feature_extractor.1.bias: copying a param with shape torch.Size([14157]) from checkpoint, the shape in current model is torch.Size([43186]).\n\tsize mismatch for feature_extractor.4.weight: copying a param with shape torch.Size([7078, 14157]) from checkpoint, the shape in current model is torch.Size([21593, 43186]).\n\tsize mismatch for feature_extractor.4.bias: copying a param with shape torch.Size([7078]) from checkpoint, the shape in current model is torch.Size([21593]).\n\tsize mismatch for feature_extractor.5.weight: copying a param with shape torch.Size([7078]) from checkpoint, the shape in current model is torch.Size([21593]).\n\tsize mismatch for feature_extractor.5.bias: copying a param with shape torch.Size([7078]) from checkpoint, the shape in current model is torch.Size([21593]).\n\tsize mismatch for feature_extractor.8.weight: copying a param with shape torch.Size([512, 7078]) from checkpoint, the shape in current model is torch.Size([512, 21593])."
     ]
    }
   ],
   "source": [
    "model = NN(\n",
    "    **CONFIG.NN_PARAMS,\n",
    "    batch_size=CONFIG.BATCH_SIZE,\n",
    "    lr=0.002,\n",
    "    lr_refit=0.001,\n",
    ")\n",
    "\n",
    "model.model = TIMESERIES_NN(num_numerical_features=len(test_x.drop([CONFIG.DATE_COL] + CONFIG.CAT_COLS).columns), **CONFIG.NN_PARAMS)\n",
    "model.model.load_state_dict(torch.load(f\"C:/Users/Admin/Desktop/Personal-Projects/Kaggle/MITSUI&CO. Commodity Prediction Challenge/sample_{0}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74756776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf4c5ab2a4a4e50acf6fd8f62cfc8a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (90,)\n",
      "Series: 'date_id' [i64]\n",
      "[\n",
      "\t1840\n",
      "\t1843\n",
      "\t1846\n",
      "\t1837\n",
      "\t1849\n",
      "\t\n",
      "\t1914\n",
      "\t1905\n",
      "\t1834\n",
      "\t1828\n",
      "\t1831\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "test_dates = test_x.select(CONFIG.DATE_COL).unique().to_series()\n",
    "for date_id in tqdm(test_dates):\n",
    "    predict_x = train_x.filter(pl.col(CONFIG.DATE_COL) == date_id)\n",
    "    type_predict_x, instr_predict_x, df_predict_x = make_data(\n",
    "        df=predict_x,\n",
    "        type_encoder=type_encoder,\n",
    "        instr_encoder=instr_encoder,\n",
    "    )\n",
    "    preds = model.predict(type_predict_x, instr_predict_x, df_predict_x, n_times=1)\n",
    "    if model.lr_refit and (cnt_dates > 0):\n",
    "        df_upd = train_x.filter(pl.col(CONFIG.DATE_COL).is_in(range(date_id - 1 - CONFIG.SEQ_LEN, date_id - 1)))\n",
    "        df_upd_y = train_y.filter(pl.col(CONFIG.DATE_COL).is_in(range(date_id - 1 - CONFIG.SEQ_LEN, date_id - 1))).drop(CONFIG.DATE_COL).to_numpy()\n",
    "        type_upd, instr_upd, df_upd = make_data(\n",
    "            df=df_upd,\n",
    "            type_encoder=type_encoder,\n",
    "            instr_encoder=instr_encoder,\n",
    "        )\n",
    "        if len(df_upd) > 0:\n",
    "            model_save.update(type_upd, instr_upd, df_upd, df_upd_y, 1)\n",
    "            preds_i = model_save.predict(type_valid_date, instr_valid_date, df_valid_date, n_times=1)\n",
    "    preds += list(preds_i[-1].reshape(-1, CONFIG.NUM_TARGET_COLUMNS))\n",
    "\n",
    "preds = np.array(preds)\n",
    "\n",
    "score = rank_correlation_sharpe(train_y.filter(pl.col(CONFIG.DATE_COL) > CONFIG.MAX_TRAIN_DATE).to_numpy(), preds)\n",
    "scores.append(score)\n",
    "\n",
    "print(f\"Sharpe: {score:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edmund",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
