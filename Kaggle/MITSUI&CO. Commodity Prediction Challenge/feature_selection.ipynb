{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05d33f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from numba import njit, prange, float64\n",
    "import pickle\n",
    "import time\n",
    "import re\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "from typing import Tuple, List, Dict, Any, Union, Literal\n",
    "from scipy.stats import rankdata, norm\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import warnings\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from CONFIG import CONFIG\n",
    "from PREPROCESSOR_V2 import PREPROCESSOR\n",
    "from FEATURE_ENGINEERING_V2 import FEATURE_ENGINEERING\n",
    "from DATASET import SequentialDataset\n",
    "from ENSEMBLE_NN import ENSEMBLE_NN\n",
    "from NN import NN\n",
    "from LOSS import CombinedICIRLoss\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# import cuml\n",
    "# from cuml.explainer import TreeExplainer\n",
    "\n",
    "\n",
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        print(f\"{func.__name__} took {end - start:.4f} seconds\")\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "159ae59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_rank_transform(arr: np.array):\n",
    "    # n_samples, n_targets = arr.shape\n",
    "    transformed_targets = np.full_like(arr, np.nan)\n",
    "\n",
    "    for i, row in enumerate(arr):\n",
    "        # Find valid (non-NaN) assets for this timestep\n",
    "        valid_mask = ~np.isnan(row)\n",
    "        valid_arr = row[valid_mask]\n",
    "        ranks = rankdata(valid_arr, method=\"average\")\n",
    "        percentile_ranks = (ranks - 0.5) / (len(ranks))\n",
    "        percentile_ranks = np.clip(percentile_ranks, 1e-8, 1 - 1e-8)\n",
    "        gaussian_values = norm.ppf(percentile_ranks)\n",
    "        transformed_targets[i, valid_mask] = gaussian_values\n",
    "    return transformed_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6705b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankCorrelationLoss:\n",
    "    \"\"\"Custom loss function for maximizing rank correlation Sharpe ratio.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.num_targets = CONFIG.NUM_TARGET_COLUMNS\n",
    "\n",
    "    def _compute_rank_correlation_batch(self, y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute rank correlation for each sample (date).\"\"\"\n",
    "        y_pred = y_pred.reshape(-1, self.num_targets)\n",
    "        corr = []\n",
    "        for i in range(y_pred.shape[0]):\n",
    "            y_pred_rank = rankdata(y_pred[i], method=\"average\")\n",
    "            y_true_rank = rankdata(y_true[i], method=\"average\")\n",
    "\n",
    "            corr.append(np.corrcoef(y_pred_rank, y_true_rank)[0, 1])\n",
    "\n",
    "        return corr\n",
    "\n",
    "\n",
    "def rank_correlation_sharpe_eval(y_pred: np.ndarray, y_true: np.ndarray, n_targets: int = 424) -> float:\n",
    "    \"\"\"Evaluation function for both XGBoost and LightGBM.\"\"\"\n",
    "    loss_fn = RankCorrelationLoss()\n",
    "    correlations = loss_fn._compute_rank_correlation_batch(y_pred, y_true)\n",
    "\n",
    "    mean_corr = np.mean(correlations)\n",
    "    std_corr = np.std(correlations)\n",
    "\n",
    "    if std_corr == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return -mean_corr / std_corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f5c5612",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelector:\n",
    "    def __init__(self, train_x, train_y, n_targets: int = 424):\n",
    "        self.train_x = train_x  # drop date\n",
    "        self.train_y = train_y\n",
    "        self.n_targets = n_targets\n",
    "        self.loss_fn = RankCorrelationLoss()\n",
    "        self.keep_features = None\n",
    "        self.total_features = train_x.columns.__len__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    @timer\n",
    "    def basic_filters(self):\n",
    "        train_x_filter_1 = self.train_x.select([col for col in self.train_x.columns if self.train_x[col].var() > 1e-3])\n",
    "        train_x_filter_2 = train_x_filter_1.select(\n",
    "            [col for col in train_x_filter_1.columns if train_x_filter_1[col].value_counts()[\"count\"].max() / len(train_x_filter_1) < 0.80]\n",
    "        )\n",
    "        train_x_filter_3 = train_x_filter_2.select([col for col in train_x_filter_2.columns if train_x_filter_2[col].n_unique() > 2])\n",
    "\n",
    "        print(f\"After Basic Filter: {train_x_filter_3.columns.__len__()} / {self.total_features}\")\n",
    "\n",
    "        return train_x_filter_3\n",
    "\n",
    "    @timer\n",
    "    def run_correlation(self, x: torch.Tensor, y: torch.Tensor, names: list) -> list:\n",
    "        \"\"\"\n",
    "        Memory-optimized version using chunked processing\n",
    "        Best for when you have enough GPU memory\n",
    "\n",
    "        Args:\n",
    "            x: Tensor\n",
    "            y: Tensor\n",
    "\n",
    "        Returns:\n",
    "            correlations: Tensor\n",
    "        \"\"\"\n",
    "        N, D1 = x.shape\n",
    "        N2, D2 = y.shape\n",
    "        assert N == N2\n",
    "\n",
    "        device = x.device\n",
    "\n",
    "        # Handle NaNs by masking\n",
    "        x_valid = ~torch.isnan(x)\n",
    "        y_valid = ~torch.isnan(y)\n",
    "\n",
    "        # Convert NaNs to 0 for computation\n",
    "        x_clean = torch.where(x_valid, x, 0.0)\n",
    "        y_clean = torch.where(y_valid, y, 0.0)\n",
    "\n",
    "        # Compute valid sample counts for each pair efficiently\n",
    "        # This is the memory bottleneck, so we chunk it\n",
    "        chunk_size = 500  # Adjust based on GPU memory\n",
    "        correlations = torch.zeros(D1, D2, device=device)\n",
    "\n",
    "        for i in range(0, D1, chunk_size):\n",
    "            end_i = min(i + chunk_size, D1)\n",
    "\n",
    "            # Get chunk\n",
    "            x_chunk = x_clean[:, i:end_i]  # (N, chunk_size)\n",
    "            x_valid_chunk = x_valid[:, i:end_i]  # (N, chunk_size)\n",
    "\n",
    "            # Compute valid sample matrix for this chunk\n",
    "            valid_matrix = x_valid_chunk.unsqueeze(2) & y_valid.unsqueeze(1)  # (N, chunk_size, D2)\n",
    "            n_valid = valid_matrix.sum(dim=0).float()  # (chunk_size, D2)\n",
    "\n",
    "            # Sufficient samples mask\n",
    "            sufficient = n_valid >= 10\n",
    "\n",
    "            if sufficient.any():\n",
    "                # Compute means over valid samples\n",
    "                x_sum = (x_chunk.unsqueeze(2) * valid_matrix).sum(dim=0)  # (chunk_size, D2)\n",
    "                y_sum = (y_clean.unsqueeze(1) * valid_matrix).sum(dim=0)  # (chunk_size, D2)\n",
    "\n",
    "                x_mean = x_sum / (n_valid + 1e-10)\n",
    "                y_mean = y_sum / (n_valid + 1e-10)\n",
    "\n",
    "                # Center data\n",
    "                x_centered = (x_chunk.unsqueeze(2) - x_mean.unsqueeze(0)) * valid_matrix  # (N, chunk_size, D2)\n",
    "                y_centered = (y_clean.unsqueeze(1) - y_mean.unsqueeze(0)) * valid_matrix  # (N, chunk_size, D2)\n",
    "\n",
    "                # Compute correlation\n",
    "                numerator = (x_centered * y_centered).sum(dim=0)\n",
    "                x_var = (x_centered**2).sum(dim=0)\n",
    "                y_var = (y_centered**2).sum(dim=0)\n",
    "\n",
    "                denominator = torch.sqrt(x_var * y_var) + 1e-10\n",
    "                chunk_corr = numerator / denominator\n",
    "\n",
    "                # Apply sufficient samples mask\n",
    "                chunk_corr = torch.where(sufficient, chunk_corr, 0.0)\n",
    "                correlations[i:end_i] = torch.abs(chunk_corr)\n",
    "\n",
    "        self.correlations = correlations.cpu().numpy()\n",
    "\n",
    "    @timer\n",
    "    def run_biserial_correlation(self, x: torch.Tensor, y: torch.Tensor, names: list) -> list:\n",
    "        \"\"\"\n",
    "        Compute biserial correlations between binary x and continuous y.\n",
    "\n",
    "        Args:\n",
    "            x: Binary tensor (N, D1)\n",
    "            y: Continuous tensor (N, D2)\n",
    "            names: Unused, for compatibility\n",
    "\n",
    "        Returns:\n",
    "            correlations: (D1, D2) Tensor of biserial correlation coefficients\n",
    "        \"\"\"\n",
    "        N, D1 = x.shape\n",
    "        N2, D2 = y.shape\n",
    "        assert N == N2\n",
    "\n",
    "        device = x.device\n",
    "\n",
    "        # Mask NaNs\n",
    "        x_valid = ~torch.isnan(x)\n",
    "        y_valid = ~torch.isnan(y)\n",
    "\n",
    "        # Replace NaNs with zeros for safe computation\n",
    "        x_clean = torch.where(x_valid, x, 0.0)\n",
    "        y_clean = torch.where(y_valid, y, 0.0)\n",
    "\n",
    "        # Output tensor\n",
    "        correlations = torch.zeros(D1, D2, device=device)\n",
    "\n",
    "        chunk_size = 500\n",
    "\n",
    "        for i in range(0, D1, chunk_size):\n",
    "            end_i = min(i + chunk_size, D1)\n",
    "\n",
    "            x_chunk = x_clean[:, i:end_i]  # (N, chunk)\n",
    "            x_valid_chunk = x_valid[:, i:end_i]  # (N, chunk)\n",
    "\n",
    "            valid_matrix = x_valid_chunk.unsqueeze(2) & y_valid.unsqueeze(1)  # (N, chunk, D2)\n",
    "            n_valid = valid_matrix.sum(dim=0).float()  # (chunk, D2)\n",
    "            sufficient = n_valid >= 10\n",
    "\n",
    "            if sufficient.any():\n",
    "                x_bin = x_chunk.unsqueeze(2)  # (N, chunk, 1)\n",
    "                y_vals = y_clean.unsqueeze(1)  # (N, 1, D2)\n",
    "\n",
    "                # Get masks for x == 1 and x == 0\n",
    "                x_eq_1 = (x_bin == 1.0) & valid_matrix  # (N, chunk, D2)\n",
    "                x_eq_0 = (x_bin == 0.0) & valid_matrix\n",
    "\n",
    "                # Counts\n",
    "                n1 = x_eq_1.sum(dim=0).float()\n",
    "                n0 = x_eq_0.sum(dim=0).float()\n",
    "                total_n = n1 + n0 + 1e-10  # avoid div by zero\n",
    "\n",
    "                p = n1 / total_n\n",
    "                q = 1 - p\n",
    "\n",
    "                # Means for y where x == 1 and x == 0\n",
    "                mean_y1 = (y_vals * x_eq_1).sum(dim=0) / (n1 + 1e-10)\n",
    "                mean_y0 = (y_vals * x_eq_0).sum(dim=0) / (n0 + 1e-10)\n",
    "\n",
    "                # Overall std of y (on valid entries)\n",
    "                y_mean = (y_vals * valid_matrix).sum(dim=0) / (n_valid + 1e-10)\n",
    "                y_centered = (y_vals - y_mean) * valid_matrix\n",
    "                y_std = torch.sqrt((y_centered**2).sum(dim=0) / (n_valid + 1e-10))\n",
    "\n",
    "                # Compute biserial correlation\n",
    "                mean_diff = mean_y1 - mean_y0\n",
    "                r_bis = (mean_diff / (y_std + 1e-10)) * torch.sqrt(p * q)\n",
    "\n",
    "                # Mask insufficient pairs\n",
    "                r_bis = torch.where(sufficient, r_bis, torch.tensor(0.0, device=device))\n",
    "\n",
    "                correlations[i:end_i] = torch.abs(r_bis)\n",
    "\n",
    "        self.biserial_correlation = correlations.cpu().numpy()\n",
    "\n",
    "    def run_selection(self):\n",
    "        # filtered = self.basic_filters()\n",
    "        binary_cols = [\n",
    "            col for col in self.train_x.columns if self.train_x[col].max() == 1 and self.train_x[col].min() == 0 and col != CONFIG.DATE_COL\n",
    "        ]\n",
    "        train_x_arr = self.train_x.drop(binary_cols).to_numpy()\n",
    "        train_x_binary_arr = self.train_x.select(binary_cols).to_numpy()\n",
    "        train_y_arr = self.train_y.to_numpy()\n",
    "        x = torch.tensor(train_x_arr, device=\"cuda\")\n",
    "        binary_x = torch.tensor(train_x_binary_arr, device=\"cuda\")\n",
    "        y = torch.tensor(train_y_arr, device=\"cuda\")\n",
    "        self.run_correlation(x=x, y=y, names=self.train_x.columns)\n",
    "        self.run_biserial_correlation(x=binary_x, y=y, names=binary_cols)\n",
    "\n",
    "        corrs_df = pl.concat(\n",
    "            [\n",
    "                pl.DataFrame(self.correlations.mean(axis=1), schema=[\"corr_mean\"]).with_columns(\n",
    "                    pl.Series(name=\"feature\", values=self.train_x.drop(binary_cols).columns)\n",
    "                ),\n",
    "                pl.DataFrame(self.correlations.std(axis=1), schema=[\"corr_std\"]),\n",
    "            ],\n",
    "            how=\"horizontal\",\n",
    "        )\n",
    "\n",
    "        keep_features = (\n",
    "            corrs_df.drop_nans()\n",
    "            .filter((pl.col(\"corr_mean\") >= corrs_df[\"corr_mean\"].median() * 1.05) & (pl.col(\"corr_std\") <= corrs_df[\"corr_std\"].median() * 0.95))\n",
    "            .sort(by=\"corr_std\")\n",
    "            .select(\"feature\")\n",
    "            .to_series()\n",
    "            # .head(100)\n",
    "            .to_list()\n",
    "        )\n",
    "\n",
    "        print(f\"After correlation, keeping: {keep_features.__len__()} features\")\n",
    "\n",
    "        # MI_scores = self.run_MI(x=np.nan_to_num(self.train_x.select(keep_features).to_numpy()), y=np.nan_to_num(train_y_arr).T)\n",
    "        # return MI_scores, keep_features\n",
    "        return self.correlations, self.biserial_correlation, keep_features, binary_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc038e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_market_features took 16.2510 seconds\n"
     ]
    }
   ],
   "source": [
    "# --- Prepare DataLoader ---\n",
    "# Create the dataset\n",
    "\n",
    "train_x = pl.scan_csv(CONFIG.TRAIN_X_PATH).drop(CONFIG.DROP_COL)\n",
    "train_x = PREPROCESSOR(df=train_x)\n",
    "train_x = train_x.clean()\n",
    "\n",
    "features = FEATURE_ENGINEERING(df=train_x)\n",
    "train_x: pl.DataFrame = features.create_market_features()\n",
    "\n",
    "train_y = pl.scan_csv(CONFIG.TRAIN_Y_PATH)\n",
    "\n",
    "curr_y = (\n",
    "    train_y.with_columns([pl.col(CONFIG.LAGS[f\"lag{i}\"]).exclude(CONFIG.DATE_COL).shift(i + 1) for i in range(1, 5)])\n",
    "    .with_columns(pl.all().exclude(CONFIG.DATE_COL).shift())\n",
    "    .filter((pl.col(CONFIG.DATE_COL).is_in(train_x.select(CONFIG.DATE_COL).to_series())))\n",
    "    .collect()\n",
    "    .fill_null(0)\n",
    "    .lazy()\n",
    ")\n",
    "\n",
    "y_feat = FEATURE_ENGINEERING(df=curr_y)\n",
    "ys = y_feat.create_Y_market_features()\n",
    "\n",
    "train_x = train_x.join(ys, on=CONFIG.DATE_COL).fill_nan(0)\n",
    "\n",
    "\n",
    "train_y = train_y.filter((pl.col(CONFIG.DATE_COL).is_in(train_x.select(CONFIG.DATE_COL).to_series()))).collect()\n",
    "train_x = (\n",
    "    train_x.with_columns([pl.when(pl.col(col).is_infinite()).then(0.0).otherwise(pl.col(col)).alias(col) for col in train_x.columns])\n",
    "    .with_columns(pl.all().shrink_dtype())\n",
    "    .filter(pl.col(CONFIG.DATE_COL).is_in(train_y.select(CONFIG.DATE_COL).to_series()))\n",
    "    .with_columns(pl.col(CONFIG.DATE_COL).cast(pl.Int64))\n",
    ")\n",
    "\n",
    "retrain_x = train_x.with_columns(pl.all().exclude(CONFIG.DATE_COL).shift(5))\n",
    "retrain_y = train_y.filter((pl.col(CONFIG.DATE_COL).is_in(train_x.select(CONFIG.DATE_COL).to_series()))).with_columns(\n",
    "    pl.all().exclude(CONFIG.DATE_COL).shift(5)\n",
    ")\n",
    "\n",
    "train_y_arr = train_y.drop(CONFIG.DATE_COL).to_numpy()\n",
    "\n",
    "train_y = pl.DataFrame(gaussian_rank_transform(train_y_arr), schema=train_y.drop(CONFIG.DATE_COL).columns).insert_column(\n",
    "    0, train_y.select(CONFIG.DATE_COL).to_series()\n",
    ")\n",
    "\n",
    "\n",
    "# pl.DataFrame(\n",
    "#     (train_y_arr - np.nanmean(train_y_arr, axis=1).reshape(train_y_arr.shape[0], -1))\n",
    "#     / np.nanstd(train_y_arr, axis=1).reshape(train_y_arr.shape[0], -1),\n",
    "#     schema=train_y.drop(CONFIG.DATE_COL).columns,\n",
    "# ).insert_column(0, train_y.select(CONFIG.DATE_COL).to_series())\n",
    "\n",
    "retrain_y_arr = retrain_y.drop(CONFIG.DATE_COL).to_numpy()\n",
    "retrain_y = pl.DataFrame(gaussian_rank_transform(retrain_y_arr), schema=train_y.drop(CONFIG.DATE_COL).columns).insert_column(\n",
    "    0, train_y.select(CONFIG.DATE_COL).to_series()\n",
    ")\n",
    "\n",
    "\n",
    "# pl.DataFrame(\n",
    "#     (retrain_y_arr - np.nanmean(retrain_y_arr, axis=1).reshape(retrain_y_arr.shape[0], -1))\n",
    "#     / np.nanstd(retrain_y_arr, axis=1).reshape(retrain_y_arr.shape[0], -1),\n",
    "#     schema=train_y.drop(CONFIG.DATE_COL).columns,\n",
    "# ).insert_column(0, train_y.select(CONFIG.DATE_COL).to_series())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ac7babc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_correlation took 31.2631 seconds\n",
      "run_biserial_correlation took 0.0785 seconds\n",
      "After correlation, keeping: 1393 features\n"
     ]
    }
   ],
   "source": [
    "s = FeatureSelector(train_x=train_x.drop(CONFIG.DATE_COL), train_y=train_y.drop(CONFIG.DATE_COL))\n",
    "corr, bi_corr, keep_features, binary_features = s.run_selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ac9c3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = pl.concat(\n",
    "    [\n",
    "        pl.concat(\n",
    "            [\n",
    "                pl.DataFrame(corr.mean(axis=1), schema=[\"corr_mean\"]).with_columns(\n",
    "                    pl.Series(name=\"feature\", values=s.train_x.drop(binary_features).columns)\n",
    "                ),\n",
    "                pl.DataFrame(corr.std(axis=1), schema=[\"corr_std\"]),\n",
    "            ],\n",
    "            how=\"horizontal\",\n",
    "        ),\n",
    "        pl.concat(\n",
    "            [\n",
    "                pl.DataFrame(bi_corr.mean(axis=1), schema=[\"corr_mean\"]).with_columns(pl.Series(name=\"feature\", values=binary_features)),\n",
    "                pl.DataFrame(bi_corr.std(axis=1), schema=[\"corr_std\"]),\n",
    "            ],\n",
    "            how=\"horizontal\",\n",
    "        ),\n",
    "    ],\n",
    "    how=\"vertical\",\n",
    ").sort(by=\"corr_mean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f83459a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = []\n",
    "keep_df = {}\n",
    "for i in range(len(CONFIG.LAG_SEQ_LEN.keys())):\n",
    "    score = corr[:, i : (i + 1) * 106]\n",
    "    bi_score = bi_corr[:, i : (i + 1) * 106]\n",
    "    df = pl.concat(\n",
    "        [\n",
    "            pl.concat(\n",
    "                [\n",
    "                    pl.DataFrame(score.mean(axis=1), schema=[\"corr_mean\"]).with_columns(\n",
    "                        pl.Series(name=\"feature\", values=s.train_x.drop(binary_features).columns)\n",
    "                    ),\n",
    "                    pl.DataFrame(score.std(axis=1), schema=[\"corr_std\"]),\n",
    "                    pl.DataFrame(np.median(score, axis=1), schema=[\"corr_median\"]),\n",
    "                ],\n",
    "                how=\"horizontal\",\n",
    "            ),\n",
    "            pl.concat(\n",
    "                [\n",
    "                    pl.DataFrame(bi_score.mean(axis=1), schema=[\"corr_mean\"]).with_columns(pl.Series(name=\"feature\", values=binary_features)),\n",
    "                    pl.DataFrame(bi_score.std(axis=1), schema=[\"corr_std\"]),\n",
    "                    pl.DataFrame(np.median(bi_score, axis=1), schema=[\"corr_median\"]),\n",
    "                ],\n",
    "                how=\"horizontal\",\n",
    "            ),\n",
    "        ],\n",
    "        how=\"vertical\",\n",
    "    )\n",
    "\n",
    "    # Normalize the statistics columns for later use in composite scoring\n",
    "    df = df.with_columns(\n",
    "        ((pl.col(\"corr_mean\") - df[\"corr_mean\"].min()) / (df[\"corr_mean\"].max() - df[\"corr_mean\"].min())).alias(\"corr_mean_norm\"),\n",
    "        ((pl.col(\"corr_std\") - df[\"corr_std\"].min()) / (df[\"corr_std\"].max() - df[\"corr_std\"].min())).alias(\"corr_std_norm\"),\n",
    "        ((pl.col(\"corr_median\") - df[\"corr_median\"].min()) / (df[\"corr_median\"].max() - df[\"corr_median\"].min())).alias(\"corr_median_norm\"),\n",
    "    )\n",
    "\n",
    "    # Compute the composite score using weighted sums of normalized mean, std, and median\n",
    "    df = df.with_columns((0.2 * pl.col(\"corr_mean_norm\") + 0.6 * pl.col(\"corr_std_norm\") + 0.2 * pl.col(\"corr_median_norm\")).alias(\"composite_score\"))\n",
    "\n",
    "    # Sort by composite score and select top 50 features\n",
    "    df_sorted = df.sort(by=\"composite_score\", descending=True).head(100)\n",
    "    # Select features from sorted list\n",
    "    keep.append(df_sorted.select(\"feature\").to_series().to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cb8ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = []\n",
    "keep_df = {}\n",
    "for i in range(len(CONFIG.LAG_SEQ_LEN.keys())):\n",
    "    score = corr[:, i : (i + 1) * 106]\n",
    "    df = pl.concat(\n",
    "        [\n",
    "            pl.DataFrame(score.mean(axis=1), schema=[\"corr_mean\"]).with_columns(pl.Series(name=\"feature\", values=s.train_x.columns)),\n",
    "            pl.DataFrame(score.std(axis=1), schema=[\"corr_std\"]),\n",
    "        ],\n",
    "        how=\"horizontal\",\n",
    "    )\n",
    "\n",
    "    filter_mean = df.sort(by=\"corr_mean\", descending=True).head(500)\n",
    "    filter_features = filter_mean.select(\"feature\").to_series().to_list()\n",
    "\n",
    "    train = train_x.select(filter_features)\n",
    "    X_train, y_train, X_val, y_val = (\n",
    "        train[: int(0.8 * len(train))],\n",
    "        train_y.select([f\"target_{i}\" for i in range(i, (i + 1) * 106)])[: int(0.8 * len(train))],\n",
    "        train[int(0.8 * len(train)) + 1 :],\n",
    "        train_y.select([f\"target_{i}\" for i in range(i, (i + 1) * 106)])[int(0.8 * len(train)) + 1 :],\n",
    "    )\n",
    "\n",
    "    model = xgb.XGBRegressor(\n",
    "        **{\n",
    "            \"n_estimators\": 100,\n",
    "            \"max_depth\": 32,\n",
    "            \"learning_rate\": 0.05,\n",
    "            \"subsample\": 0.7,\n",
    "            \"colsample_bytree\": 0.5,\n",
    "            \"random_state\": CONFIG.RANDOM_STATE,\n",
    "            \"device\": \"gpu\",\n",
    "            \"early_stopping_rounds\": 10,\n",
    "            \"verbosity\": 2,\n",
    "        }\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train.to_pandas(),\n",
    "        y_train.fill_nan(-999).to_pandas(),\n",
    "        eval_set=[(X_val.to_pandas(), y_val.fill_nan(-999).to_pandas())],\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    raw_score = model.get_booster().get_score(importance_type=\"gain\")\n",
    "    importance_dict = {f: raw_score.get(f, 0.0) for f in X_train.columns}\n",
    "    feat_impt_xgb = (\n",
    "        pl.DataFrame(importance_dict)\n",
    "        .transpose(include_header=True)\n",
    "        .with_columns(pl.col(\"column_0\") / pl.col(\"column_0\").sum())\n",
    "        .sort(by=\"column_0\", descending=True)\n",
    "    )\n",
    "    feat_impt_xgb.columns = [\"feature\", \"score\"]\n",
    "    final_scores = (\n",
    "        feat_impt_xgb.join(filter_mean, on=\"feature\")\n",
    "        .with_columns(total_score=(pl.col(\"corr_mean\") * 0.5 + pl.col(\"score\") * 0.5))\n",
    "        .sort(by=\"total_score\", descending=True)\n",
    "    )\n",
    "\n",
    "    feats = final_scores.select(\"feature\").to_series().head(50).to_list()\n",
    "    keep.append(feats)\n",
    "    keep_df[i] = final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55ac8e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set([j for i in keep for j in i])).__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4efc562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['US_Stock_DE_adj_low_std_90_252',\n",
       " 'target_96_return_lag_4_std_252_252',\n",
       " 'US_Stock_SHEL_adj_low_vol_90_std_10_90',\n",
       " 'target_251_auto_corr_90_std_252_90',\n",
       " 'US_Stock_JNK_adj_close_log_ret_return_skew_10_std_252_90',\n",
       " 'target_63_return_lag_4_std_10_252',\n",
       " 'US_Stock_CLF_adj_open_vol_10_std_252_10',\n",
       " 'target_381_return_lag_1_std_90_90',\n",
       " 'target_385_return_skew_5_std_252_10',\n",
       " 'US_Stock_DE_adj_open_10_sortino_std_10_252',\n",
       " 'target_96_return_lag_4_std_10_252',\n",
       " 'LME_ZS_Close_log_ret_US_Stock_MS_adj_close_log_ret_sigmoid_diff_std_90_252',\n",
       " 'US_Stock_JNK_adj_close_log_ret_return_skew_252_std_10_252',\n",
       " 'FX_ZAREUR_10_sortino_std_10_10',\n",
       " 'LME_ZS_Close_log_ret_FX_NOKGBP_log_ret_log_ratio_std_90_10',\n",
       " 'US_Stock_CVX_adj_close_std_252_10',\n",
       " 'US_Stock_DE_adj_close_sma_10_std_90_252',\n",
       " 'US_Stock_ENB_adj_close_log_ret_return_lag_5_std_90_252',\n",
       " 'target_1_return_lag_2_std_90_10',\n",
       " 'target_184_return_skew_252_std_90_90',\n",
       " 'LME_ZS_Close_log_ret_US_Stock_MS_adj_close_log_ret_sigmoid_diff_std_252_90',\n",
       " 'US_Stock_OXY_adj_high_log_ret_return_skew_5_std_10_10',\n",
       " 'US_Stock_DE_adj_high_std_90_252',\n",
       " 'US_Stock_DE_adj_high_10_sortino_std_90_252',\n",
       " 'LME_ZS_Close_log_ret_US_Stock_MS_adj_close_log_ret_sub_std_252_252',\n",
       " 'LME_ZS_Close_log_ret_US_Stock_MS_adj_close_log_ret_sub_std_90_252',\n",
       " 'target_6_return_lag_2_std_252_90',\n",
       " 'US_Stock_JNK_adj_low_log_ret_return_skew_252_std_10_252',\n",
       " 'US_Stock_DE_adj_close_252_sharpe_std_90_252',\n",
       " 'US_Stock_SLB_adj_open_log_ret_return_skew_5_std_10_10',\n",
       " 'target_385_return_skew_5_std_10_90',\n",
       " 'US_Stock_DE_adj_close_std_90_252',\n",
       " 'US_Stock_XLE_adj_close_log_ret_auto_corr_10_std_10_10',\n",
       " 'target_141_auto_corr_252_std_10_10',\n",
       " 'target_46_return_lag_4_std_252_252',\n",
       " 'JPX_Platinum_Mini_Futures_Close_log_ret_auto_corr_90_std_10_252',\n",
       " 'US_Stock_JNK_adj_close_log_ret_return_skew_10_std_90_90',\n",
       " 'US_Stock_DE_adj_close_10_sortino_std_90_90',\n",
       " 'US_Stock_DE_adj_open_sma_10_std_10_252',\n",
       " 'US_Stock_EFA_adj_low_log_ret_return_skew_252_std_10_10',\n",
       " 'FX_ZARCHF_log_ret_JPX_Platinum_Standard_Futures_Close_log_ret_mul_std_10_10',\n",
       " 'target_46_return_lag_4_std_90_90',\n",
       " 'ranked_US_Stock_GLD_adj_close_log_ret_LME_PB_Close_log_ret_exp_diff_std_90_10',\n",
       " 'US_Stock_DE_adj_open_90_sortino_std_10_90',\n",
       " 'target_108_return_skew_90_std_10_90',\n",
       " 'target_97_return_lag_4_std_252_252',\n",
       " 'target_1_return_lag_4_std_90_252',\n",
       " 'LME_ZS_Close_log_ret_US_Stock_MS_adj_close_log_ret_exp_diff_std_10_252',\n",
       " 'JPX_Platinum_Standard_Futures_Close_log_ret_FX_ZARUSD_log_ret_atan_ratio_std_10_90',\n",
       " 'US_Stock_DE_adj_low_sma_5_std_90_252',\n",
       " 'target_263_return_lag_5_std_252_10',\n",
       " 'US_Stock_MS_adj_open_vol_90_std_90_90',\n",
       " 'US_Stock_DE_adj_low_90_sharpe_std_252_252',\n",
       " 'target_1_return_lag_4_std_10_252',\n",
       " 'LME_ZS_Close_log_ret_US_Stock_MS_adj_close_log_ret_sigmoid_diff_std_10_252',\n",
       " 'US_Stock_DE_adj_close_std_90_90',\n",
       " 'US_Stock_GLD_adj_low_log_ret_return_skew_10_std_10_90',\n",
       " 'FX_NZDUSD_log_ret_LME_PB_Close_log_ret_sqrt_mul_std_252_90',\n",
       " 'US_Stock_DE_adj_low_252_sharpe_std_90_252',\n",
       " 'target_176_auto_corr_90_std_90_10',\n",
       " 'US_Stock_JNK_adj_close_log_ret_return_skew_10_std_252_10',\n",
       " 'target_30_auto_corr_252_std_10_10',\n",
       " 'target_46_return_lag_4_std_252_90',\n",
       " 'target_36_return_lag_4_std_252_90',\n",
       " 'US_Stock_EWY_adj_close_log_ret_auto_corr_90_std_90_90',\n",
       " 'LME_PB_Close_log_ret_FX_NZDUSD_log_ret_sqrt_mul_std_252_90',\n",
       " 'US_Stock_MS_adj_close_log_ret_return_skew_252_std_10_252',\n",
       " 'US_Stock_DE_adj_high_5_sortino_std_90_252',\n",
       " 'target_97_return_lag_4_std_90_252',\n",
       " 'US_Stock_WPM_obv_90_std_252_90',\n",
       " 'US_Stock_CVX_adj_low_std_90_10',\n",
       " 'target_262_auto_corr_90_std_252_10',\n",
       " 'US_Stock_SHEL_adj_close_vol_90_std_10_90',\n",
       " 'LME_ZS_Close_log_ret_US_Stock_MS_adj_close_log_ret_sub_std_252_90',\n",
       " 'US_Stock_IEMG_adj_high_log_ret_return_skew_252_std_10_252',\n",
       " 'target_1_return_lag_4_std_10_90',\n",
       " 'US_Stock_MS_adj_close_log_ret_LME_PB_Close_log_ret_sigmoid_diff_std_252_252',\n",
       " 'FX_GBPAUD_log_ret_auto_corr_10_std_90_90',\n",
       " 'US_Stock_DE_adj_low_10_sortino_std_90_252',\n",
       " 'target_46_return_lag_4_std_10_90',\n",
       " 'LME_ZS_Close_log_ret_US_Stock_MS_adj_close_log_ret_sub_std_10_252',\n",
       " 'US_Stock_ENB_adj_close_log_ret_return_lag_5_std_10_252',\n",
       " 'US_Stock_DE_adj_open_10_sortino_std_90_90',\n",
       " 'target_385_return_skew_5_std_10_252',\n",
       " 'target_1_return_lag_4_std_252_252',\n",
       " 'target_354_return_skew_252_std_10_90',\n",
       " 'US_Stock_SLB_adj_open_log_ret_return_skew_5_std_10_252',\n",
       " 'US_Stock_JNK_adj_close_log_ret_return_skew_10_std_90_10',\n",
       " 'target_381_return_lag_1_std_90_10',\n",
       " 'FX_ZARCHF_log_ret_JPX_Platinum_Standard_Futures_Close_log_ret_log_mul_std_10_90',\n",
       " 'JPX_Platinum_Standard_Futures_Close_log_ret_FX_ZARUSD_log_ret_atan_ratio_std_10_252',\n",
       " 'target_322_return_skew_90_std_10_10',\n",
       " 'US_Stock_WPM_obv_90_std_252_252',\n",
       " 'US_Stock_KGC_obv_90_std_252_10',\n",
       " 'US_Stock_MS_adj_low_vol_90_std_90_90',\n",
       " 'FX_EURGBP_log_ret_auto_corr_252_std_10_90',\n",
       " 'target_416_auto_corr_90_std_90_252',\n",
       " 'target_101_auto_corr_90_std_10_252',\n",
       " 'FX_GBPAUD_log_ret_auto_corr_10_std_90_10',\n",
       " 'LME_ZS_Close_log_ret_US_Stock_MS_adj_close_log_ret_exp_diff_std_90_252',\n",
       " 'target_363_return_skew_10_std_10_90',\n",
       " 'target_36_return_lag_4_std_90_90',\n",
       " 'US_Stock_SCCO_adj_high_log_ret_return_skew_10_std_90_90',\n",
       " 'FX_NOKJPY_log_ret_LME_PB_Close_log_ret_harmonic_mean_std_10_10',\n",
       " 'US_Stock_KGC_obv_90_std_252_90',\n",
       " 'target_179_return_skew_10_std_90_10',\n",
       " 'JPX_Gold_Mini_Futures_open_interest_std_90_10',\n",
       " 'US_Stock_DE_adj_high_sma_10_std_10_252',\n",
       " 'US_Stock_DE_adj_open_std_90_252',\n",
       " 'target_415_return_lag_1_std_90_252',\n",
       " 'US_Stock_XLB_adj_open_log_ret_return_skew_90_std_90_10',\n",
       " 'FX_GBPAUD_log_ret_auto_corr_10_std_252_10',\n",
       " 'target_251_auto_corr_90_std_90_90',\n",
       " 'US_Stock_DVN_adj_close_log_ret_LME_PB_Close_log_ret_norm_diff_std_252_252',\n",
       " 'target_36_return_lag_4_std_10_252',\n",
       " 'target_1_return_lag_4_std_90_90',\n",
       " 'US_Stock_DE_adj_open_sma_10_std_90_90',\n",
       " 'FX_GBPAUD_log_ret_auto_corr_10_std_252_252',\n",
       " 'US_Stock_DE_adj_open_10_sortino_std_90_252',\n",
       " 'LME_AH_Close_vol_252_std_10_10',\n",
       " 'FX_GBPAUD_log_ret_auto_corr_10_std_252_90',\n",
       " 'target_385_return_skew_5_std_252_90',\n",
       " 'US_Stock_DE_adj_high_sma_10_std_90_252',\n",
       " 'target_96_return_lag_4_std_90_90',\n",
       " 'target_416_auto_corr_90_std_90_90',\n",
       " 'US_Stock_DE_adj_low_sma_10_std_90_252',\n",
       " 'US_Stock_DE_adj_high_90_sharpe_std_252_252',\n",
       " 'US_Stock_MS_adj_close_log_ret_LME_PB_Close_log_ret_exp_diff_std_90_10',\n",
       " 'target_96_return_lag_4_std_10_90',\n",
       " 'US_Stock_CVX_adj_close_sma_5_std_252_10',\n",
       " 'US_Stock_IAU_adj_low_log_ret_return_skew_10_std_10_252',\n",
       " 'US_Stock_SHEL_adj_high_vol_90_std_10_90',\n",
       " 'ranked_FX_GBPNZD_log_ret_LME_CA_Close_log_ret_abs_diff_std_90_10',\n",
       " 'target_292_return_lag_5_std_90_90',\n",
       " 'US_Stock_DE_adj_close_90_sortino_std_10_90',\n",
       " 'target_6_return_lag_2_std_90_90',\n",
       " 'target_179_return_skew_10_std_252_10',\n",
       " 'US_Stock_DE_adj_open_sma_5_std_90_252',\n",
       " 'target_292_return_lag_5_std_252_90',\n",
       " 'target_25_return_lag_4_std_90_252',\n",
       " 'JPX_Platinum_Standard_Futures_Close_log_ret_FX_ZARUSD_log_ret_mul_std_10_90',\n",
       " 'target_6_return_lag_2_std_10_252',\n",
       " 'US_Stock_YINN_adj_open_log_ret_return_skew_252_std_90_90',\n",
       " 'target_381_return_lag_1_std_252_10',\n",
       " 'ranked_JPX_Gold_Standard_Futures_Close_log_ret_FX_EURCHF_log_ret_norm_diff_std_252_10',\n",
       " 'US_Stock_DE_adj_low_sma_10_std_90_90',\n",
       " 'LME_ZS_Close_log_ret_US_Stock_MS_adj_close_log_ret_sigmoid_diff_std_252_252',\n",
       " 'ranked_LME_CA_Close_log_ret_US_Stock_VYM_adj_close_log_ret_log_ratio_std_90_10',\n",
       " 'US_Stock_DE_adj_close_sma_90_std_10_90',\n",
       " 'target_25_return_lag_4_std_252_252',\n",
       " 'target_82_auto_corr_252_std_10_10',\n",
       " 'target_251_auto_corr_90_std_90_252',\n",
       " 'US_Stock_ALB_adj_open_sma_252_std_252_10',\n",
       " 'target_264_return_lag_5_std_252_10',\n",
       " 'target_46_return_lag_4_std_10_252',\n",
       " 'JPX_Platinum_Standard_Futures_Close_log_ret_US_Stock_JNK_adj_close_log_ret_harmonic_mean_std_252_10',\n",
       " 'US_Stock_KGC_adj_high_log_ret_return_lag_3_std_252_10',\n",
       " 'LME_PB_Close_log_ret_FX_NZDUSD_log_ret_sqrt_mul_std_90_90',\n",
       " 'US_Stock_DE_adj_open_5_sortino_std_90_252',\n",
       " 'target_390_return_skew_10_std_10_10',\n",
       " 'US_Stock_DE_adj_high_sma_5_std_90_252',\n",
       " 'target_96_return_lag_4_std_90_252',\n",
       " 'US_Stock_CVX_adj_close_5_sortino_std_252_10',\n",
       " 'US_Stock_OXY_adj_low_log_ret_auto_corr_10_std_10_90',\n",
       " 'FX_ZARCHF_log_ret_JPX_Platinum_Standard_Futures_Close_log_ret_log_mul_std_10_10',\n",
       " 'US_Stock_OXY_adj_high_log_ret_auto_corr_10_std_10_90',\n",
       " 'US_Stock_KGC_obv_90_std_90_10',\n",
       " 'target_6_return_lag_2_std_252_252',\n",
       " 'JPX_Gold_Mini_Futures_open_interest_std_90_90',\n",
       " 'US_Stock_JNK_adj_low_log_ret_return_skew_252_std_10_90',\n",
       " 'target_392_auto_corr_252_std_90_10',\n",
       " 'target_292_return_lag_5_std_252_10',\n",
       " 'target_385_return_skew_5_std_90_252',\n",
       " 'US_Stock_DE_adj_low_std_90_90',\n",
       " 'US_Stock_DE_adj_open_sma_10_std_90_252',\n",
       " 'US_Stock_DE_adj_close_5_sortino_std_90_252',\n",
       " 'ranked_FX_GBPNZD_log_ret_LME_CA_Close_log_ret_abs_diff_std_252_10',\n",
       " 'FX_ZARCHF_5_sortino_std_10_90',\n",
       " 'US_Stock_SHEL_adj_low_vol_90_std_10_252',\n",
       " 'US_Stock_IAU_adj_low_log_ret_return_skew_10_std_10_90',\n",
       " 'US_Stock_BSV_adj_open_log_ret_return_lag_3_std_10_90',\n",
       " 'US_Stock_WPM_adj_high_log_ret_return_skew_90_std_10_10',\n",
       " 'FX_NOKEUR_vol_90_std_10_90',\n",
       " 'target_385_return_skew_5_std_252_252',\n",
       " 'FX_ZARCHF_log_ret_JPX_Platinum_Standard_Futures_Close_log_ret_mul_std_10_90',\n",
       " 'US_Stock_OXY_adj_low_log_ret_auto_corr_10_std_10_10',\n",
       " 'target_263_return_lag_5_std_90_10',\n",
       " 'US_Stock_MS_adj_close_vol_90_std_90_90',\n",
       " 'US_Stock_DVN_adj_close_log_ret_LME_PB_Close_log_ret_norm_diff_std_90_252',\n",
       " 'target_358_auto_corr_90_std_90_252',\n",
       " 'US_Stock_MS_adj_close_log_ret_LME_PB_Close_log_ret_sub_std_252_252',\n",
       " 'target_381_return_lag_1_std_90_252',\n",
       " 'US_Stock_ENB_adj_close_log_ret_return_lag_5_std_252_252',\n",
       " 'JPX_Platinum_Standard_Futures_Close_log_ret_FX_ZARUSD_log_ret_log_mul_std_10_90',\n",
       " 'US_Stock_DE_adj_high_sma_10_std_90_90',\n",
       " 'target_346_return_skew_252_std_10_10',\n",
       " 'target_264_return_lag_5_std_252_90',\n",
       " 'LME_ZS_Close_log_ret_US_Stock_MS_adj_close_log_ret_exp_diff_std_252_252',\n",
       " 'US_Stock_DE_adj_low_10_sortino_std_90_90',\n",
       " 'target_46_return_lag_4_std_90_252',\n",
       " 'US_Stock_MS_adj_close_log_ret_LME_PB_Close_log_ret_exp_diff_std_90_252',\n",
       " 'JPX_Platinum_Standard_Futures_Close_log_ret_FX_ZARUSD_log_ret_atan_ratio_std_10_10',\n",
       " 'FX_NZDUSD_log_ret_LME_PB_Close_log_ret_sqrt_mul_std_90_90',\n",
       " 'LME_ZS_Close_log_ret_auto_corr_252_std_90_10',\n",
       " 'target_6_return_lag_2_std_10_90',\n",
       " 'US_Stock_MS_adj_close_log_ret_LME_PB_Close_log_ret_exp_diff_std_252_252',\n",
       " 'LME_ZS_Close_log_ret_US_Stock_MS_adj_close_log_ret_exp_diff_std_90_90',\n",
       " 'US_Stock_DE_adj_high_10_sortino_std_90_90',\n",
       " 'FX_ZAREUR_sma_5_std_10_90',\n",
       " 'target_385_return_skew_5_std_10_10',\n",
       " 'target_42_auto_corr_10_std_10_90',\n",
       " 'US_Stock_MS_adj_close_log_ret_LME_PB_Close_log_ret_sub_std_90_252',\n",
       " 'US_Stock_DE_adj_close_90_sharpe_std_252_252',\n",
       " 'US_Stock_MS_adj_close_log_ret_LME_PB_Close_log_ret_sigmoid_diff_std_90_252',\n",
       " 'LME_ZS_Close_log_ret_US_Stock_MS_adj_close_log_ret_exp_diff_std_252_90',\n",
       " 'US_Stock_DE_adj_open_sma_90_std_10_90',\n",
       " 'ranked_US_Stock_OKE_adj_close_log_ret_LME_ZS_Close_log_ret_poly3_std_10_252',\n",
       " 'FX_ZARCHF_sma_5_std_10_90',\n",
       " 'US_Stock_GDXJ_obv_90_std_252_90',\n",
       " 'US_Stock_CAT_adj_open_log_ret_return_skew_90_std_90_10',\n",
       " 'FX_AUDCHF_log_ret_return_skew_10_std_10_252',\n",
       " 'US_Stock_DE_adj_high_252_sharpe_std_90_252',\n",
       " 'LME_ZS_Close_log_ret_US_Stock_MS_adj_close_log_ret_sub_std_90_90',\n",
       " 'US_Stock_OXY_adj_high_log_ret_auto_corr_10_std_10_252',\n",
       " 'US_Stock_CVX_adj_close_std_90_10',\n",
       " 'US_Stock_JNK_adj_close_log_ret_JPX_Platinum_Standard_Futures_Close_log_ret_harmonic_mean_std_252_10',\n",
       " 'US_Stock_KGC_adj_high_log_ret_return_lag_3_std_90_10',\n",
       " 'US_Stock_SHEL_adj_close_vol_90_std_10_252',\n",
       " 'target_97_return_lag_4_std_90_90',\n",
       " 'target_385_return_skew_5_std_90_90',\n",
       " 'US_Stock_KGC_obv_90_std_252_252',\n",
       " 'US_Stock_DE_adj_close_sma_5_std_90_252',\n",
       " 'target_255_return_skew_10_std_10_10',\n",
       " 'target_6_return_lag_2_std_90_252',\n",
       " 'US_Stock_DE_adj_low_sma_90_std_10_90',\n",
       " 'ranked_US_Stock_GLD_adj_close_log_ret_LME_PB_Close_log_ret_exp_diff_std_252_10',\n",
       " 'FX_NOKGBP_log_ret_LME_ZS_Close_log_ret_log_ratio_std_90_10',\n",
       " 'US_Stock_DE_adj_close_10_sortino_std_90_252',\n",
       " 'target_36_return_lag_4_std_90_252',\n",
       " 'LME_ZS_Close_log_ret_US_Stock_MS_adj_close_log_ret_sigmoid_diff_std_90_90',\n",
       " 'target_58_return_lag_4_std_10_252',\n",
       " 'US_Stock_MS_adj_high_vol_90_std_90_90',\n",
       " 'target_358_auto_corr_90_std_90_90',\n",
       " 'US_Stock_SLB_adj_open_log_ret_return_skew_5_std_10_90',\n",
       " 'LME_PB_Close_log_ret_FX_NOKJPY_log_ret_harmonic_mean_std_10_10',\n",
       " 'FX_ZAREUR_5_sortino_std_10_90',\n",
       " 'target_292_return_lag_5_std_90_10',\n",
       " 'US_Stock_CVX_adj_low_std_252_10',\n",
       " 'target_36_return_lag_4_std_252_252',\n",
       " 'target_1_return_lag_2_std_10_10',\n",
       " 'US_Stock_JNK_adj_close_log_ret_return_skew_252_std_10_90',\n",
       " 'US_Stock_FXI_adj_open_log_ret_return_skew_252_std_90_90',\n",
       " 'US_Stock_DE_adj_close_sma_10_std_90_90',\n",
       " 'FX_GBPAUD_log_ret_auto_corr_10_std_90_252',\n",
       " 'US_Stock_CAT_adj_close_log_ret_return_skew_90_std_90_10',\n",
       " 'target_1_return_lag_4_std_252_90',\n",
       " 'target_42_auto_corr_10_std_10_252',\n",
       " 'US_Stock_DE_adj_high_10_sortino_std_10_252',\n",
       " 'US_Stock_ENB_adj_open_log_ret_auto_corr_10_std_90_10',\n",
       " 'US_Stock_DE_adj_low_90_sortino_std_10_90',\n",
       " 'target_97_return_lag_4_std_10_252',\n",
       " 'FX_AUDCHF_log_ret_return_skew_10_std_10_90',\n",
       " 'JPX_Platinum_Mini_Futures_Low_log_ret_return_skew_10_std_10_252',\n",
       " 'US_Stock_GDXJ_obv_90_std_252_252',\n",
       " 'US_Stock_SHEL_adj_open_vol_90_std_10_90',\n",
       " 'FX_ZAREUR_sma_10_std_10_10',\n",
       " 'target_264_return_lag_5_std_90_10',\n",
       " 'US_Stock_DE_adj_low_5_sortino_std_90_252',\n",
       " 'LME_CA_Close_log_ret_US_Stock_BNDX_adj_close_log_ret_log_ratio_std_10_10',\n",
       " 'FX_GBPCAD_log_ret_return_skew_10_std_252_10']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set([j for i in keep for j in i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edmund",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
