{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cbt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if (\n",
    "                    c_min > np.finfo(np.float16).min\n",
    "                    and c_max < np.finfo(np.float16).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif (\n",
    "                    c_min > np.finfo(np.float32).min\n",
    "                    and c_max < np.finfo(np.float32).max\n",
    "                ):\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "    decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "    print(f\"Decreased by {decrease:.2f}%\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom R2 metric for XGBoost\n",
    "def r2_xgb(y_true, y_pred, sample_weight):\n",
    "    r2 = 1 - np.average((y_pred - y_true) ** 2, weights=sample_weight) / (\n",
    "        np.average((y_true) ** 2, weights=sample_weight) + 1e-38\n",
    "    )\n",
    "    return -r2\n",
    "\n",
    "\n",
    "# Custom R2 metric for LightGBM\n",
    "def r2_lgb(y_true, y_pred, sample_weight):\n",
    "    r2 = 1 - np.average((y_pred - y_true) ** 2, weights=sample_weight) / (\n",
    "        np.average((y_true) ** 2, weights=sample_weight) + 1e-38\n",
    "    )\n",
    "    return \"r2\", r2, True\n",
    "\n",
    "\n",
    "# Custom R2 metric for CatBoost\n",
    "class r2_cbt(object):\n",
    "    def get_final_error(self, error, weight):\n",
    "        return 1 - error / (weight + 1e-38)\n",
    "\n",
    "    def is_max_optimal(self):\n",
    "        return True\n",
    "\n",
    "    def evaluate(self, approxes, target, weight):\n",
    "        assert len(approxes) == 1\n",
    "        assert len(target) == len(approxes[0])\n",
    "\n",
    "        approx = approxes[0]\n",
    "\n",
    "        error_sum = 0.0\n",
    "        weight_sum = 0.0\n",
    "\n",
    "        for i in range(len(approx)):\n",
    "            w = 1.0 if weight is None else weight[i]\n",
    "            weight_sum += w * (target[i] ** 2)\n",
    "            error_sum += w * ((approx[i] - target[i]) ** 2)\n",
    "\n",
    "        return error_sum, weight_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"responder_6\"\n",
    "main = f\"C:/Users/edmun/OneDrive/Desktop/Personal-Projects/Kaggle/Jane Street Real Time Market Data Forecasting\"\n",
    "\n",
    "TRAINING = False\n",
    "\n",
    "LGB_Params = {\n",
    "    \"n_estimators\": 500,\n",
    "    \"device\": \"gpu\",\n",
    "    \"gpu_use_dp\": True,\n",
    "    \"objective\": \"l2\",\n",
    "    \"n_jobs\": 2,\n",
    "}\n",
    "\n",
    "XGB_Params = {\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"max_depth\": 6,\n",
    "    \"n_estimators\": 200,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"reg_alpha\": 1,\n",
    "    \"reg_lambda\": 5,\n",
    "    \"eval_metric\": r2_xgb,\n",
    "    \"disable_default_eval_metric\": True,\n",
    "    \"device\": \"cuda\",\n",
    "    \"early_stopping_rounds\": 100,\n",
    "}\n",
    "\n",
    "CBT_Params = {\n",
    "    \"iterations\": 1000,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"task_type\": \"GPU\",\n",
    "    \"loss_function\": \"RMSE\",\n",
    "    \"eval_metric\": r2_cbt(),\n",
    "}\n",
    "\n",
    "# Number of dates to skip from the beginning of the dataset\n",
    "skip_dates = 500\n",
    "\n",
    "# Global variable for fitted models for the test set inference\n",
    "fitted_models = []\n",
    "\n",
    "# Define the feature names based on the number of features (79 in this case)\n",
    "feature_names = [f\"feature_{i:02d}\" for i in range(79)]\n",
    "exogeneous_features = [\n",
    "    \"sin_time_id\",\n",
    "    \"cos_time_id\",\n",
    "    \"sin_time_id_halfday\",\n",
    "    \"cos_time_id_halfday\",\n",
    "]\n",
    "\n",
    "N_fold = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = pd.read_parquet(f\"{main}/data/training_data/X_valid.parquet\")\n",
    "y_valid = (\n",
    "    pd.read_parquet(f\"{main}/data/training_data/y_valid.parquet\").to_numpy().flatten()\n",
    ")\n",
    "w_valid = (\n",
    "    pd.read_parquet(f\"{main}/data/training_data/w_valid.parquet\").to_numpy().flatten()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"XGB\": xgb.XGBRegressor(**XGB_Params),\n",
    "    \"LGB\": lgb.LGBMRegressor(**LGB_Params),\n",
    "    \"CBT\": cbt.CatBoostRegressor(**CBT_Params),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_models(model_dict, fold):\n",
    "    X_train = reduce_mem_usage(\n",
    "        pd.read_parquet(f\"{main}/data/training_data/X_train_{fold}.parquet\")\n",
    "    )\n",
    "    y_train = (\n",
    "        reduce_mem_usage(\n",
    "            pd.read_parquet(f\"{main}/data/training_data/y_train_{fold}.parquet\")\n",
    "        )\n",
    "        .to_numpy()\n",
    "        .flatten()\n",
    "    )\n",
    "    w_train = (\n",
    "        reduce_mem_usage(\n",
    "            pd.read_parquet(f\"{main}/data/training_data/w_train_{fold}.parquet\")\n",
    "        )\n",
    "        .to_numpy()\n",
    "        .flatten()\n",
    "    )\n",
    "\n",
    "    for name, model in model_dict.items():\n",
    "        if name == \"XGB\":\n",
    "            model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                sample_weight=w_train,\n",
    "                eval_set=[(X_valid, y_valid)],\n",
    "                sample_weight_eval_set=[w_valid],\n",
    "                verbose=10,\n",
    "            )\n",
    "\n",
    "        if name == \"LGB\":\n",
    "            model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                w_train,\n",
    "                eval_metric=[r2_lgb],\n",
    "                eval_set=[(X_valid, y_valid, w_valid)],\n",
    "                callbacks=[lgb.early_stopping(100), lgb.log_evaluation(10)],\n",
    "            )\n",
    "\n",
    "        if name == \"CBT\":\n",
    "            evalset = cbt.Pool(X_valid, y_valid, weight=w_valid)\n",
    "\n",
    "            model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                sample_weight=w_train,\n",
    "                eval_set=[evalset],\n",
    "                verbose=10,\n",
    "                early_stopping_rounds=100,\n",
    "            )\n",
    "\n",
    "        model_filename = f\"{main}/Scaled_Models/{name}/{name}_{fold+1}.pkl\"\n",
    "        joblib.dump(model, model_filename)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    del (\n",
    "        X_train,\n",
    "        y_train,\n",
    "        w_train,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(N_fold):\n",
    "    training_models(model_dict=model_dict, fold=fold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
