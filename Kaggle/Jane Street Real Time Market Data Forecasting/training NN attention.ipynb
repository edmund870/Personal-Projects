{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "import gc\n",
    "\n",
    "\n",
    "from utils import reduce_memory, config\n",
    "\n",
    "CONFIG = config.CONFIG\n",
    "columns = CONFIG.feature_names + CONFIG.exogeneous_features + CONFIG.lag_features\n",
    "\n",
    "tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "# for gpu in gpus:\n",
    "#     tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class R2Metric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"r2\", **kwargs):\n",
    "        super(R2Metric, self).__init__(name=name, **kwargs)\n",
    "        self.squared_error = self.add_weight(name=\"squared_error\", initializer=\"zeros\")\n",
    "        self.total_error = self.add_weight(name=\"total_error\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Calculate squared error\n",
    "        y_true = tf.cast(tf.squeeze(y_true), dtype=tf.float32)\n",
    "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "        sample_weight = tf.cast(tf.squeeze(sample_weight), dtype=tf.float32)\n",
    "        squared_error = (y_pred - y_true) ** 2\n",
    "        total_error = y_true**2\n",
    "\n",
    "        # Update the total squared error, total error, and total weight\n",
    "        self.squared_error.assign_add(tf.reduce_sum(squared_error * sample_weight))\n",
    "        self.total_error.assign_add(tf.reduce_sum(total_error * sample_weight))\n",
    "\n",
    "    def result(self):\n",
    "        # Compute RÂ²: 1 - (squared_error / total_error)\n",
    "        return 1 - (self.squared_error / (self.total_error + 1e-38))\n",
    "\n",
    "    def reset_state(self):\n",
    "        # Reset all metrics at the end of each epoch\n",
    "        self.squared_error.assign(0)\n",
    "        self.total_error.assign(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(inputs, inputs)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"silu\")(res)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    return x + res\n",
    "\n",
    "\n",
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size=128,\n",
    "    num_heads=8,\n",
    "    ff_dim=128,\n",
    "    num_transformer_blocks=2,\n",
    "    mlp_units=[128],\n",
    "    mlp_dropout=0.1,\n",
    "    dropout=0.25,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    for units in mlp_units:\n",
    "        x = layers.Dense(units, activation=\"silu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "\n",
    "    outputs = layers.Dense(1, activation=\"tanh\")(x)\n",
    "    outputs = Lambda(lambda x: x * 5)(outputs)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-3, decay=5e-4),\n",
    "        loss=\"mean_squared_error\",\n",
    "        weighted_metrics=[R2Metric()],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_features_file_path = f\"{CONFIG.main}/data/training_data_w_lag/X_valid.parquet\"\n",
    "valid_labels_file_path = f\"{CONFIG.main}/data/training_data_w_lag/y_valid.parquet\"\n",
    "valid_weights_file_path = f\"{CONFIG.main}/data/training_data_w_lag/w_valid.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 730.06 MB\n",
      "Memory usage after optimization is: 645.01 MB\n",
      "Decreased by 11.65%\n",
      "Memory usage of dataframe is 7.09 MB\n",
      "Memory usage after optimization is: 7.09 MB\n",
      "Decreased by 0.00%\n",
      "Memory usage of dataframe is 7.09 MB\n",
      "Memory usage after optimization is: 7.09 MB\n",
      "Decreased by 0.00%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_len = 256\n",
    "feature_len = len(columns)\n",
    "features_shape = (seq_len, feature_len)\n",
    "epochs = 2_000\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    ")\n",
    "\n",
    "features_batch = reduce_memory.reduce_mem_usage(\n",
    "    pd.read_parquet(valid_features_file_path, columns=columns).fillna(0)\n",
    ").values\n",
    "\n",
    "labels_batch = reduce_memory.reduce_mem_usage(\n",
    "    pd.read_parquet(valid_labels_file_path).fillna(0)\n",
    ").values.squeeze()\n",
    "weights_batch = reduce_memory.reduce_mem_usage(\n",
    "    pd.read_parquet(valid_weights_file_path).fillna(0)\n",
    ").values.squeeze()\n",
    "\n",
    "original_size = features_batch.size\n",
    "target_elements = seq_len * feature_len\n",
    "if original_size % target_elements != 0:\n",
    "    num_elements_to_keep = int(\n",
    "        (original_size // target_elements) * target_elements / feature_len\n",
    "    )\n",
    "    features_batch = features_batch[:num_elements_to_keep]\n",
    "    labels_batch = labels_batch[:num_elements_to_keep]\n",
    "    weights_batch = weights_batch[:num_elements_to_keep]\n",
    "\n",
    "features_batch = features_batch.reshape(-1, seq_len, feature_len)\n",
    "labels_batch = labels_batch.reshape(-1, seq_len, 1)\n",
    "weights_batch = weights_batch.reshape(-1, seq_len, 1)\n",
    "\n",
    "with tf.device(\"/CPU:0\"):\n",
    "    valid_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (features_batch, labels_batch, weights_batch)\n",
    "    )\n",
    "    valid_dataset = valid_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Memory usage of dataframe is 2266.00 MB\n",
      "Memory usage after optimization is: 2002.00 MB\n",
      "Decreased by 11.65%\n",
      "Memory usage of dataframe is 22.00 MB\n",
      "Memory usage after optimization is: 22.00 MB\n",
      "Decreased by 0.00%\n",
      "Memory usage of dataframe is 22.00 MB\n",
      "Memory usage after optimization is: 22.00 MB\n",
      "Decreased by 0.00%\n",
      "Epoch 1/2000\n",
      "352/352 [==============================] - 107s 294ms/step - loss: 35.3350 - r2: -25.1235 - val_loss: 12.0858 - val_r2: -7.1653\n",
      "Epoch 2/2000\n",
      "352/352 [==============================] - 103s 293ms/step - loss: 3.5426 - r2: -1.5212 - val_loss: 1.5628 - val_r2: -0.0042\n",
      "Epoch 3/2000\n",
      "352/352 [==============================] - 102s 289ms/step - loss: 1.5468 - r2: -0.0390 - val_loss: 1.5585 - val_r2: -0.0011\n",
      "Epoch 4/2000\n",
      "352/352 [==============================] - 102s 289ms/step - loss: 1.5182 - r2: -0.0172 - val_loss: 1.5576 - val_r2: -4.0078e-04\n",
      "Epoch 5/2000\n",
      "352/352 [==============================] - 101s 288ms/step - loss: 1.5066 - r2: -0.0085 - val_loss: 1.5581 - val_r2: -7.8678e-04\n",
      "Epoch 6/2000\n",
      "352/352 [==============================] - 99s 282ms/step - loss: 1.5013 - r2: -0.0049 - val_loss: 1.5581 - val_r2: -8.2731e-04\n",
      "Epoch 7/2000\n",
      "352/352 [==============================] - 100s 284ms/step - loss: 1.4991 - r2: -0.0031 - val_loss: 1.5587 - val_r2: -0.0013\n",
      "Epoch 8/2000\n",
      "352/352 [==============================] - 101s 288ms/step - loss: 1.4973 - r2: -0.0018 - val_loss: 1.5587 - val_r2: -0.0013\n",
      "Epoch 9/2000\n",
      "352/352 [==============================] - 101s 288ms/step - loss: 1.4962 - r2: -8.7905e-04 - val_loss: 1.5586 - val_r2: -0.0012\n",
      "11\n",
      "Memory usage of dataframe is 2266.00 MB\n",
      "Memory usage after optimization is: 2002.00 MB\n",
      "Decreased by 11.65%\n",
      "Memory usage of dataframe is 22.00 MB\n",
      "Memory usage after optimization is: 22.00 MB\n",
      "Decreased by 0.00%\n",
      "Memory usage of dataframe is 22.00 MB\n",
      "Memory usage after optimization is: 22.00 MB\n",
      "Decreased by 0.00%\n",
      "Epoch 1/2000\n",
      "352/352 [==============================] - 100s 282ms/step - loss: 1.5929 - r2: -0.0032 - val_loss: 1.5595 - val_r2: -0.0016\n",
      "Epoch 2/2000\n",
      "352/352 [==============================] - 100s 284ms/step - loss: 1.5908 - r2: -0.0016 - val_loss: 1.5597 - val_r2: -0.0018\n",
      "Epoch 3/2000\n",
      "352/352 [==============================] - 100s 285ms/step - loss: 1.5880 - r2: 1.5634e-04 - val_loss: 1.5586 - val_r2: -9.9432e-04\n",
      "Epoch 4/2000\n",
      "352/352 [==============================] - 100s 284ms/step - loss: 1.5879 - r2: 2.0361e-04 - val_loss: 1.5581 - val_r2: -6.8152e-04\n",
      "Epoch 5/2000\n",
      "352/352 [==============================] - 95s 269ms/step - loss: 1.5870 - r2: 6.9493e-04 - val_loss: 1.5568 - val_r2: 2.4158e-04\n",
      "Epoch 6/2000\n",
      "352/352 [==============================] - 88s 251ms/step - loss: 1.5863 - r2: 0.0014 - val_loss: 1.5572 - val_r2: -2.7895e-05\n",
      "Epoch 7/2000\n",
      "352/352 [==============================] - 88s 250ms/step - loss: 1.5853 - r2: 0.0020 - val_loss: 1.5570 - val_r2: 1.2571e-04\n",
      "Epoch 8/2000\n",
      "352/352 [==============================] - 88s 250ms/step - loss: 1.5849 - r2: 0.0022 - val_loss: 1.5565 - val_r2: 3.9935e-04\n",
      "Epoch 9/2000\n",
      "352/352 [==============================] - 88s 250ms/step - loss: 1.5844 - r2: 0.0024 - val_loss: 1.5563 - val_r2: 5.0628e-04\n",
      "Epoch 10/2000\n",
      "352/352 [==============================] - 87s 248ms/step - loss: 1.5835 - r2: 0.0031 - val_loss: 1.5560 - val_r2: 7.4273e-04\n",
      "Epoch 11/2000\n",
      "352/352 [==============================] - 88s 251ms/step - loss: 1.5829 - r2: 0.0033 - val_loss: 1.5554 - val_r2: 0.0011\n",
      "Epoch 12/2000\n",
      "352/352 [==============================] - 88s 250ms/step - loss: 1.5818 - r2: 0.0041 - val_loss: 1.5560 - val_r2: 6.8796e-04\n",
      "Epoch 13/2000\n",
      "352/352 [==============================] - 88s 249ms/step - loss: 1.5805 - r2: 0.0050 - val_loss: 1.5576 - val_r2: -4.2784e-04\n",
      "Epoch 14/2000\n",
      "352/352 [==============================] - 89s 253ms/step - loss: 1.5797 - r2: 0.0055 - val_loss: 1.5686 - val_r2: -0.0077\n",
      "Epoch 15/2000\n",
      "352/352 [==============================] - 84s 238ms/step - loss: 1.5826 - r2: 0.0032 - val_loss: 1.5613 - val_r2: -0.0028\n",
      "Epoch 16/2000\n",
      "352/352 [==============================] - 79s 223ms/step - loss: 1.5790 - r2: 0.0060 - val_loss: 1.5584 - val_r2: -9.0289e-04\n",
      "22\n",
      "Memory usage of dataframe is 2266.00 MB\n",
      "Memory usage after optimization is: 2002.00 MB\n",
      "Decreased by 11.65%\n",
      "Memory usage of dataframe is 22.00 MB\n",
      "Memory usage after optimization is: 22.00 MB\n",
      "Decreased by 0.00%\n",
      "Memory usage of dataframe is 22.00 MB\n",
      "Memory usage after optimization is: 22.00 MB\n",
      "Decreased by 0.00%\n",
      "Epoch 1/2000\n",
      "352/352 [==============================] - 81s 229ms/step - loss: 1.6613 - r2: 2.4348e-04 - val_loss: 1.5566 - val_r2: 3.2949e-04\n",
      "Epoch 2/2000\n",
      "352/352 [==============================] - 80s 228ms/step - loss: 1.6594 - r2: 0.0011 - val_loss: 1.5565 - val_r2: 3.5834e-04\n",
      "Epoch 3/2000\n",
      "352/352 [==============================] - 81s 230ms/step - loss: 1.6582 - r2: 0.0021 - val_loss: 1.5562 - val_r2: 5.3108e-04\n",
      "Epoch 4/2000\n",
      "352/352 [==============================] - 80s 229ms/step - loss: 1.6575 - r2: 0.0024 - val_loss: 1.5566 - val_r2: 2.4915e-04\n",
      "Epoch 5/2000\n",
      "352/352 [==============================] - 81s 229ms/step - loss: 1.6562 - r2: 0.0035 - val_loss: 1.5562 - val_r2: 5.1135e-04\n",
      "Epoch 6/2000\n",
      "352/352 [==============================] - 82s 232ms/step - loss: 1.6552 - r2: 0.0040 - val_loss: 1.5565 - val_r2: 3.0023e-04\n",
      "Epoch 7/2000\n",
      "352/352 [==============================] - 81s 230ms/step - loss: 1.6539 - r2: 0.0048 - val_loss: 1.5566 - val_r2: 2.4074e-04\n",
      "Epoch 8/2000\n",
      "352/352 [==============================] - 81s 230ms/step - loss: 1.6529 - r2: 0.0055 - val_loss: 1.5582 - val_r2: -7.2861e-04\n",
      "Epoch 9/2000\n",
      "352/352 [==============================] - 79s 224ms/step - loss: 1.6519 - r2: 0.0060 - val_loss: 1.5624 - val_r2: -0.0034\n",
      "Epoch 10/2000\n",
      "352/352 [==============================] - 79s 225ms/step - loss: 1.6509 - r2: 0.0066 - val_loss: 1.5628 - val_r2: -0.0037\n",
      "33\n",
      "Memory usage of dataframe is 23.13 MB\n",
      "Memory usage after optimization is: 20.43 MB\n",
      "Decreased by 11.65%\n",
      "Memory usage of dataframe is 0.22 MB\n",
      "Memory usage after optimization is: 0.22 MB\n",
      "Decreased by 0.00%\n",
      "Memory usage of dataframe is 0.22 MB\n",
      "Memory usage after optimization is: 0.22 MB\n",
      "Decreased by 0.00%\n",
      "Epoch 1/2000\n",
      "4/4 [==============================] - 9s 3s/step - loss: 2.8058 - r2: 0.0063 - val_loss: 1.5560 - val_r2: 6.9892e-04\n",
      "Epoch 2/2000\n",
      "4/4 [==============================] - 8s 3s/step - loss: 2.8043 - r2: 0.0070 - val_loss: 1.5559 - val_r2: 8.1944e-04\n",
      "Epoch 3/2000\n",
      "4/4 [==============================] - 8s 3s/step - loss: 2.7996 - r2: 0.0083 - val_loss: 1.5558 - val_r2: 8.7762e-04\n",
      "Epoch 4/2000\n",
      "4/4 [==============================] - 8s 3s/step - loss: 2.7989 - r2: 0.0083 - val_loss: 1.5558 - val_r2: 8.1110e-04\n",
      "Epoch 5/2000\n",
      "4/4 [==============================] - 8s 3s/step - loss: 2.7976 - r2: 0.0085 - val_loss: 1.5562 - val_r2: 4.9990e-04\n",
      "Epoch 6/2000\n",
      "4/4 [==============================] - 8s 3s/step - loss: 2.7952 - r2: 0.0087 - val_loss: 1.5566 - val_r2: 2.4486e-04\n",
      "Epoch 7/2000\n",
      "4/4 [==============================] - 8s 3s/step - loss: 2.7980 - r2: 0.0079 - val_loss: 1.5562 - val_r2: 4.8488e-04\n",
      "Epoch 8/2000\n",
      "4/4 [==============================] - 8s 3s/step - loss: 2.7936 - r2: 0.0095 - val_loss: 1.5560 - val_r2: 6.7687e-04\n",
      "0\n",
      "Memory usage of dataframe is 2266.00 MB\n",
      "Memory usage after optimization is: 2002.00 MB\n",
      "Decreased by 11.65%\n",
      "Memory usage of dataframe is 22.00 MB\n",
      "Memory usage after optimization is: 22.00 MB\n",
      "Decreased by 0.00%\n",
      "Memory usage of dataframe is 22.00 MB\n",
      "Memory usage after optimization is: 22.00 MB\n",
      "Decreased by 0.00%\n",
      "Epoch 1/2000\n",
      "352/352 [==============================] - 83s 228ms/step - loss: 37.9525 - r2: -27.3816 - val_loss: 52.6648 - val_r2: -34.7649\n",
      "Epoch 2/2000\n",
      "352/352 [==============================] - 81s 231ms/step - loss: 9.0320 - r2: -5.6614 - val_loss: 2.3170 - val_r2: -0.5161\n",
      "Epoch 3/2000\n",
      "352/352 [==============================] - 79s 226ms/step - loss: 1.7070 - r2: -0.1771 - val_loss: 1.6052 - val_r2: -0.0325\n",
      "Epoch 4/2000\n",
      "352/352 [==============================] - 80s 227ms/step - loss: 1.5528 - r2: -0.0503 - val_loss: 1.5604 - val_r2: -0.0022\n",
      "Epoch 5/2000\n",
      "352/352 [==============================] - 79s 225ms/step - loss: 1.5116 - r2: -0.0194 - val_loss: 1.5594 - val_r2: -0.0015\n",
      "Epoch 6/2000\n",
      "352/352 [==============================] - 80s 226ms/step - loss: 1.4933 - r2: -0.0070 - val_loss: 1.5592 - val_r2: -0.0015\n",
      "Epoch 7/2000\n",
      "352/352 [==============================] - 79s 225ms/step - loss: 1.4881 - r2: -0.0032 - val_loss: 1.5601 - val_r2: -0.0021\n",
      "Epoch 8/2000\n",
      "352/352 [==============================] - 79s 225ms/step - loss: 1.4859 - r2: -0.0017 - val_loss: 1.5591 - val_r2: -0.0015\n",
      "Epoch 9/2000\n",
      "352/352 [==============================] - 80s 226ms/step - loss: 1.4847 - r2: -7.6079e-04 - val_loss: 1.5586 - val_r2: -0.0011\n",
      "Epoch 10/2000\n",
      "352/352 [==============================] - 79s 226ms/step - loss: 1.4843 - r2: -4.6301e-04 - val_loss: 1.5587 - val_r2: -0.0012\n",
      "Epoch 11/2000\n",
      "352/352 [==============================] - 80s 226ms/step - loss: 1.4836 - r2: 4.6015e-05 - val_loss: 1.5582 - val_r2: -8.2457e-04\n",
      "Epoch 12/2000\n",
      "352/352 [==============================] - 79s 225ms/step - loss: 1.4831 - r2: 5.3155e-04 - val_loss: 1.5574 - val_r2: -3.1638e-04\n",
      "Epoch 13/2000\n",
      "352/352 [==============================] - 80s 226ms/step - loss: 1.4824 - r2: 0.0011 - val_loss: 1.5575 - val_r2: -3.8385e-04\n",
      "Epoch 14/2000\n",
      "352/352 [==============================] - 80s 228ms/step - loss: 1.4821 - r2: 0.0012 - val_loss: 1.5566 - val_r2: 2.3299e-04\n",
      "Epoch 15/2000\n",
      "352/352 [==============================] - 80s 227ms/step - loss: 1.4814 - r2: 0.0017 - val_loss: 1.5566 - val_r2: 1.7798e-04\n",
      "Epoch 16/2000\n",
      "352/352 [==============================] - 79s 225ms/step - loss: 1.4813 - r2: 0.0018 - val_loss: 1.5563 - val_r2: 3.9154e-04\n",
      "Epoch 17/2000\n",
      "352/352 [==============================] - 79s 225ms/step - loss: 1.4809 - r2: 0.0023 - val_loss: 1.5561 - val_r2: 5.5468e-04\n",
      "Epoch 18/2000\n",
      "352/352 [==============================] - 79s 225ms/step - loss: 1.4806 - r2: 0.0024 - val_loss: 1.5561 - val_r2: 5.2625e-04\n",
      "Epoch 19/2000\n",
      "352/352 [==============================] - 81s 229ms/step - loss: 1.4802 - r2: 0.0027 - val_loss: 1.5568 - val_r2: 2.0742e-05\n",
      "Epoch 20/2000\n",
      "352/352 [==============================] - 81s 230ms/step - loss: 1.4796 - r2: 0.0032 - val_loss: 1.5562 - val_r2: 4.5085e-04\n",
      "Epoch 21/2000\n",
      "352/352 [==============================] - 82s 233ms/step - loss: 1.4792 - r2: 0.0035 - val_loss: 1.5566 - val_r2: 1.2034e-04\n",
      "Epoch 22/2000\n",
      "352/352 [==============================] - 82s 232ms/step - loss: 1.4785 - r2: 0.0041 - val_loss: 1.5573 - val_r2: -4.0317e-04\n",
      "11\n",
      "Memory usage of dataframe is 2266.00 MB\n",
      "Memory usage after optimization is: 2002.00 MB\n",
      "Decreased by 11.65%\n",
      "Memory usage of dataframe is 22.00 MB\n",
      "Memory usage after optimization is: 22.00 MB\n",
      "Decreased by 0.00%\n",
      "Memory usage of dataframe is 22.00 MB\n",
      "Memory usage after optimization is: 22.00 MB\n",
      "Decreased by 0.00%\n",
      "Epoch 1/2000\n",
      "352/352 [==============================] - 80s 227ms/step - loss: 1.5987 - r2: -0.0012 - val_loss: 1.5578 - val_r2: -5.1129e-04\n",
      "Epoch 2/2000\n",
      "352/352 [==============================] - 81s 231ms/step - loss: 1.5982 - r2: -5.0187e-04 - val_loss: 1.5564 - val_r2: 3.8260e-04\n",
      "Epoch 3/2000\n",
      "352/352 [==============================] - 80s 228ms/step - loss: 1.5969 - r2: 2.8145e-04 - val_loss: 1.5561 - val_r2: 5.1141e-04\n",
      "Epoch 4/2000\n",
      "352/352 [==============================] - 79s 224ms/step - loss: 1.5957 - r2: 0.0011 - val_loss: 1.5565 - val_r2: 1.8340e-04\n",
      "Epoch 5/2000\n",
      "352/352 [==============================] - 77s 219ms/step - loss: 1.5918 - r2: 0.0036 - val_loss: 1.5557 - val_r2: 7.0685e-04\n",
      "Epoch 6/2000\n",
      "352/352 [==============================] - 77s 219ms/step - loss: 1.5921 - r2: 0.0030 - val_loss: 1.5560 - val_r2: 4.6390e-04\n",
      "Epoch 7/2000\n",
      "352/352 [==============================] - 78s 223ms/step - loss: 1.5884 - r2: 0.0057 - val_loss: 1.5566 - val_r2: -2.2650e-06\n",
      "Epoch 8/2000\n",
      "352/352 [==============================] - 78s 221ms/step - loss: 1.5877 - r2: 0.0061 - val_loss: 1.5567 - val_r2: -5.0426e-05\n",
      "Epoch 9/2000\n",
      "352/352 [==============================] - 78s 222ms/step - loss: 1.5863 - r2: 0.0071 - val_loss: 1.5572 - val_r2: -4.3917e-04\n",
      "Epoch 10/2000\n",
      "352/352 [==============================] - 78s 222ms/step - loss: 1.5854 - r2: 0.0075 - val_loss: 1.5573 - val_r2: -5.7340e-04\n",
      "22\n",
      "Memory usage of dataframe is 2266.00 MB\n",
      "Memory usage after optimization is: 2002.00 MB\n",
      "Decreased by 11.65%\n",
      "Memory usage of dataframe is 22.00 MB\n",
      "Memory usage after optimization is: 22.00 MB\n",
      "Decreased by 0.00%\n",
      "Memory usage of dataframe is 22.00 MB\n",
      "Memory usage after optimization is: 22.00 MB\n",
      "Decreased by 0.00%\n",
      "Epoch 1/2000\n",
      "352/352 [==============================] - 78s 222ms/step - loss: 1.6571 - r2: 0.0014 - val_loss: 1.5556 - val_r2: 8.3572e-04\n",
      "Epoch 2/2000\n",
      "352/352 [==============================] - 79s 224ms/step - loss: 1.6553 - r2: 0.0025 - val_loss: 1.5558 - val_r2: 7.5316e-04\n",
      "Epoch 3/2000\n",
      "352/352 [==============================] - 80s 227ms/step - loss: 1.6548 - r2: 0.0029 - val_loss: 1.5556 - val_r2: 8.6820e-04\n",
      "Epoch 4/2000\n",
      "352/352 [==============================] - 79s 225ms/step - loss: 1.6540 - r2: 0.0034 - val_loss: 1.5554 - val_r2: 9.9957e-04\n",
      "Epoch 5/2000\n",
      "352/352 [==============================] - 79s 225ms/step - loss: 1.6535 - r2: 0.0036 - val_loss: 1.5554 - val_r2: 0.0010\n",
      "Epoch 6/2000\n",
      "352/352 [==============================] - 79s 224ms/step - loss: 1.6528 - r2: 0.0042 - val_loss: 1.5551 - val_r2: 0.0012\n",
      "Epoch 7/2000\n",
      "352/352 [==============================] - 80s 227ms/step - loss: 1.6519 - r2: 0.0046 - val_loss: 1.5551 - val_r2: 0.0012\n",
      "Epoch 8/2000\n",
      "352/352 [==============================] - 80s 226ms/step - loss: 1.6508 - r2: 0.0053 - val_loss: 1.5553 - val_r2: 0.0011\n",
      "Epoch 9/2000\n",
      " 97/352 [=======>......................] - ETA: 52s - loss: 1.2337 - r2: 0.0029"
     ]
    }
   ],
   "source": [
    "for fold in range(CONFIG.N_fold):\n",
    "    model = build_model(input_shape=features_shape)\n",
    "\n",
    "    train_features_file_path = (\n",
    "        f\"{CONFIG.main}/data/training_data_w_lag/X_train_{fold}.parquet\"\n",
    "    )\n",
    "    train_labels_file_path = (\n",
    "        f\"{CONFIG.main}/data/training_data_w_lag/y_train_{fold}.parquet\"\n",
    "    )\n",
    "    train_weights_file_path = (\n",
    "        f\"{CONFIG.main}/data/training_data_w_lag/w_train_{fold}.parquet\"\n",
    "    )\n",
    "\n",
    "    # Create the TensorFlow Dataset\n",
    "    train_features_file = pq.ParquetFile(train_features_file_path)\n",
    "    train_labels_file = pq.ParquetFile(train_labels_file_path)\n",
    "    train_weights_file = pq.ParquetFile(train_weights_file_path)\n",
    "\n",
    "    train_row_group = train_features_file.num_row_groups\n",
    "\n",
    "    step = int(train_row_group / 3)\n",
    "\n",
    "    for i in range(0, train_row_group, step):\n",
    "        print(i)\n",
    "        batch_end = min(i + step, train_row_group)\n",
    "        features_batch = reduce_memory.reduce_mem_usage(\n",
    "            train_features_file.read_row_groups(\n",
    "                [i for i in range(i, batch_end)], columns=columns\n",
    "            )\n",
    "            .to_pandas()\n",
    "            .fillna(0)\n",
    "        ).values\n",
    "\n",
    "        original_size = features_batch.size\n",
    "        target_elements = seq_len * feature_len\n",
    "\n",
    "        labels_batch = reduce_memory.reduce_mem_usage(\n",
    "            train_labels_file.read_row_groups([i for i in range(i, batch_end)])\n",
    "            .to_pandas()\n",
    "            .fillna(0)\n",
    "        ).values.squeeze()\n",
    "        weights_batch = reduce_memory.reduce_mem_usage(\n",
    "            train_weights_file.read_row_groups([i for i in range(i, batch_end)])\n",
    "            .to_pandas()\n",
    "            .fillna(0)\n",
    "        ).values.squeeze()\n",
    "\n",
    "        if original_size % target_elements != 0:\n",
    "            num_elements_to_keep = int(\n",
    "                (original_size // target_elements) * target_elements / feature_len\n",
    "            )\n",
    "            features_batch = features_batch[:num_elements_to_keep]\n",
    "            labels_batch = labels_batch[:num_elements_to_keep]\n",
    "            weights_batch = weights_batch[:num_elements_to_keep]\n",
    "\n",
    "        features_batch = features_batch.reshape(-1, seq_len, feature_len)\n",
    "        labels_batch = labels_batch.reshape(-1, seq_len, 1)\n",
    "        weights_batch = weights_batch.reshape(-1, seq_len, 1)\n",
    "\n",
    "        with tf.device(\"/CPU:0\"):\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "                (features_batch, labels_batch, weights_batch)\n",
    "            )\n",
    "            train_dataset = train_dataset.batch(batch_size).prefetch(\n",
    "                tf.data.experimental.AUTOTUNE\n",
    "            )\n",
    "\n",
    "        with tf.device(\"/GPU:0\"):\n",
    "            model.fit(\n",
    "                train_dataset,\n",
    "                epochs=epochs,\n",
    "                validation_data=valid_dataset,\n",
    "                callbacks=[callback],\n",
    "            )\n",
    "\n",
    "        del train_dataset\n",
    "\n",
    "    tf.keras.models.save_model(\n",
    "        model, f\"{CONFIG.main}/Models_impt/NN/NN_attn_{fold+1}.h5\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
