{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "import gc\n",
    "\n",
    "\n",
    "from utils import reduce_memory, config\n",
    "\n",
    "CONFIG = config.CONFIG\n",
    "\n",
    "tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "# for gpu in gpus:\n",
    "#     tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ANN(shape):\n",
    "    inputs = layers.Input(shape=(shape,), name=\"input_layer\")\n",
    "\n",
    "    x1 = layers.BatchNormalization()(inputs)\n",
    "    x1 = layers.Dense(512, activation=None)(x1)\n",
    "    x1 = layers.Activation(\"silu\")(x1)\n",
    "\n",
    "    x2 = layers.Dropout(0.1)(x1)\n",
    "    x2 = layers.BatchNormalization()(x2)\n",
    "    x2 = layers.Dense(512, activation=None)(x2)\n",
    "    x2 = layers.Activation(\"silu\")(x2)\n",
    "\n",
    "    x3 = layers.Dropout(0.1)(x2)\n",
    "    x3 = layers.BatchNormalization()(x3)\n",
    "    x3 = layers.Dense(256, activation=None)(x3)\n",
    "    x3 = layers.Activation(\"silu\")(x3)\n",
    "\n",
    "    outputs = layers.Dense(1, activation=\"tanh\")(x3)\n",
    "    outputs = Lambda(lambda x: x * 5)(outputs)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-4, decay=5e-4),\n",
    "        loss=\"mean_squared_error\",\n",
    "        weighted_metrics=[R2Metric()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "class R2Metric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"r2\", **kwargs):\n",
    "        super(R2Metric, self).__init__(name=name, **kwargs)\n",
    "        self.squared_error = self.add_weight(name=\"squared_error\", initializer=\"zeros\")\n",
    "        self.total_error = self.add_weight(name=\"total_error\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Calculate squared error\n",
    "        y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "        sample_weight = tf.cast(sample_weight, dtype=tf.float32)\n",
    "        squared_error = (y_pred - y_true) ** 2\n",
    "        total_error = y_true**2\n",
    "\n",
    "        # Update the total squared error, total error, and total weight\n",
    "        self.squared_error.assign_add(tf.reduce_sum(squared_error * sample_weight))\n",
    "        self.total_error.assign_add(tf.reduce_sum(total_error * sample_weight))\n",
    "\n",
    "    def result(self):\n",
    "        # Compute RÂ²: 1 - (squared_error / total_error)\n",
    "        return 1 - (self.squared_error / (self.total_error + 1e-38))\n",
    "\n",
    "    def reset_state(self):\n",
    "        # Reset all metrics at the end of each epoch\n",
    "        self.squared_error.assign(0)\n",
    "        self.total_error.assign(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_features_file_path = f\"{CONFIG.main}/data/training_data_impt/X_valid.parquet\"\n",
    "valid_labels_file_path = f\"{CONFIG.main}/data/training_data_impt/y_valid.parquet\"\n",
    "valid_weights_file_path = f\"{CONFIG.main}/data/training_data_impt/w_valid.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 2055.52 MB\n",
      "Memory usage after optimization is: 1183.70 MB\n",
      "Decreased by 42.41%\n",
      "Memory usage of dataframe is 7.09 MB\n",
      "Memory usage after optimization is: 7.09 MB\n",
      "Decreased by 0.00%\n",
      "Memory usage of dataframe is 7.09 MB\n",
      "Memory usage after optimization is: 7.09 MB\n",
      "Decreased by 0.00%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8192\n",
    "features_shape = 159\n",
    "epochs = 2_000\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=25,\n",
    ")\n",
    "\n",
    "features_batch = reduce_memory.reduce_mem_usage(\n",
    "    pd.read_parquet(valid_features_file_path).fillna(0)\n",
    ").values\n",
    "labels_batch = reduce_memory.reduce_mem_usage(\n",
    "    pd.read_parquet(valid_labels_file_path).fillna(0)\n",
    ").values.squeeze()\n",
    "weights_batch = reduce_memory.reduce_mem_usage(\n",
    "    pd.read_parquet(valid_weights_file_path).fillna(0)\n",
    ").values.squeeze()\n",
    "\n",
    "with tf.device(\"/CPU:0\"):\n",
    "    valid_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (features_batch, labels_batch, weights_batch)\n",
    "    )\n",
    "    valid_dataset = valid_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Memory usage of dataframe is 3960.00 MB\n",
      "Memory usage after optimization is: 3696.00 MB\n",
      "Decreased by 6.67%\n",
      "Memory usage of dataframe is 22.00 MB\n",
      "Memory usage after optimization is: 22.00 MB\n",
      "Decreased by 0.00%\n",
      "Memory usage of dataframe is 22.00 MB\n",
      "Memory usage after optimization is: 22.00 MB\n",
      "Decreased by 0.00%\n",
      "Epoch 1/2000\n",
      "1408/1408 [==============================] - 74s 50ms/step - loss: 2.7824 - r2: -1.0800 - val_loss: 3.1608 - val_r2: -1.1496\n",
      "Epoch 2/2000\n",
      "1408/1408 [==============================] - 40s 28ms/step - loss: 1.6393 - r2: -0.2255 - val_loss: 2.2021 - val_r2: -0.4976\n",
      "Epoch 3/2000\n",
      "1408/1408 [==============================] - 64s 45ms/step - loss: 1.4929 - r2: -0.1161 - val_loss: 1.8535 - val_r2: -0.2606\n",
      "Epoch 4/2000\n",
      "1408/1408 [==============================] - 64s 45ms/step - loss: 1.4288 - r2: -0.0681 - val_loss: 1.7607 - val_r2: -0.1974\n",
      "Epoch 5/2000\n",
      "1408/1408 [==============================] - 64s 45ms/step - loss: 1.3928 - r2: -0.0412 - val_loss: 1.6874 - val_r2: -0.1476\n",
      "Epoch 6/2000\n",
      "1408/1408 [==============================] - 64s 46ms/step - loss: 1.3698 - r2: -0.0240 - val_loss: 1.6602 - val_r2: -0.1291\n",
      "Epoch 7/2000\n",
      "1408/1408 [==============================] - 65s 46ms/step - loss: 1.3539 - r2: -0.0122 - val_loss: 1.6356 - val_r2: -0.1124\n",
      "Epoch 8/2000\n",
      "1408/1408 [==============================] - 57s 40ms/step - loss: 1.3423 - r2: -0.0035 - val_loss: 1.6267 - val_r2: -0.1063\n",
      "Epoch 9/2000\n",
      "1408/1408 [==============================] - 34s 24ms/step - loss: 1.3361 - r2: 0.0012 - val_loss: 1.6408 - val_r2: -0.1159\n",
      "Epoch 10/2000\n",
      "1408/1408 [==============================] - 35s 25ms/step - loss: 1.3316 - r2: 0.0046 - val_loss: 1.6329 - val_r2: -0.1105\n",
      "Epoch 11/2000\n",
      "1408/1408 [==============================] - 35s 25ms/step - loss: 1.3277 - r2: 0.0074 - val_loss: 1.6504 - val_r2: -0.1224\n",
      "Epoch 12/2000\n",
      "1408/1408 [==============================] - 39s 28ms/step - loss: 1.3254 - r2: 0.0092 - val_loss: 1.6561 - val_r2: -0.1263\n",
      "Epoch 13/2000\n",
      "1408/1408 [==============================] - 42s 30ms/step - loss: 1.3232 - r2: 0.0108 - val_loss: 1.6498 - val_r2: -0.1220\n",
      "Epoch 14/2000\n",
      "1408/1408 [==============================] - 42s 30ms/step - loss: 1.3210 - r2: 0.0125 - val_loss: 1.6571 - val_r2: -0.1269\n",
      "Epoch 15/2000\n",
      "1408/1408 [==============================] - 42s 30ms/step - loss: 1.3197 - r2: 0.0134 - val_loss: 1.6589 - val_r2: -0.1282\n",
      "Epoch 16/2000\n",
      "1408/1408 [==============================] - 43s 30ms/step - loss: 1.3185 - r2: 0.0143 - val_loss: 1.6654 - val_r2: -0.1326\n",
      "Epoch 17/2000\n",
      "1408/1408 [==============================] - 42s 30ms/step - loss: 1.3174 - r2: 0.0152 - val_loss: 1.6748 - val_r2: -0.1390\n",
      "Epoch 18/2000\n",
      "1408/1408 [==============================] - 42s 30ms/step - loss: 1.3162 - r2: 0.0160 - val_loss: 1.6783 - val_r2: -0.1414\n",
      "Epoch 19/2000\n",
      "1408/1408 [==============================] - 43s 30ms/step - loss: 1.3152 - r2: 0.0168 - val_loss: 1.6807 - val_r2: -0.1431\n",
      "Epoch 20/2000\n",
      "1408/1408 [==============================] - 41s 29ms/step - loss: 1.3144 - r2: 0.0174 - val_loss: 1.6873 - val_r2: -0.1475\n",
      "Epoch 21/2000\n",
      "1408/1408 [==============================] - 64s 45ms/step - loss: 1.3138 - r2: 0.0178 - val_loss: 1.6888 - val_r2: -0.1485\n",
      "Epoch 22/2000\n",
      "1408/1408 [==============================] - 63s 45ms/step - loss: 1.3131 - r2: 0.0183 - val_loss: 1.6888 - val_r2: -0.1485\n",
      "Epoch 23/2000\n",
      "1408/1408 [==============================] - 61s 43ms/step - loss: 1.3125 - r2: 0.0188 - val_loss: 1.6952 - val_r2: -0.1529\n",
      "Epoch 24/2000\n",
      "1408/1408 [==============================] - 67s 47ms/step - loss: 1.3117 - r2: 0.0194 - val_loss: 1.6989 - val_r2: -0.1554\n",
      "Epoch 25/2000\n",
      "1408/1408 [==============================] - 67s 48ms/step - loss: 1.3110 - r2: 0.0200 - val_loss: 1.7027 - val_r2: -0.1580\n",
      "Epoch 26/2000\n",
      "1408/1408 [==============================] - 64s 46ms/step - loss: 1.3104 - r2: 0.0204 - val_loss: 1.7118 - val_r2: -0.1642\n",
      "Epoch 27/2000\n",
      "1408/1408 [==============================] - 64s 45ms/step - loss: 1.3100 - r2: 0.0207 - val_loss: 1.7058 - val_r2: -0.1601\n",
      "Epoch 28/2000\n",
      "1408/1408 [==============================] - 65s 46ms/step - loss: 1.3095 - r2: 0.0210 - val_loss: 1.7140 - val_r2: -0.1657\n",
      "Epoch 29/2000\n",
      "1408/1408 [==============================] - 65s 46ms/step - loss: 1.3090 - r2: 0.0214 - val_loss: 1.7223 - val_r2: -0.1713\n",
      "Epoch 30/2000\n",
      "1408/1408 [==============================] - 63s 45ms/step - loss: 1.3087 - r2: 0.0217 - val_loss: 1.7120 - val_r2: -0.1643\n",
      "Epoch 31/2000\n",
      "1408/1408 [==============================] - 39s 27ms/step - loss: 1.3081 - r2: 0.0221 - val_loss: 1.7213 - val_r2: -0.1706\n",
      "Epoch 32/2000\n",
      "1408/1408 [==============================] - 43s 31ms/step - loss: 1.3075 - r2: 0.0225 - val_loss: 1.7303 - val_r2: -0.1767\n",
      "Epoch 33/2000\n",
      "1408/1408 [==============================] - 45s 32ms/step - loss: 1.3075 - r2: 0.0225 - val_loss: 1.7154 - val_r2: -0.1666\n",
      "11\n",
      "Memory usage of dataframe is 3960.00 MB\n",
      "Memory usage after optimization is: 3696.00 MB\n",
      "Decreased by 6.67%\n",
      "Memory usage of dataframe is 22.00 MB\n",
      "Memory usage after optimization is: 22.00 MB\n",
      "Decreased by 0.00%\n",
      "Memory usage of dataframe is 22.00 MB\n",
      "Memory usage after optimization is: 22.00 MB\n",
      "Decreased by 0.00%\n",
      "Epoch 1/2000\n",
      "1408/1408 [==============================] - 48s 34ms/step - loss: 1.4761 - r2: 0.0033 - val_loss: 1.6073 - val_r2: -0.0931\n",
      "Epoch 2/2000\n",
      "1408/1408 [==============================] - 43s 31ms/step - loss: 1.4711 - r2: 0.0066 - val_loss: 1.6225 - val_r2: -0.1034\n",
      "Epoch 3/2000\n",
      "1408/1408 [==============================] - 62s 44ms/step - loss: 1.4682 - r2: 0.0086 - val_loss: 1.6276 - val_r2: -0.1069\n",
      "Epoch 4/2000\n",
      "1408/1408 [==============================] - 65s 46ms/step - loss: 1.4663 - r2: 0.0099 - val_loss: 1.6281 - val_r2: -0.1073\n",
      "Epoch 5/2000\n",
      "1408/1408 [==============================] - 66s 47ms/step - loss: 1.4646 - r2: 0.0111 - val_loss: 1.6384 - val_r2: -0.1143\n",
      "Epoch 6/2000\n",
      "1408/1408 [==============================] - 65s 46ms/step - loss: 1.4635 - r2: 0.0118 - val_loss: 1.6524 - val_r2: -0.1238\n",
      "Epoch 7/2000\n",
      "1408/1408 [==============================] - 66s 47ms/step - loss: 1.4625 - r2: 0.0124 - val_loss: 1.6479 - val_r2: -0.1207\n",
      "Epoch 8/2000\n",
      "1408/1408 [==============================] - 66s 47ms/step - loss: 1.4616 - r2: 0.0131 - val_loss: 1.6536 - val_r2: -0.1246\n",
      "Epoch 9/2000\n",
      "1408/1408 [==============================] - 65s 46ms/step - loss: 1.4606 - r2: 0.0137 - val_loss: 1.6585 - val_r2: -0.1279\n",
      "Epoch 10/2000\n",
      "1408/1408 [==============================] - 65s 46ms/step - loss: 1.4597 - r2: 0.0144 - val_loss: 1.6645 - val_r2: -0.1320\n",
      "Epoch 11/2000\n",
      "1408/1408 [==============================] - 65s 46ms/step - loss: 1.4592 - r2: 0.0147 - val_loss: 1.6712 - val_r2: -0.1365\n",
      "Epoch 12/2000\n",
      "1408/1408 [==============================] - 69s 49ms/step - loss: 1.4583 - r2: 0.0153 - val_loss: 1.6641 - val_r2: -0.1317\n",
      "Epoch 13/2000\n",
      "1408/1408 [==============================] - 72s 51ms/step - loss: 1.4579 - r2: 0.0156 - val_loss: 1.6651 - val_r2: -0.1324\n",
      "Epoch 14/2000\n",
      "1408/1408 [==============================] - 79s 56ms/step - loss: 1.4575 - r2: 0.0159 - val_loss: 1.6638 - val_r2: -0.1315\n",
      "Epoch 15/2000\n",
      "1408/1408 [==============================] - 82s 58ms/step - loss: 1.4566 - r2: 0.0164 - val_loss: 1.6751 - val_r2: -0.1392\n",
      "Epoch 16/2000\n",
      "1408/1408 [==============================] - 85s 60ms/step - loss: 1.4563 - r2: 0.0167 - val_loss: 1.6768 - val_r2: -0.1403\n",
      "Epoch 17/2000\n",
      "1408/1408 [==============================] - 53s 38ms/step - loss: 1.4556 - r2: 0.0172 - val_loss: 1.6804 - val_r2: -0.1428\n",
      "Epoch 18/2000\n",
      "1408/1408 [==============================] - 45s 32ms/step - loss: 1.4551 - r2: 0.0175 - val_loss: 1.6850 - val_r2: -0.1460\n",
      "Epoch 19/2000\n",
      "1408/1408 [==============================] - 41s 29ms/step - loss: 1.4548 - r2: 0.0176 - val_loss: 1.6803 - val_r2: -0.1427\n",
      "Epoch 20/2000\n",
      "1408/1408 [==============================] - 45s 32ms/step - loss: 1.4545 - r2: 0.0179 - val_loss: 1.6827 - val_r2: -0.1444\n",
      "Epoch 21/2000\n",
      "1408/1408 [==============================] - 53s 38ms/step - loss: 1.4541 - r2: 0.0182 - val_loss: 1.6824 - val_r2: -0.1442\n",
      "Epoch 22/2000\n",
      "1408/1408 [==============================] - 57s 41ms/step - loss: 1.4536 - r2: 0.0185 - val_loss: 1.6784 - val_r2: -0.1414\n",
      "Epoch 23/2000\n",
      "1408/1408 [==============================] - 35s 25ms/step - loss: 1.4529 - r2: 0.0190 - val_loss: 1.6846 - val_r2: -0.1457\n",
      "Epoch 24/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4529 - r2: 0.0189 - val_loss: 1.6799 - val_r2: -0.1425\n",
      "Epoch 25/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4527 - r2: 0.0191 - val_loss: 1.6772 - val_r2: -0.1406\n",
      "Epoch 26/2000\n",
      "1408/1408 [==============================] - 32s 22ms/step - loss: 1.4523 - r2: 0.0193 - val_loss: 1.6794 - val_r2: -0.1421\n",
      "22\n",
      "Memory usage of dataframe is 3960.00 MB\n",
      "Memory usage after optimization is: 3696.00 MB\n",
      "Decreased by 6.67%\n",
      "Memory usage of dataframe is 22.00 MB\n",
      "Memory usage after optimization is: 22.00 MB\n",
      "Decreased by 0.00%\n",
      "Memory usage of dataframe is 22.00 MB\n",
      "Memory usage after optimization is: 22.00 MB\n",
      "Decreased by 0.00%\n",
      "Epoch 1/2000\n",
      "1408/1408 [==============================] - 33s 23ms/step - loss: 1.5045 - r2: 0.0044 - val_loss: 1.4944 - val_r2: -0.0163\n",
      "Epoch 2/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.5010 - r2: 0.0067 - val_loss: 1.4905 - val_r2: -0.0136\n",
      "Epoch 3/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4995 - r2: 0.0077 - val_loss: 1.4876 - val_r2: -0.0117\n",
      "Epoch 4/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4985 - r2: 0.0084 - val_loss: 1.4879 - val_r2: -0.0119\n",
      "Epoch 5/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4971 - r2: 0.0093 - val_loss: 1.4846 - val_r2: -0.0097\n",
      "Epoch 6/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4962 - r2: 0.0098 - val_loss: 1.4870 - val_r2: -0.0113\n",
      "Epoch 7/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4954 - r2: 0.0104 - val_loss: 1.4839 - val_r2: -0.0092\n",
      "Epoch 8/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4949 - r2: 0.0107 - val_loss: 1.4842 - val_r2: -0.0094\n",
      "Epoch 9/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4941 - r2: 0.0112 - val_loss: 1.4823 - val_r2: -0.0081\n",
      "Epoch 10/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4936 - r2: 0.0116 - val_loss: 1.4845 - val_r2: -0.0096\n",
      "Epoch 11/2000\n",
      "1408/1408 [==============================] - 33s 23ms/step - loss: 1.4928 - r2: 0.0121 - val_loss: 1.4816 - val_r2: -0.0077\n",
      "Epoch 12/2000\n",
      "1408/1408 [==============================] - 33s 23ms/step - loss: 1.4927 - r2: 0.0122 - val_loss: 1.4831 - val_r2: -0.0086\n",
      "Epoch 13/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4922 - r2: 0.0125 - val_loss: 1.4820 - val_r2: -0.0079\n",
      "Epoch 14/2000\n",
      "1408/1408 [==============================] - 33s 23ms/step - loss: 1.4916 - r2: 0.0129 - val_loss: 1.4819 - val_r2: -0.0078\n",
      "Epoch 15/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4911 - r2: 0.0132 - val_loss: 1.4826 - val_r2: -0.0083\n",
      "Epoch 16/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4907 - r2: 0.0135 - val_loss: 1.4824 - val_r2: -0.0081\n",
      "Epoch 17/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4903 - r2: 0.0138 - val_loss: 1.4821 - val_r2: -0.0080\n",
      "Epoch 18/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4900 - r2: 0.0140 - val_loss: 1.4822 - val_r2: -0.0080\n",
      "Epoch 19/2000\n",
      "1408/1408 [==============================] - 33s 23ms/step - loss: 1.4896 - r2: 0.0142 - val_loss: 1.4809 - val_r2: -0.0071\n",
      "Epoch 20/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4892 - r2: 0.0145 - val_loss: 1.4816 - val_r2: -0.0076\n",
      "Epoch 21/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4890 - r2: 0.0147 - val_loss: 1.4806 - val_r2: -0.0069\n",
      "Epoch 22/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4884 - r2: 0.0150 - val_loss: 1.4818 - val_r2: -0.0077\n",
      "Epoch 23/2000\n",
      "1408/1408 [==============================] - 33s 23ms/step - loss: 1.4884 - r2: 0.0150 - val_loss: 1.4812 - val_r2: -0.0073\n",
      "Epoch 24/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4879 - r2: 0.0154 - val_loss: 1.4801 - val_r2: -0.0066\n",
      "Epoch 25/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4878 - r2: 0.0154 - val_loss: 1.4808 - val_r2: -0.0070\n",
      "Epoch 26/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4874 - r2: 0.0157 - val_loss: 1.4808 - val_r2: -0.0071\n",
      "Epoch 27/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4873 - r2: 0.0157 - val_loss: 1.4799 - val_r2: -0.0065\n",
      "Epoch 28/2000\n",
      "1408/1408 [==============================] - 33s 23ms/step - loss: 1.4868 - r2: 0.0161 - val_loss: 1.4806 - val_r2: -0.0070\n",
      "Epoch 29/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4866 - r2: 0.0162 - val_loss: 1.4802 - val_r2: -0.0066\n",
      "Epoch 30/2000\n",
      "1408/1408 [==============================] - 33s 23ms/step - loss: 1.4864 - r2: 0.0164 - val_loss: 1.4799 - val_r2: -0.0065\n",
      "Epoch 31/2000\n",
      "1408/1408 [==============================] - 35s 25ms/step - loss: 1.4860 - r2: 0.0166 - val_loss: 1.4802 - val_r2: -0.0067\n",
      "Epoch 32/2000\n",
      "1408/1408 [==============================] - 33s 23ms/step - loss: 1.4858 - r2: 0.0168 - val_loss: 1.4796 - val_r2: -0.0063\n",
      "Epoch 33/2000\n",
      "1408/1408 [==============================] - 33s 24ms/step - loss: 1.4854 - r2: 0.0170 - val_loss: 1.4801 - val_r2: -0.0066\n",
      "Epoch 34/2000\n",
      "1408/1408 [==============================] - 33s 24ms/step - loss: 1.4854 - r2: 0.0170 - val_loss: 1.4806 - val_r2: -0.0069\n",
      "Epoch 35/2000\n",
      "1408/1408 [==============================] - 34s 24ms/step - loss: 1.4850 - r2: 0.0172 - val_loss: 1.4799 - val_r2: -0.0065\n",
      "Epoch 36/2000\n",
      "1408/1408 [==============================] - 33s 23ms/step - loss: 1.4849 - r2: 0.0174 - val_loss: 1.4809 - val_r2: -0.0072\n",
      "Epoch 37/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4847 - r2: 0.0175 - val_loss: 1.4796 - val_r2: -0.0063\n",
      "Epoch 38/2000\n",
      "1408/1408 [==============================] - 33s 23ms/step - loss: 1.4846 - r2: 0.0175 - val_loss: 1.4802 - val_r2: -0.0067\n",
      "Epoch 39/2000\n",
      "1408/1408 [==============================] - 33s 23ms/step - loss: 1.4844 - r2: 0.0176 - val_loss: 1.4798 - val_r2: -0.0064\n",
      "Epoch 40/2000\n",
      "1408/1408 [==============================] - 33s 23ms/step - loss: 1.4840 - r2: 0.0179 - val_loss: 1.4803 - val_r2: -0.0067\n",
      "Epoch 41/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4837 - r2: 0.0181 - val_loss: 1.4802 - val_r2: -0.0066\n",
      "Epoch 42/2000\n",
      "1408/1408 [==============================] - 33s 24ms/step - loss: 1.4835 - r2: 0.0182 - val_loss: 1.4804 - val_r2: -0.0068\n",
      "Epoch 43/2000\n",
      "1408/1408 [==============================] - 33s 23ms/step - loss: 1.4834 - r2: 0.0183 - val_loss: 1.4798 - val_r2: -0.0064\n",
      "Epoch 44/2000\n",
      "1408/1408 [==============================] - 33s 23ms/step - loss: 1.4833 - r2: 0.0184 - val_loss: 1.4801 - val_r2: -0.0066\n",
      "Epoch 45/2000\n",
      "1408/1408 [==============================] - 33s 23ms/step - loss: 1.4831 - r2: 0.0185 - val_loss: 1.4805 - val_r2: -0.0069\n",
      "Epoch 46/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4829 - r2: 0.0187 - val_loss: 1.4805 - val_r2: -0.0069\n",
      "Epoch 47/2000\n",
      "1408/1408 [==============================] - 33s 23ms/step - loss: 1.4826 - r2: 0.0189 - val_loss: 1.4799 - val_r2: -0.0065\n",
      "Epoch 48/2000\n",
      "1408/1408 [==============================] - 33s 23ms/step - loss: 1.4824 - r2: 0.0190 - val_loss: 1.4807 - val_r2: -0.0070\n",
      "Epoch 49/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4823 - r2: 0.0191 - val_loss: 1.4805 - val_r2: -0.0069\n",
      "Epoch 50/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4821 - r2: 0.0192 - val_loss: 1.4809 - val_r2: -0.0071\n",
      "Epoch 51/2000\n",
      "1408/1408 [==============================] - 33s 23ms/step - loss: 1.4817 - r2: 0.0194 - val_loss: 1.4804 - val_r2: -0.0068\n",
      "Epoch 52/2000\n",
      "1408/1408 [==============================] - 33s 23ms/step - loss: 1.4817 - r2: 0.0195 - val_loss: 1.4795 - val_r2: -0.0062\n",
      "Epoch 53/2000\n",
      "1408/1408 [==============================] - 33s 23ms/step - loss: 1.4816 - r2: 0.0195 - val_loss: 1.4800 - val_r2: -0.0065\n",
      "Epoch 54/2000\n",
      "1408/1408 [==============================] - 33s 23ms/step - loss: 1.4813 - r2: 0.0197 - val_loss: 1.4810 - val_r2: -0.0072\n",
      "Epoch 55/2000\n",
      "1408/1408 [==============================] - 33s 23ms/step - loss: 1.4811 - r2: 0.0198 - val_loss: 1.4802 - val_r2: -0.0066\n",
      "Epoch 56/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4812 - r2: 0.0198 - val_loss: 1.4807 - val_r2: -0.0070\n",
      "Epoch 57/2000\n",
      "1408/1408 [==============================] - 34s 24ms/step - loss: 1.4810 - r2: 0.0199 - val_loss: 1.4802 - val_r2: -0.0067\n",
      "Epoch 58/2000\n",
      "1408/1408 [==============================] - 33s 23ms/step - loss: 1.4806 - r2: 0.0202 - val_loss: 1.4806 - val_r2: -0.0070\n",
      "Epoch 59/2000\n",
      "1408/1408 [==============================] - 33s 23ms/step - loss: 1.4802 - r2: 0.0204 - val_loss: 1.4808 - val_r2: -0.0071\n",
      "Epoch 60/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4803 - r2: 0.0204 - val_loss: 1.4801 - val_r2: -0.0066\n",
      "Epoch 61/2000\n",
      "1408/1408 [==============================] - 33s 23ms/step - loss: 1.4802 - r2: 0.0204 - val_loss: 1.4809 - val_r2: -0.0071\n",
      "Epoch 62/2000\n",
      "1408/1408 [==============================] - 33s 23ms/step - loss: 1.4799 - r2: 0.0206 - val_loss: 1.4816 - val_r2: -0.0076\n",
      "Epoch 63/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4798 - r2: 0.0207 - val_loss: 1.4803 - val_r2: -0.0067\n",
      "Epoch 64/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4797 - r2: 0.0208 - val_loss: 1.4804 - val_r2: -0.0068\n",
      "Epoch 65/2000\n",
      "1408/1408 [==============================] - 33s 23ms/step - loss: 1.4797 - r2: 0.0208 - val_loss: 1.4807 - val_r2: -0.0070\n",
      "Epoch 66/2000\n",
      "1408/1408 [==============================] - 33s 23ms/step - loss: 1.4797 - r2: 0.0208 - val_loss: 1.4805 - val_r2: -0.0068\n",
      "Epoch 67/2000\n",
      "1408/1408 [==============================] - 32s 23ms/step - loss: 1.4795 - r2: 0.0209 - val_loss: 1.4815 - val_r2: -0.0076\n",
      "Epoch 68/2000\n",
      "1408/1408 [==============================] - 45s 32ms/step - loss: 1.4792 - r2: 0.0211 - val_loss: 1.4806 - val_r2: -0.0070\n",
      "Epoch 69/2000\n",
      "1408/1408 [==============================] - 45s 32ms/step - loss: 1.4789 - r2: 0.0213 - val_loss: 1.4810 - val_r2: -0.0072\n",
      "Epoch 70/2000\n",
      "1408/1408 [==============================] - 44s 31ms/step - loss: 1.4787 - r2: 0.0214 - val_loss: 1.4808 - val_r2: -0.0071\n",
      "Epoch 71/2000\n",
      "1408/1408 [==============================] - 40s 28ms/step - loss: 1.4788 - r2: 0.0214 - val_loss: 1.4814 - val_r2: -0.0075\n",
      "Epoch 72/2000\n",
      "1408/1408 [==============================] - 36s 25ms/step - loss: 1.4787 - r2: 0.0214 - val_loss: 1.4812 - val_r2: -0.0074\n",
      "Epoch 73/2000\n",
      "1408/1408 [==============================] - 36s 26ms/step - loss: 1.4783 - r2: 0.0217 - val_loss: 1.4817 - val_r2: -0.0077\n",
      "Epoch 74/2000\n",
      "1408/1408 [==============================] - 36s 25ms/step - loss: 1.4783 - r2: 0.0217 - val_loss: 1.4810 - val_r2: -0.0072\n",
      "Epoch 75/2000\n",
      "1408/1408 [==============================] - 39s 28ms/step - loss: 1.4781 - r2: 0.0218 - val_loss: 1.4809 - val_r2: -0.0072\n",
      "Epoch 76/2000\n",
      "1408/1408 [==============================] - 37s 26ms/step - loss: 1.4781 - r2: 0.0218 - val_loss: 1.4811 - val_r2: -0.0073\n",
      "Epoch 77/2000\n",
      "1408/1408 [==============================] - 33s 24ms/step - loss: 1.4779 - r2: 0.0220 - val_loss: 1.4808 - val_r2: -0.0071\n",
      "33\n",
      "Memory usage of dataframe is 39.79 MB\n",
      "Memory usage after optimization is: 36.69 MB\n",
      "Decreased by 7.78%\n",
      "Memory usage of dataframe is 0.22 MB\n",
      "Memory usage after optimization is: 0.22 MB\n",
      "Decreased by 0.00%\n",
      "Memory usage of dataframe is 0.22 MB\n",
      "Memory usage after optimization is: 0.22 MB\n",
      "Decreased by 0.00%\n",
      "Epoch 1/2000\n",
      "15/15 [==============================] - 7s 515ms/step - loss: 2.4076 - r2: 0.0207 - val_loss: 1.4806 - val_r2: -0.0069\n",
      "Epoch 2/2000\n",
      "15/15 [==============================] - 7s 506ms/step - loss: 2.4062 - r2: 0.0213 - val_loss: 1.4828 - val_r2: -0.0085\n",
      "Epoch 3/2000\n",
      "15/15 [==============================] - 7s 509ms/step - loss: 2.4062 - r2: 0.0212 - val_loss: 1.4857 - val_r2: -0.0104\n",
      "Epoch 4/2000\n",
      "15/15 [==============================] - 7s 506ms/step - loss: 2.4042 - r2: 0.0221 - val_loss: 1.4887 - val_r2: -0.0124\n",
      "Epoch 5/2000\n",
      "15/15 [==============================] - 7s 513ms/step - loss: 2.4009 - r2: 0.0234 - val_loss: 1.4914 - val_r2: -0.0143\n",
      "Epoch 6/2000\n",
      "15/15 [==============================] - 7s 515ms/step - loss: 2.3992 - r2: 0.0241 - val_loss: 1.4942 - val_r2: -0.0162\n",
      "Epoch 7/2000\n",
      "15/15 [==============================] - 7s 508ms/step - loss: 2.4000 - r2: 0.0238 - val_loss: 1.4970 - val_r2: -0.0181\n",
      "Epoch 8/2000\n",
      "15/15 [==============================] - 7s 510ms/step - loss: 2.3953 - r2: 0.0257 - val_loss: 1.4994 - val_r2: -0.0197\n",
      "Epoch 9/2000\n",
      "15/15 [==============================] - 8s 541ms/step - loss: 2.3954 - r2: 0.0257 - val_loss: 1.5016 - val_r2: -0.0212\n",
      "Epoch 10/2000\n",
      "15/15 [==============================] - 10s 736ms/step - loss: 2.3944 - r2: 0.0261 - val_loss: 1.5034 - val_r2: -0.0224\n",
      "Epoch 11/2000\n",
      "15/15 [==============================] - 10s 700ms/step - loss: 2.3953 - r2: 0.0257 - val_loss: 1.5052 - val_r2: -0.0236\n",
      "Epoch 12/2000\n",
      "15/15 [==============================] - 10s 677ms/step - loss: 2.3956 - r2: 0.0256 - val_loss: 1.5063 - val_r2: -0.0244\n",
      "Epoch 13/2000\n",
      "15/15 [==============================] - 9s 619ms/step - loss: 2.3934 - r2: 0.0264 - val_loss: 1.5076 - val_r2: -0.0253\n",
      "Epoch 14/2000\n",
      "15/15 [==============================] - 7s 504ms/step - loss: 2.3889 - r2: 0.0283 - val_loss: 1.5088 - val_r2: -0.0261\n",
      "Epoch 15/2000\n",
      "15/15 [==============================] - 7s 505ms/step - loss: 2.3903 - r2: 0.0277 - val_loss: 1.5095 - val_r2: -0.0266\n",
      "Epoch 16/2000\n",
      "15/15 [==============================] - 7s 495ms/step - loss: 2.3880 - r2: 0.0287 - val_loss: 1.5111 - val_r2: -0.0277\n",
      "Epoch 17/2000\n",
      "15/15 [==============================] - 7s 512ms/step - loss: 2.3898 - r2: 0.0279 - val_loss: 1.5114 - val_r2: -0.0279\n",
      "Epoch 18/2000\n",
      "15/15 [==============================] - 7s 500ms/step - loss: 2.3864 - r2: 0.0293 - val_loss: 1.5123 - val_r2: -0.0285\n",
      "Epoch 19/2000\n",
      "15/15 [==============================] - 7s 508ms/step - loss: 2.3857 - r2: 0.0296 - val_loss: 1.5130 - val_r2: -0.0290\n",
      "Epoch 20/2000\n",
      "15/15 [==============================] - 7s 512ms/step - loss: 2.3874 - r2: 0.0289 - val_loss: 1.5137 - val_r2: -0.0295\n",
      "Epoch 21/2000\n",
      "15/15 [==============================] - 7s 501ms/step - loss: 2.3876 - r2: 0.0288 - val_loss: 1.5144 - val_r2: -0.0299\n",
      "Epoch 22/2000\n",
      "15/15 [==============================] - 7s 502ms/step - loss: 2.3846 - r2: 0.0300 - val_loss: 1.5151 - val_r2: -0.0304\n",
      "Epoch 23/2000\n",
      "15/15 [==============================] - 10s 693ms/step - loss: 2.3824 - r2: 0.0309 - val_loss: 1.5154 - val_r2: -0.0306\n",
      "Epoch 24/2000\n",
      "15/15 [==============================] - 10s 687ms/step - loss: 2.3819 - r2: 0.0311 - val_loss: 1.5164 - val_r2: -0.0313\n",
      "Epoch 25/2000\n",
      "15/15 [==============================] - 8s 598ms/step - loss: 2.3825 - r2: 0.0309 - val_loss: 1.5170 - val_r2: -0.0317\n",
      "Epoch 26/2000\n",
      "15/15 [==============================] - 7s 512ms/step - loss: 2.3803 - r2: 0.0318 - val_loss: 1.5172 - val_r2: -0.0319\n"
     ]
    }
   ],
   "source": [
    "for fold in range(4, CONFIG.N_fold):\n",
    "    model = ANN(features_shape)\n",
    "\n",
    "    train_features_file_path = (\n",
    "        f\"{CONFIG.main}/data/training_data_impt/X_train_{fold}.parquet\"\n",
    "    )\n",
    "    train_labels_file_path = (\n",
    "        f\"{CONFIG.main}/data/training_data_impt/y_train_{fold}.parquet\"\n",
    "    )\n",
    "    train_weights_file_path = (\n",
    "        f\"{CONFIG.main}/data/training_data_impt/w_train_{fold}.parquet\"\n",
    "    )\n",
    "\n",
    "    # Create the TensorFlow Dataset\n",
    "    train_features_file = pq.ParquetFile(train_features_file_path)\n",
    "    train_labels_file = pq.ParquetFile(train_labels_file_path)\n",
    "    train_weights_file = pq.ParquetFile(train_weights_file_path)\n",
    "\n",
    "    train_row_group = train_features_file.num_row_groups\n",
    "\n",
    "    step = int(train_row_group / 3)\n",
    "\n",
    "    for i in range(0, train_row_group, step):\n",
    "        print(i)\n",
    "        batch_end = min(i + step, train_row_group)\n",
    "        features_batch = reduce_memory.reduce_mem_usage(\n",
    "            train_features_file.read_row_groups([i for i in range(i, batch_end)])\n",
    "            .to_pandas()\n",
    "            .fillna(0)\n",
    "        ).values\n",
    "        labels_batch = reduce_memory.reduce_mem_usage(\n",
    "            train_labels_file.read_row_groups([i for i in range(i, batch_end)])\n",
    "            .to_pandas()\n",
    "            .fillna(0)\n",
    "        ).values.squeeze()\n",
    "        weights_batch = reduce_memory.reduce_mem_usage(\n",
    "            train_weights_file.read_row_groups([i for i in range(i, batch_end)])\n",
    "            .to_pandas()\n",
    "            .fillna(0)\n",
    "        ).values.squeeze()\n",
    "\n",
    "        with tf.device(\"/CPU:0\"):\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "                (features_batch, labels_batch, weights_batch)\n",
    "            )\n",
    "            train_dataset = (\n",
    "                train_dataset.shuffle(buffer_size=batch_size)\n",
    "                .batch(batch_size)\n",
    "                .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "            )\n",
    "\n",
    "        with tf.device(\"/GPU:0\"):\n",
    "            model.fit(\n",
    "                train_dataset,\n",
    "                epochs=epochs,\n",
    "                validation_data=valid_dataset,\n",
    "                callbacks=[callback],\n",
    "            )\n",
    "\n",
    "        del train_dataset\n",
    "\n",
    "    tf.keras.models.save_model(model, f\"{CONFIG.main}/Models_impt/NN/NN_{fold+1}.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
