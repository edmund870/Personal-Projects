{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9abf5928",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-12-06T00:24:53.855475Z",
     "iopub.status.busy": "2023-12-06T00:24:53.854866Z",
     "iopub.status.idle": "2023-12-06T00:24:57.068238Z",
     "shell.execute_reply": "2023-12-06T00:24:57.066653Z"
    },
    "papermill": {
     "duration": 3.231171,
     "end_time": "2023-12-06T00:24:57.071645",
     "exception": false,
     "start_time": "2023-12-06T00:24:53.840474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "import time\n",
    "import gc\n",
    "import polars as pl\n",
    "import joblib\n",
    "from numba import njit, prange\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c107aaee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T00:24:57.096347Z",
     "iopub.status.busy": "2023-12-06T00:24:57.095625Z",
     "iopub.status.idle": "2023-12-06T00:25:23.008694Z",
     "shell.execute_reply": "2023-12-06T00:25:23.007285Z"
    },
    "papermill": {
     "duration": 25.929106,
     "end_time": "2023-12-06T00:25:23.012077",
     "exception": false,
     "start_time": "2023-12-06T00:24:57.082971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/optiver-trading-at-the-close/train.csv')\n",
    "flag = 'submission' #submission None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8c51f5",
   "metadata": {
    "papermill": {
     "duration": 0.010754,
     "end_time": "2023-12-06T00:25:23.068741",
     "exception": false,
     "start_time": "2023-12-06T00:25:23.057987",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Memory Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "644fce98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T00:25:23.093878Z",
     "iopub.status.busy": "2023-12-06T00:25:23.093403Z",
     "iopub.status.idle": "2023-12-06T00:25:23.111538Z",
     "shell.execute_reply": "2023-12-06T00:25:23.110082Z"
    },
    "papermill": {
     "duration": 0.03474,
     "end_time": "2023-12-06T00:25:23.114832",
     "exception": false,
     "start_time": "2023-12-06T00:25:23.080092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    if flag == 'test':\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        start_mem = df.memory_usage().sum() / 1024**2\n",
    "        print(f'Memory usage of dataframe is {start_mem:.2f} MB')\n",
    "\n",
    "        for col in df.columns:\n",
    "            col_type = df[col].dtype\n",
    "\n",
    "            if col_type != object:\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "                if str(col_type)[:3] == 'int':\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df[col] = df[col].astype(np.int8)\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df[col] = df[col].astype(np.int16)\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df[col] = df[col].astype(np.int32)\n",
    "                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                        df[col] = df[col].astype(np.int64)  \n",
    "                else:\n",
    "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                        df[col] = df[col].astype(np.float16)\n",
    "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "                    else:\n",
    "                        df[col] = df[col].astype(np.float32)\n",
    "\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        print(f'Memory usage after optimization is: {end_mem:.2f} MB')\n",
    "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "        print(f'Decreased by {decrease:.2f}%')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# memory management; remove useless columns after each iteration\n",
    "\n",
    "def memory_management(df, columns):\n",
    "    df = df[df.columns[~df.columns.isin(columns)]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16c8b5a",
   "metadata": {},
   "source": [
    "# List of Insignificant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70099c6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T00:25:23.139501Z",
     "iopub.status.busy": "2023-12-06T00:25:23.138983Z",
     "iopub.status.idle": "2023-12-06T00:25:23.160124Z",
     "shell.execute_reply": "2023-12-06T00:25:23.158227Z"
    },
    "papermill": {
     "duration": 0.03685,
     "end_time": "2023-12-06T00:25:23.162686",
     "exception": false,
     "start_time": "2023-12-06T00:25:23.125836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "useless = [\n",
    "'ask_price',\n",
    " 'ask_price__1_ewm',\n",
    " 'ask_price__2_ewm',\n",
    " 'ask_price__3_ewm',\n",
    " 'ask_price_bid_ask_spread_imb',\n",
    " 'ask_price_bid_ask_spread_imb3',\n",
    " 'ask_price_bid_price_bid_ask_spread_imb2',\n",
    " 'ask_price_bid_price_imb',\n",
    " 'ask_price_bid_price_imb3',\n",
    " 'ask_price_mid_price_bid_ask_spread_imb2',\n",
    " 'ask_price_mid_price_bid_price_imb2',\n",
    " 'ask_price_mid_price_imb',\n",
    " 'ask_price_mid_price_imb3',\n",
    " 'ask_price_mid_price_wap_imb2',\n",
    " 'ask_size__1_ewm',\n",
    " 'ask_size__2_ewm',\n",
    " 'ask_size__3_ewm',\n",
    " 'ask_size__4_ewm',\n",
    " 'bid_ask_ratio__1_ewm',\n",
    " 'bid_ask_ratio__1_rolling_mean',\n",
    " 'bid_ask_ratio__2_ewm',\n",
    " 'bid_ask_ratio__3_ewm',\n",
    " 'bid_ask_ratio__4_ewm',\n",
    " 'bid_ask_spread',\n",
    " 'bid_ask_spread_ratio',\n",
    " 'bid_ask_spread_vols_ratio',\n",
    " 'bid_plus_ask_sizes',\n",
    " 'bid_price',\n",
    " 'bid_price__1_ewm',\n",
    " 'bid_price__2_ewm',\n",
    " 'bid_price__3_ewm',\n",
    " 'bid_price_bid_ask_spread_imb',\n",
    " 'bid_price_bid_ask_spread_imb3',\n",
    " 'bid_size__1_ewm',\n",
    " 'bid_size__1_rolling_mean',\n",
    " 'bid_size__2_ewm',\n",
    " 'bid_size__3_ewm',\n",
    " 'bid_size__4_ewm',\n",
    " 'bid_size_ask_size_bid_ask_ratio_imb2',\n",
    " 'bid_size_ask_size_bid_plus_ask_sizes_imb2',\n",
    " 'bid_size_ask_size_imb',\n",
    " 'bid_size_bid_ask_ratio_imb3',\n",
    " 'bid_size_bid_plus_ask_sizes_imb',\n",
    " 'bid_size_bid_plus_ask_sizes_imb3',\n",
    " 'far_price',\n",
    " 'far_price_ask_price_bid_ask_spread_imb2',\n",
    " 'far_price_ask_price_imb',\n",
    " 'far_price_ask_price_imb3',\n",
    " 'far_price_ask_price_mid_price_imb2',\n",
    " 'far_price_ask_price_wap_imb2',\n",
    " 'far_price_bid_ask_spread_imb',\n",
    " 'far_price_bid_ask_spread_imb3',\n",
    " 'far_price_bid_price_bid_ask_spread_imb2',\n",
    " 'far_price_bid_price_imb',\n",
    " 'far_price_bid_price_imb3',\n",
    " 'far_price_bid_price_wap_imb2',\n",
    " 'far_price_mid_price_bid_ask_spread_imb2',\n",
    " 'far_price_mid_price_bid_price_imb2',\n",
    " 'far_price_mid_price_imb',\n",
    " 'far_price_mid_price_imb3',\n",
    " 'far_price_mid_price_wap_imb2',\n",
    " 'far_price_near_price_ask_price_imb2',\n",
    " 'far_price_near_price_bid_ask_spread_imb2',\n",
    " 'far_price_near_price_bid_price_imb2',\n",
    " 'far_price_near_price_imb',\n",
    " 'far_price_near_price_imb3',\n",
    " 'far_price_near_price_mid_price_imb2',\n",
    " 'far_price_near_price_wap_imb2',\n",
    " 'far_price_wap_bid_ask_spread_imb2',\n",
    " 'far_price_wap_imb',\n",
    " 'far_price_wap_imb3',\n",
    " 'imbalance_buy_sell_flag__1_ewm',\n",
    " 'imbalance_buy_sell_flag__1_rolling_mean',\n",
    " 'imbalance_buy_sell_flag__2_ewm',\n",
    " 'imbalance_buy_sell_flag__3_ewm',\n",
    " 'imbalance_buy_sell_flag_pct_change_2',\n",
    " 'imbalance_buy_sell_flag_pct_change_3',\n",
    " 'imbalance_size_ask_size_bid_ask_ratio_imb2',\n",
    " 'imbalance_size_ask_size_bid_plus_ask_sizes_imb2',\n",
    " 'imbalance_size_ask_size_imb',\n",
    " 'imbalance_size_bid_ask_ratio_imb',\n",
    " 'imbalance_size_bid_ask_ratio_imb3',\n",
    " 'imbalance_size_bid_plus_ask_sizes_bid_ask_ratio_imb2',\n",
    " 'imbalance_size_bid_plus_ask_sizes_imb',\n",
    " 'imbalance_size_bid_plus_ask_sizes_imb3',\n",
    " 'imbalance_size_bid_size_ask_size_imb2',\n",
    " 'imbalance_size_bid_size_bid_ask_ratio_imb2',\n",
    " 'imbalance_size_bid_size_bid_plus_ask_sizes_imb2',\n",
    " 'imbalance_size_bid_size_imb',\n",
    " 'imbalance_size_bid_size_imb3',\n",
    " 'imbalance_size_matched_size_ask_size_imb2',\n",
    " 'imbalance_size_matched_size_bid_ask_ratio_imb2',\n",
    " 'imbalance_size_matched_size_bid_plus_ask_sizes_imb2',\n",
    " 'imbalance_size_matched_size_bid_size_imb2',\n",
    " 'imbalance_size_matched_size_imb',\n",
    " 'imbalance_size_matched_size_imb3',\n",
    " 'liquidity_imbalance__1_ewm',\n",
    " 'liquidity_imbalance__1_rolling_mean',\n",
    " 'liquidity_imbalance__2_ewm',\n",
    " 'liquidity_imbalance__3_ewm',\n",
    " 'liquidity_imbalance__4_ewm',\n",
    " 'log_bid_vol__1_ewm',\n",
    " 'log_bid_vol__1_rolling_mean',\n",
    " 'log_bid_vol__2_ewm',\n",
    " 'log_bid_vol__3_ewm',\n",
    " 'log_bid_vol__4_ewm',\n",
    " 'log_far_price',\n",
    " 'market_urgency__1_ewm',\n",
    " 'market_urgency__1_rolling_mean',\n",
    " 'market_urgency__2_ewm',\n",
    " 'market_urgency__3_ewm',\n",
    " 'market_urgency__4_ewm',\n",
    " 'matched_imbalance',\n",
    " 'matched_size_ask_size_imb',\n",
    " 'matched_size_bid_ask_ratio_imb',\n",
    " 'matched_size_bid_plus_ask_sizes_bid_ask_ratio_imb2',\n",
    " 'matched_size_bid_plus_ask_sizes_imb',\n",
    " 'matched_size_bid_plus_ask_sizes_imb3',\n",
    " 'matched_size_bid_size_ask_size_imb2',\n",
    " 'mid_price',\n",
    " 'mid_price__1_ewm',\n",
    " 'mid_price__2_ewm',\n",
    " 'mid_price__3_ewm',\n",
    " 'mid_price__4_ewm',\n",
    " 'mid_price_bid_ask_spread_imb',\n",
    " 'mid_price_bid_ask_spread_imb3',\n",
    " 'mid_price_bid_price_bid_ask_spread_imb2',\n",
    " 'mid_price_bid_price_imb',\n",
    " 'mid_price_bid_price_imb3',\n",
    " 'mid_price_wap_bid_ask_spread_imb2',\n",
    " 'near_price',\n",
    " 'near_price_ask_price_bid_ask_spread_imb2',\n",
    " 'near_price_ask_price_bid_price_imb2',\n",
    " 'near_price_ask_price_imb',\n",
    " 'near_price_ask_price_imb3',\n",
    " 'near_price_ask_price_mid_price_imb2',\n",
    " 'near_price_ask_price_wap_imb2',\n",
    " 'near_price_bid_ask_spread_imb',\n",
    " 'near_price_bid_ask_spread_imb3',\n",
    " 'near_price_bid_price_bid_ask_spread_imb2',\n",
    " 'near_price_bid_price_imb',\n",
    " 'near_price_bid_price_imb3',\n",
    " 'near_price_bid_price_wap_imb2',\n",
    " 'near_price_mid_price_bid_ask_spread_imb2',\n",
    " 'near_price_mid_price_bid_price_imb2',\n",
    " 'near_price_mid_price_imb',\n",
    " 'near_price_mid_price_imb3',\n",
    " 'near_price_mid_price_wap_imb2',\n",
    " 'near_price_wap_bid_ask_spread_imb2',\n",
    " 'near_price_wap_imb',\n",
    " 'near_price_wap_imb3',\n",
    " 'price_diff_auction_vs_non_auction__1_ewm',\n",
    " 'price_diff_auction_vs_non_auction__1_rolling_mean',\n",
    " 'price_diff_auction_vs_non_auction__2_ewm',\n",
    " 'price_diff_auction_vs_non_auction__3_ewm',\n",
    " 'price_diff_auction_vs_non_auction__4_ewm',\n",
    " 'price_spread',\n",
    " 'ref_mid_spread_ratio__1_ewm',\n",
    " 'ref_mid_spread_ratio__2_ewm',\n",
    " 'ref_mid_spread_ratio__3_ewm',\n",
    " 'ref_mid_spread_ratio__4_ewm',\n",
    " 'reference_price_ask_price_bid_ask_spread_imb2',\n",
    " 'reference_price_ask_price_bid_price_imb2',\n",
    " 'reference_price_ask_price_imb3',\n",
    " 'reference_price_ask_price_mid_price_imb2',\n",
    " 'reference_price_ask_price_wap_imb2',\n",
    " 'reference_price_bid_ask_spread_imb',\n",
    " 'reference_price_bid_ask_spread_imb3',\n",
    " 'reference_price_bid_price_bid_ask_spread_imb2',\n",
    " 'reference_price_bid_price_imb3',\n",
    " 'reference_price_bid_price_wap_imb2',\n",
    " 'reference_price_far_price_ask_price_imb2',\n",
    " 'reference_price_far_price_bid_ask_spread_imb2',\n",
    " 'reference_price_far_price_bid_price_imb2',\n",
    " 'reference_price_far_price_imb',\n",
    " 'reference_price_far_price_imb3',\n",
    " 'reference_price_far_price_mid_price_imb2',\n",
    " 'reference_price_far_price_wap_imb2',\n",
    " 'reference_price_mid_price_bid_ask_spread_imb2',\n",
    " 'reference_price_mid_price_bid_price_imb2',\n",
    " 'reference_price_mid_price_imb',\n",
    " 'reference_price_mid_price_imb3',\n",
    " 'reference_price_mid_price_wap_imb2',\n",
    " 'reference_price_near_price_ask_price_imb2',\n",
    " 'reference_price_near_price_bid_ask_spread_imb2',\n",
    " 'reference_price_near_price_bid_price_imb2',\n",
    " 'reference_price_near_price_imb',\n",
    " 'reference_price_near_price_imb3',\n",
    " 'reference_price_near_price_mid_price_imb2',\n",
    " 'reference_price_near_price_wap_imb2',\n",
    " 'reference_price_wap_bid_ask_spread_imb2',\n",
    " 'wap',\n",
    " 'wap__2_ewm',\n",
    " 'wap__3_ewm',\n",
    " 'wap__4_ewm',\n",
    " 'wap_bid_ask_spread_imb',\n",
    " 'wap_bid_ask_spread_imb3'\n",
    " 'rsi_11_reference_price',\n",
    " 'rsi_3_reference_price',\n",
    " 'rsi_4_reference_price',\n",
    " 'rsi_4_wap',\n",
    " 'rsi_6_reference_price',\n",
    " 'rsi_7_reference_price',\n",
    " 'rsi_8_reference_price'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f2f2d4",
   "metadata": {
    "papermill": {
     "duration": 0.010436,
     "end_time": "2023-12-06T00:25:23.183992",
     "exception": false,
     "start_time": "2023-12-06T00:25:23.173556",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4db42a",
   "metadata": {},
   "source": [
    "#### Polars is used for majority of feature generation due to its faster computation vs pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899cbef7",
   "metadata": {
    "papermill": {
     "duration": 0.010522,
     "end_time": "2023-12-06T00:25:23.205295",
     "exception": false,
     "start_time": "2023-12-06T00:25:23.194773",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Function To Generate Single Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ab7153c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T00:25:23.229343Z",
     "iopub.status.busy": "2023-12-06T00:25:23.228838Z",
     "iopub.status.idle": "2023-12-06T00:25:24.363105Z",
     "shell.execute_reply": "2023-12-06T00:25:24.361673Z"
    },
    "papermill": {
     "duration": 1.150215,
     "end_time": "2023-12-06T00:25:24.366325",
     "exception": false,
     "start_time": "2023-12-06T00:25:23.216110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Grouped metrics\n",
    "grouped_bid = train.groupby('stock_id')['bid_size']\n",
    "grouped_ask = train.groupby('stock_id')['ask_size']\n",
    "\n",
    "# Various Aggregations\n",
    "global_feats = {\n",
    "    'median_sizes' : grouped_bid.median() + grouped_ask.median(),\n",
    "    'std_sizes' : grouped_bid.std() + grouped_ask.std(),\n",
    "    'max_sizes' : grouped_bid.max() + grouped_ask.max(),\n",
    "    'min_sizes' : grouped_bid.min() + grouped_ask.min(),\n",
    "    'first_sizes' : grouped_bid.first() + grouped_ask.first(),\n",
    "    'std_price' : train.groupby('stock_id')['bid_price'].std() + train.groupby('stock_id')['ask_price'].std()\n",
    "}\n",
    "\n",
    "# Function to Generate Single Features\n",
    "def generate_single_features(train):\n",
    "    \n",
    "    # Remove Missing Data\n",
    "    train.dropna(subset = ['reference_price'], inplace = True)\n",
    "    \n",
    "    # Fill NAs with 1 for log transformation\n",
    "    train.fillna(1, inplace = True)\n",
    "    \n",
    "    # Define columns to add 1 to for log transformation\n",
    "    col_add_one = ['imbalance_size', 'reference_price', 'matched_size', \n",
    "                   'far_price', 'near_price', 'bid_price', \n",
    "                   'bid_size', 'ask_price', 'ask_size']\n",
    "    \n",
    "    # Add 1 to those specified columns where those columns are 0 so that log1 = 0\n",
    "    for col in col_add_one:\n",
    "        train[col] = np.where(train[col] == 0, 1, train[col])\n",
    "    \n",
    "# Volume Features\n",
    "\n",
    "    # Ratio in bid ask size\n",
    "    train['bid_ask_ratio'] = train.bid_size / train.ask_size\n",
    "    \n",
    "    # Log bid vol\n",
    "    train['log_bid_vol'] = np.log(train.bid_size)\n",
    "      \n",
    "    # Total_vol\n",
    "    train['bid_plus_ask_sizes'] = train['bid_size'] + train['ask_size']\n",
    "    \n",
    "    # Mapping aggregated features\n",
    "    for key, value in global_feats.items():\n",
    "        train[key] = train['stock_id'].map(value.to_dict())\n",
    "    \n",
    "# Price Features\n",
    "\n",
    "    # Bid ask spread\n",
    "    train['bid_ask_spread'] = train.ask_price - train.bid_price\n",
    "\n",
    "    # Bid ask spread ratio\n",
    "    train['bid_ask_spread_ratio'] = (train.ask_price - train.bid_price) / train.ask_price\n",
    "    \n",
    "    # Mid price\n",
    "    train['mid_price'] = (train.ask_price + train.bid_price) / 2\n",
    "    \n",
    "    # Reference mid price spread ratio\n",
    "    train['ref_mid_spread_ratio'] = (train.mid_price - train.reference_price) / train.mid_price\n",
    "    \n",
    "    # Far and near price logs\n",
    "    train['log_far_price'] = np.log(train.far_price)\n",
    "    \n",
    "    # Far and near price ratio\n",
    "    train['far_near_ratio'] = train.far_price / train.near_price\n",
    "    \n",
    "    # Bid ask spread ratio\n",
    "    train['far_near_spread_ratio'] = \\\n",
    "        np.where(train.far_price != 0,\n",
    "                 (train.far_price - train.near_price) / train.far_price, 0\n",
    "                )\n",
    "    \n",
    "    # Reference price vs WAP\n",
    "    train['price_diff_auction_vs_non_auction'] = train['reference_price'] - train['wap']\n",
    "\n",
    "# Mixed features\n",
    "    \n",
    "    # Bid ask vol ratio\n",
    "    train['bid_ask_spread_vols_ratio'] = (train.bid_price - train.ask_price) / (train.bid_size + train.ask_size)\n",
    "    \n",
    "    # Ask Bid spread\n",
    "    train['price_spread'] = train.ask_price - train.bid_price\n",
    "    \n",
    "    # Liquidity imbalance using bid and ask size\n",
    "    train['liquidity_imbalance'] = (train.bid_size - train.ask_size) / (train.bid_size + train.ask_size)\n",
    "    \n",
    "    # Market urgency: combination of price spread and liquidity imbalance\n",
    "    train['market_urgency'] = train.price_spread * train.liquidity_imbalance\n",
    "    \n",
    "    # Liquidity imbalance using imblaance size and matched size\n",
    "    train['matched_imbalance'] = (train.imbalance_size - train.matched_size) / (train.matched_size + train.imbalance_size)\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7f4494",
   "metadata": {
    "papermill": {
     "duration": 0.011768,
     "end_time": "2023-12-06T00:25:24.389259",
     "exception": false,
     "start_time": "2023-12-06T00:25:24.377491",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Function To Generate Imbalance Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d9c80a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T00:25:24.414337Z",
     "iopub.status.busy": "2023-12-06T00:25:24.413789Z",
     "iopub.status.idle": "2023-12-06T00:25:24.543023Z",
     "shell.execute_reply": "2023-12-06T00:25:24.541339Z"
    },
    "papermill": {
     "duration": 0.145698,
     "end_time": "2023-12-06T00:25:24.546271",
     "exception": false,
     "start_time": "2023-12-06T00:25:24.400573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to compute triplet imbalance in parallel using Numba (Inspired From Other Kaggle Users)\n",
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "\n",
    "    # Loop through all combinations of triplets\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        \n",
    "        # Loop through rows of the DataFrame\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            \n",
    "            # Prevent division by zero\n",
    "            if mid_val == min_val:\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "# Function to calculate triplet imbalance for given price data and a DataFrame\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    # Convert DataFrame to numpy array for Numba compatibility\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "\n",
    "    # Calculate the triplet imbalance using the Numba-optimized function\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    columns = [f'{a}_{b}_{c}_imb2' for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "\n",
    "    return features\n",
    "\n",
    "def generate_imb_features(train):\n",
    "       \n",
    "# Computing Imbalance Features\n",
    "    prices = ['reference_price','far_price', 'near_price', 'ask_price', \n",
    "              'mid_price', 'bid_price', 'wap', 'bid_ask_spread']\n",
    "    \n",
    "    volume = ['imbalance_size', 'matched_size', 'bid_size', \n",
    "              'ask_size', 'bid_plus_ask_sizes', 'bid_ask_ratio']\n",
    "    \n",
    "    correl_columns = ['reference_price_ask_price_imb', 'reference_price_bid_price_imb', 'target',\n",
    "               'far_price_ask_price_bid_price_imb2', 'far_near_spread_ratio', 'reference_price_far_price_near_price_imb2', \n",
    "                'liquidity_imbalance', 'market_urgency', 'price_diff_auction_vs_non_auction', 'log_bid_vol', 'bid_ask_ratio', \n",
    "              'bid_size', 'ask_size', 'ref_mid_spread_ratio', 'imbalance_buy_sell_flag', 'bid_price', 'ask_price', 'mid_price', 'reference_price']\n",
    "    \n",
    "    rolling_columns = ['imbalance_size', 'matched_size', 'reference_price', 'imbalance_buy_sell_flag']\n",
    "    \n",
    "    loop_remove_cols = [col for col in useless if (col not in prices and col not in volume and col not in correl_columns and col not in rolling_columns)]  \n",
    "    \n",
    "    for groups in [prices, volume]:\n",
    "        \n",
    "        #imb1\n",
    "        for c in combinations(groups, 2):\n",
    "            if f'{c[0]}_{c[1]}_imb' not in useless:\n",
    "                train[f'{c[0]}_{c[1]}_imb'] = train.eval(f'({c[0]} - {c[1]})/({c[0]} + {c[1]})')\n",
    "        #imb3\n",
    "            if f'{c[0]}_{c[1]}_imb3' not in useless:\n",
    "                train[f'{c[0]}_{c[1]}_imb3'] = (train[c[0]] / train[c[0]].sum()) / (train[c[1]] / train[c[1]].sum())\n",
    "        \n",
    "        #imb2\n",
    "        triplet_feature = calculate_triplet_imbalance_numba(groups, train)\n",
    "        train[triplet_feature.columns] = triplet_feature.values\n",
    "        \n",
    "        del triplet_feature\n",
    "        \n",
    "        train = memory_management(train, loop_remove_cols)\n",
    "        \n",
    "        reduce_mem_cols = [col for col in train.columns if (col not in prices and col not in volume and col not in correl_columns and col not in rolling_columns)]\n",
    "        \n",
    "        train[reduce_mem_cols] = reduce_mem_usage(train[reduce_mem_cols])\n",
    "        \n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ca2149",
   "metadata": {
    "papermill": {
     "duration": 0.011348,
     "end_time": "2023-12-06T00:25:24.569287",
     "exception": false,
     "start_time": "2023-12-06T00:25:24.557939",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Function To Generate Lag Features\n",
    "\n",
    "#### Generate lag features using `shift`, `diff` and `pct_change`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceb12dbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T00:25:24.594493Z",
     "iopub.status.busy": "2023-12-06T00:25:24.593938Z",
     "iopub.status.idle": "2023-12-06T00:25:24.604099Z",
     "shell.execute_reply": "2023-12-06T00:25:24.602674Z"
    },
    "papermill": {
     "duration": 0.026201,
     "end_time": "2023-12-06T00:25:24.606787",
     "exception": false,
     "start_time": "2023-12-06T00:25:24.580586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_lag_features(train):\n",
    "                 \n",
    "    pl_df = pl.from_pandas(train)\n",
    "    \n",
    "    rolling_columns = ['imbalance_size', 'matched_size', 'reference_price', 'imbalance_buy_sell_flag']\n",
    "    windows = [1, 2, 3, 10]  \n",
    "\n",
    "    # Need to groupby stock and date to prevent spillovers from EOD to the following day\n",
    "    group = ['stock_id', 'date_id']\n",
    "    \n",
    "    for col in rolling_columns:\n",
    "        for window in windows:\n",
    "\n",
    "            expressions = [\n",
    "                 pl.col(col).shift(window)\n",
    "                            .over(group)\n",
    "                            .alias(f'{col}_shift_{window}'),\n",
    "                 pl.col(col).diff(window)\n",
    "                            .over(group)\n",
    "                            .alias(f'{col}_diff_{window}'),\n",
    "                 pl.col(col).pct_change(window)\n",
    "                            .over(group)\n",
    "                            .alias(f'{col}_pct_change_{window}')   \n",
    "            ]\n",
    "\n",
    "            pl_df = pl_df.with_columns(expressions)\n",
    "    \n",
    "    \n",
    "    train = pl_df.to_pandas()\n",
    "   \n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f235de",
   "metadata": {},
   "source": [
    "## Function To Generate Rolling Features\n",
    "#### Generate rolling features using rolling mean (`rolling_mean`) and expoential moving average (`ewm_mean`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b9dc219",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T00:25:24.631399Z",
     "iopub.status.busy": "2023-12-06T00:25:24.630888Z",
     "iopub.status.idle": "2023-12-06T00:25:24.645413Z",
     "shell.execute_reply": "2023-12-06T00:25:24.644155Z"
    },
    "papermill": {
     "duration": 0.030374,
     "end_time": "2023-12-06T00:25:24.648301",
     "exception": false,
     "start_time": "2023-12-06T00:25:24.617927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_rolling_features(train):\n",
    "    \n",
    "    # Relevant Features\n",
    "    keep = ['liquidity_imbalance__1_rolling_mean', 'liquidity_imbalance', 'liquidity_imbalance__2_rolling_mean', 'market_urgency__1_rolling_mean', \n",
    "            'market_urgency', 'liquidity_imbalance__3_rolling_mean', 'price_diff_auction_vs_non_auction__1_rolling_mean', 'price_diff_auction_vs_non_auction', \n",
    "            'market_urgency__2_rolling_mean', 'liquidity_imbalance__4_rolling_mean', 'log_bid_vol', 'log_bid_vol__1_rolling_mean', 'market_urgency__3_rolling_mean', \n",
    "            'price_diff_auction_vs_non_auction__2_rolling_mean', 'bid_ask_ratio__1_rolling_mean', 'bid_ask_ratio', 'market_urgency__4_rolling_mean', 'log_bid_vol__2_rolling_mean', \n",
    "            'price_diff_auction_vs_non_auction__3_rolling_mean', 'price_diff_auction_vs_non_auction__4_rolling_mean', 'bid_ask_ratio__2_rolling_mean', 'log_bid_vol__3_rolling_mean', \n",
    "            'bid_ask_ratio__3_rolling_mean', 'log_bid_vol__4_rolling_mean', 'bid_size', 'bid_size__1_rolling_mean', 'bid_ask_ratio__4_rolling_mean', 'ask_size__1_rolling_mean', \n",
    "            'ask_size', 'bid_size__2_rolling_mean', 'ref_mid_spread_ratio__1_rolling_mean', 'ref_mid_spread_ratio', 'wap__2_rolling_mean', 'ask_size__2_rolling_mean', \n",
    "            'ref_mid_spread_ratio__2_rolling_mean', 'bid_size__3_rolling_mean', 'ref_mid_spread_ratio__3_rolling_mean', 'wap__3_rolling_mean', 'ref_mid_spread_ratio__4_rolling_mean', \n",
    "            'ask_size__3_rolling_mean', 'bid_size__4_rolling_mean', 'imbalance_buy_sell_flag', 'imbalance_buy_sell_flag__1_rolling_mean', 'wap__4_rolling_mean', 'ask_size__4_rolling_mean', \n",
    "            'mid_price__2_rolling_mean', 'bid_price__2_rolling_mean', 'bid_price__1_rolling_mean', 'bid_price', 'ask_price__2_rolling_mean', 'ask_price__1_rolling_mean', 'ask_price', \n",
    "            'imbalance_buy_sell_flag__2_rolling_mean', 'mid_price__1_rolling_mean', 'mid_price', 'mid_price__3_rolling_mean', 'bid_price__3_rolling_mean', 'ask_price__3_rolling_mean', \n",
    "            'imbalance_buy_sell_flag__3_rolling_mean', 'mid_price__4_rolling_mean']\n",
    "    \n",
    "    # Convert from pandas\n",
    "    pl_df = pl.from_pandas(train)\n",
    "\n",
    "    # Prepare the operations\n",
    "    column = [col for col in train.columns if col not in ['stock_id', 'seconds_in_bucket', 'date_id', 'target', 'row_id', 'time_id']]\n",
    "    windows = list(range(1,5))\n",
    "\n",
    "    # Need to groupby stock and date to prevent spillovers from EOD to the following day\n",
    "    group = ['stock_id', 'date_id']\n",
    "    types = ['rolling_mean', 'ewm']\n",
    "    \n",
    "    for col in column:\n",
    "        for window in windows:\n",
    "            for i in types:\n",
    "                if f'{col}__{window}_{i}' in keep: # Only generate the required rolling features\n",
    "                    \n",
    "                    expressions = [\n",
    "                         pl.col(col).rolling_mean(window)\n",
    "                                    .over(group)\n",
    "                                    .alias(f'{col}__{window}_rolling_mean'),\n",
    "                         pl.col(col).ewm_mean(half_life = window)\n",
    "                                    .over(group)\n",
    "                                    .last()\n",
    "                                    .alias(f'{col}__{window}_ewm')\n",
    "                    ]\n",
    "\n",
    "                    # run the operations\n",
    "                    pl_df = pl_df.with_columns(expressions)\n",
    "\n",
    "    # back to pandas\n",
    "    train = pl_df.to_pandas()\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9643214a",
   "metadata": {
    "papermill": {
     "duration": 0.010956,
     "end_time": "2023-12-06T00:25:24.670302",
     "exception": false,
     "start_time": "2023-12-06T00:25:24.659346",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Technical Analysis\n",
    "#### Generate technical analysis - RSI, Bollinger Bands, Averate True Range, MACD, Historical Volatility, Keltner Channel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6feb40",
   "metadata": {},
   "source": [
    "### RSI\n",
    "#### Relative Strength Index (RSI) is a momentum indicator that measures the magnitude of recent price changes to analyze overbought or oversold conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e5d528e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T00:25:24.694938Z",
     "iopub.status.busy": "2023-12-06T00:25:24.694447Z",
     "iopub.status.idle": "2023-12-06T00:25:24.859713Z",
     "shell.execute_reply": "2023-12-06T00:25:24.858324Z"
    },
    "papermill": {
     "duration": 0.181641,
     "end_time": "2023-12-06T00:25:24.862921",
     "exception": false,
     "start_time": "2023-12-06T00:25:24.681280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_rsi(train):\n",
    "    \n",
    "    pl_df = pl.from_pandas(train)\n",
    "\n",
    "    rsi_col = ['reference_price', 'far_price', 'near_price', 'matched_size', 'wap']\n",
    "    windows = [3, 7, 14]\n",
    "    group = ['stock_id', 'date_id']\n",
    "\n",
    "    for col in rsi_col:\n",
    "        for window in windows: \n",
    "            if f'rsi_{window}_{col}' not in useless:\n",
    "              \n",
    "                U = pl.when(pl.col(col).pct_change() >= 0).then(pl.col(col).pct_change()).otherwise(0.0).alias('U')\n",
    "                V = pl.when(pl.col(col).pct_change() < 0).then((pl.col(col).pct_change()).abs()).otherwise(0.0).alias('V')\n",
    "\n",
    "                pl_df = pl_df.with_columns([U, V])\n",
    "\n",
    "                U = pl.col('U').rolling_mean(window).over(group)\n",
    "                V = pl.col('V').rolling_mean(window).over(group)\n",
    "\n",
    "                rsi_expression = (100 * (U / (U + V))).alias(f'rsi_{window}_{col}')\n",
    "                pl_df = pl_df.with_columns(rsi_expression)\n",
    "\n",
    "    pl_df = pl_df.drop(['U', 'V'])\n",
    "\n",
    "    train = pl_df.to_pandas()\n",
    "                               \n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5fd9e2",
   "metadata": {},
   "source": [
    "### Bollinger Bands\n",
    "#### Generate oversold or overbought signals; identify sharp, short-term price movements and potential entry and exit points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c8f65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bbands(train):\n",
    "    \n",
    "    pl_df = pl.from_pandas(train)\n",
    "    \n",
    "    bband_col = ['reference_price', 'far_price', 'near_price', 'imbalance_size', 'matched_size', 'wap']\n",
    "    window = 20\n",
    "    multiplier = 2\n",
    "    group = ['stock_id', 'date_id']\n",
    "    \n",
    "    for col in bband_col:\n",
    "        \n",
    "        expressions = [\n",
    "            (\n",
    "                pl.col(col).rolling_mean(window).over(group)\n",
    "                + (multiplier * pl.col(col).rolling_std(window).over(group))\n",
    "            ).alias(f'upper_bband_{col}'),\n",
    "            (\n",
    "                pl.col(col).rolling_mean(window).over(group)\n",
    "                - (multiplier * pl.col(col).rolling_std(window).over(group))\n",
    "            ).alias(f'lower_band_{col}')\n",
    "                ]\n",
    "        pl_df = pl_df.with_columns(expressions)\n",
    "\n",
    "    # Computing Bollinger Band breakout (narrow bands) over different windows    \n",
    "    windows = [3, 7, 14]\n",
    "    for col in bband_col:\n",
    "        for window in windows:\n",
    "        \n",
    "            expressions = [\n",
    "                (pl.col(f'upper_bband_{col}') - pl.col(f'lower_band_{col}')).shift(window)\n",
    "                                                                            .over(group)\n",
    "                                                                            .alias(f'bband_breakout_{window}_{col}')\n",
    "            ]\n",
    "            \n",
    "            pl_df = pl_df.with_columns(expressions)\n",
    "        \n",
    "    train = pl_df.to_pandas()\n",
    "                               \n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c23512",
   "metadata": {},
   "source": [
    "### Moving average convergence/divergence\n",
    "#### Momentum indicator that shows the relationship between two moving averages of a security's price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d412c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_macd(train):\n",
    "    \n",
    "    pl_df = pl.from_pandas(train)\n",
    "    \n",
    "    column = ['reference_price', 'far_price', 'near_price', 'imbalance_size', 'matched_size', 'wap']\n",
    "    group = ['stock_id', 'date_id']\n",
    "    \n",
    "    for col in column:\n",
    "    \n",
    "        expressions = [\n",
    "             (pl.col(col).ewm_mean(span = 12)\n",
    "                        .over(group)\n",
    "                        .last()\n",
    "                        -\n",
    "             pl.col(col).ewm_mean(span = 26)\n",
    "                        .over(group)\n",
    "                        .last())\n",
    "                        .alias(f'MACD_{col}'),\n",
    "             pl.col(col).ewm_mean(span = 9)\n",
    "                        .over(group)\n",
    "                        .last()\n",
    "                        .alias(f'Signal_{col}')\n",
    "        ]    \n",
    "\n",
    "        pl_df = pl_df.with_columns(expressions)\n",
    "\n",
    "    train = pl_df.to_pandas()\n",
    "                               \n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38868c85",
   "metadata": {},
   "source": [
    "### Average True Range\n",
    "#### Volatility indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d280259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_atr(train):\n",
    "    \n",
    "    pl_df = pl.from_pandas(train)\n",
    "    \n",
    "    groups =  ['reference_price', 'far_price', 'near_price', 'wap', 'bid_price', 'ask_price']\n",
    "    group = ['stock_id', 'date_id']\n",
    "    window = 14\n",
    "    \n",
    "    for c in combinations(groups, 2):\n",
    "    \n",
    "        expressions = [\n",
    "            (pl.col(c[0]) - pl.col(c[1]))\n",
    "                    .fill_null(0)\n",
    "                    .abs()\n",
    "                    .rolling_mean(window)\n",
    "                    .over(group)\n",
    "                    .alias(f'{c[0]}_{c[1]}_atr_{window}'),\n",
    "        ]\n",
    "\n",
    "        pl_df = pl_df.with_columns(expressions)\n",
    "        \n",
    "    train = pl_df.to_pandas()\n",
    "                               \n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65207fe0",
   "metadata": {},
   "source": [
    "### Historical Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d41a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_HV(train):\n",
    "    \n",
    "    pl_df = pl.from_pandas(train)\n",
    "    \n",
    "    column = ['reference_price', 'far_price', 'near_price', 'wap', 'ask_price', 'bid_price']\n",
    "    group = ['stock_id', 'date_id']\n",
    "    \n",
    "    windows = [5, 10, 14]\n",
    "    for col in column:\n",
    "        for win in windows:\n",
    "            expressions = [            \n",
    "       pl.col(col).pct_change()\n",
    "                  .over(group).alias('pct_c')\n",
    "            ]    \n",
    "\n",
    "            pl_df = pl_df.with_columns(expressions)\n",
    "            \n",
    "            expressions = [\n",
    "                pl.col('pct_c')\n",
    "                  .rolling_mean(win)\n",
    "                  .over(group)\n",
    "                  .std()\n",
    "                  .alias(f'HV_{win}_{col}')\n",
    "            ]\n",
    "            \n",
    "            pl_df = pl_df.with_columns(expressions)\n",
    "            \n",
    "    pl_df.drop('pct_c')\n",
    "    train = pl_df.to_pandas()\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f44ddc5",
   "metadata": {},
   "source": [
    "### Keltner Channel\n",
    "#### Tracks volatility using an asset's exponential moving average and average true range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1781a034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_keltner(train):\n",
    "    \n",
    "    pl_df = pl.from_pandas(train)\n",
    "    \n",
    "    window = 14\n",
    "    multiplier = 2\n",
    "    group = ['stock_id','date_id']\n",
    "    \n",
    "    expressions = [\n",
    "        ((pl.col('ask_price') - pl.col('bid_price'))\n",
    "                .rolling_mean(window).over(group) + pl.col(f'bid_price_ask_price_atr_{window}') * multiplier).alias(f'keltner_{window}_upper_bid_ask'),\n",
    "        ((pl.col('ask_price') - pl.col('bid_price'))\n",
    "                .rolling_mean(window).over(group) - pl.col(f'bid_price_ask_price_atr_{window}') * multiplier).alias(f'keltner_{window}_lower_bid_ask'),\n",
    "        ((pl.col('reference_price') - pl.col('wap'))\n",
    "                .fill_null(0).abs()\n",
    "                .rolling_mean(window).over(group) + pl.col(f'reference_price_wap_atr_{window}') * multiplier).alias(f'keltner_{window}_upper_ref_wap'),\n",
    "        ((pl.col('reference_price') - pl.col('wap'))\n",
    "                .fill_null(0).abs()\n",
    "                .rolling_mean(window).over(group) - pl.col(f'reference_price_wap_atr_{window}') * multiplier).alias(f'keltner_{window}_lower_ref_wap')        \n",
    "    ]    \n",
    "\n",
    "    pl_df = pl_df.with_columns(expressions)\n",
    "    \n",
    "    train = pl_df.to_pandas()\n",
    "                               \n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05021ee",
   "metadata": {},
   "source": [
    "## Function To Generate All Technical Analysis Features\n",
    "`reduce_mem_usage` is applied after every function call to handle memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab80993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ta(train):\n",
    "\n",
    "    train = generate_rsi(train)\n",
    "    train = generate_bbands(train)\n",
    "    \n",
    "    train = reduce_mem_usage(train) \n",
    "    \n",
    "    train = generate_atr(train)    \n",
    "    train = generate_macd(train)\n",
    "    \n",
    "    train = reduce_mem_usage(train)\n",
    "    \n",
    "    train = generate_HV(train)\n",
    "    train = generate_keltner(train)\n",
    "    \n",
    "    train = reduce_mem_usage(train)\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315a3006",
   "metadata": {
    "papermill": {
     "duration": 0.011006,
     "end_time": "2023-12-06T00:25:24.885704",
     "exception": false,
     "start_time": "2023-12-06T00:25:24.874698",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f5223df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T00:25:24.917381Z",
     "iopub.status.busy": "2023-12-06T00:25:24.916842Z",
     "iopub.status.idle": "2023-12-06T00:25:24.928791Z",
     "shell.execute_reply": "2023-12-06T00:25:24.927269Z"
    },
    "papermill": {
     "duration": 0.034452,
     "end_time": "2023-12-06T00:25:24.936131",
     "exception": false,
     "start_time": "2023-12-06T00:25:24.901679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def standardize_by_group(df, by):\n",
    "    \n",
    "    columns = ['imbalance_size', 'matched_size', 'ref_mid_spread_ratio', 'far_near_ratio']\n",
    "    \n",
    "    groups = df.groupby(by)\n",
    "    \n",
    "    mean = groups[columns].transform('mean')\n",
    "    std = groups[columns].transform('std')\n",
    "    normalized = (df[columns] - mean) / std\n",
    "\n",
    "    merged = df.merge(\n",
    "        normalized,\n",
    "        how='outer', \n",
    "        left_index = True, \n",
    "        right_index = True, \n",
    "        suffixes = ('_x', '_standardized')\n",
    "        )\n",
    "    \n",
    "    merged = merged[[x for x in merged.columns.tolist() if '_x' not in x]]\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca9663",
   "metadata": {
    "papermill": {
     "duration": 0.011375,
     "end_time": "2023-12-06T00:25:24.961923",
     "exception": false,
     "start_time": "2023-12-06T00:25:24.950548",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e41980d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T00:25:24.996829Z",
     "iopub.status.busy": "2023-12-06T00:25:24.995845Z",
     "iopub.status.idle": "2023-12-06T00:25:25.009274Z",
     "shell.execute_reply": "2023-12-06T00:25:25.008110Z"
    },
    "papermill": {
     "duration": 0.033856,
     "end_time": "2023-12-06T00:25:25.012221",
     "exception": false,
     "start_time": "2023-12-06T00:25:24.978365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def correl(dictionary, cols, train):\n",
    "    \n",
    "    pl_df = pl.from_pandas(train)\n",
    "\n",
    "    # Aggregates Data to get the mean as the market Data\n",
    "    for col_name in cols:\n",
    "        market_movement = pl_df.select(['date_id', 'seconds_in_bucket', col_name]).group_by(['date_id', 'seconds_in_bucket']).mean().rename({col_name : 'market'})\n",
    "        stock_movement = pl_df.select(['stock_id', 'date_id', 'seconds_in_bucket', col_name]).pivot(columns = 'stock_id', values = col_name, index = ['date_id', 'seconds_in_bucket'])\n",
    "\n",
    "        correl = (market_movement.join(stock_movement, how = 'left',\n",
    "                left_on = ['date_id','seconds_in_bucket'],\n",
    "                right_on = ['date_id','seconds_in_bucket']).drop(['date_id','seconds_in_bucket']))\n",
    "\n",
    "        correl = correl.to_pandas()\n",
    "\n",
    "        correlation = \\\n",
    "            correl['market'] \\\n",
    "                .iloc[:-1,] \\\n",
    "                    .reset_index() \\\n",
    "                        .rename(\n",
    "                            columns = {'index' : 'stock_id', \n",
    "                                    'market' : f'correl_{col_name}'}\n",
    "                            )\n",
    "\n",
    "        del correl, market_movement\n",
    "        \n",
    "        dictionary[f'correl_{col_name}'] = correlation[f'correl_{col_name}'].to_dict()\n",
    "        \n",
    "    return dictionary\n",
    "\n",
    "def correlation_features(train):\n",
    "    \n",
    "    correl_columns = ['reference_price_ask_price_imb', 'reference_price_bid_price_imb', 'target',\n",
    "               'far_price_ask_price_bid_price_imb2', 'far_near_spread_ratio', 'reference_price_far_price_near_price_imb2', \n",
    "                'liquidity_imbalance', 'market_urgency', 'price_diff_auction_vs_non_auction', 'log_bid_vol', 'bid_ask_ratio', \n",
    "              'bid_size', 'ask_size', 'ref_mid_spread_ratio', 'imbalance_buy_sell_flag', 'bid_price', 'ask_price', 'mid_price', 'reference_price']\n",
    "    \n",
    "    correl_feats = {}\n",
    "\n",
    "    correl_feats = correl(correl_feats, correl_columns, train)\n",
    "    \n",
    "    return correl_feats\n",
    "\n",
    "def map_correl_feats(df, feats):\n",
    "    for key, values in feats.items():\n",
    "\n",
    "        df[key] = df['stock_id'].map(values)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b0ab1d",
   "metadata": {
    "papermill": {
     "duration": 0.010563,
     "end_time": "2023-12-06T00:25:25.033945",
     "exception": false,
     "start_time": "2023-12-06T00:25:25.023382",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generate All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8a89be6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T00:25:25.059753Z",
     "iopub.status.busy": "2023-12-06T00:25:25.058027Z",
     "iopub.status.idle": "2023-12-06T00:34:07.240407Z",
     "shell.execute_reply": "2023-12-06T00:34:07.238691Z"
    },
    "papermill": {
     "duration": 522.210738,
     "end_time": "2023-12-06T00:34:07.255585",
     "exception": false,
     "start_time": "2023-12-06T00:25:25.044847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 2637.42 MB\n",
      "Memory usage after optimization is: 819.20 MB\n",
      "Decreased by 68.94%\n",
      "Memory usage of dataframe is 1578.46 MB\n",
      "Memory usage after optimization is: 1088.94 MB\n",
      "Decreased by 31.01%\n",
      "Memory usage of dataframe is 6353.79 MB\n",
      "Memory usage after optimization is: 2662.40 MB\n",
      "Decreased by 58.10%\n",
      "Memory usage of dataframe is 4540.56 MB\n",
      "Memory usage after optimization is: 2972.09 MB\n",
      "Decreased by 34.54%\n",
      "Memory usage of dataframe is 5040.07 MB\n",
      "Memory usage after optimization is: 3201.87 MB\n",
      "Decreased by 36.47%\n",
      "261\n"
     ]
    }
   ],
   "source": [
    "def generate_all_features(train, correl_feats = None):\n",
    "    \n",
    "    train = generate_single_features(train)\n",
    "    train = generate_rolling_features(train)\n",
    "    stand_df = train[['stock_id','imbalance_size', 'matched_size', 'ref_mid_spread_ratio', 'far_near_ratio']]\n",
    "    train = generate_imb_features(train)\n",
    "    train = generate_lag_features(train)\n",
    "    train = generate_ta(train)\n",
    "\n",
    "    if flag != 'test':\n",
    "        correl_feats = correlation_features(train)\n",
    "        train = standardize_by_group(train, 'stock_id')\n",
    "        \n",
    "    train = map_correl_feats(train, correl_feats)\n",
    "\n",
    "    train = memory_management(train, useless)\n",
    "    \n",
    "    train = train.replace([np.inf], 9999)\n",
    "    train = train.replace([-np.inf], -9999)\n",
    "    \n",
    "    if flag != 'test':\n",
    "        return train, stand_df, correl_feats\n",
    "    elif flag == 'test':\n",
    "        return train\n",
    "\n",
    "train1, stand_df, correl_feats = generate_all_features(train)\n",
    "print(len(train1.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9456d0",
   "metadata": {
    "papermill": {
     "duration": 0.01136,
     "end_time": "2023-12-06T00:34:07.278895",
     "exception": false,
     "start_time": "2023-12-06T00:34:07.267535",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "867daa1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T00:34:07.308048Z",
     "iopub.status.busy": "2023-12-06T00:34:07.307223Z",
     "iopub.status.idle": "2023-12-06T00:34:10.200988Z",
     "shell.execute_reply": "2023-12-06T00:34:10.199315Z"
    },
    "papermill": {
     "duration": 2.912675,
     "end_time": "2023-12-06T00:34:10.204632",
     "exception": false,
     "start_time": "2023-12-06T00:34:07.291957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error as mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80a51d02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T00:34:10.232523Z",
     "iopub.status.busy": "2023-12-06T00:34:10.231978Z",
     "iopub.status.idle": "2023-12-06T00:34:15.678325Z",
     "shell.execute_reply": "2023-12-06T00:34:15.676561Z"
    },
    "papermill": {
     "duration": 5.464946,
     "end_time": "2023-12-06T00:34:15.681969",
     "exception": false,
     "start_time": "2023-12-06T00:34:10.217023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = train1.drop(columns = ['date_id', 'target', 'row_id', 'time_id'])\n",
    "Y = train1['target']\n",
    "\n",
    "date_ids = train1['date_id'].values\n",
    "\n",
    "del train1\n",
    "\n",
    "y_min = np.min(Y)\n",
    "y_max = np.max(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60eccd99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T00:34:15.709273Z",
     "iopub.status.busy": "2023-12-06T00:34:15.708708Z",
     "iopub.status.idle": "2023-12-06T00:34:15.717803Z",
     "shell.execute_reply": "2023-12-06T00:34:15.716322Z"
    },
    "papermill": {
     "duration": 0.026626,
     "end_time": "2023-12-06T00:34:15.721336",
     "exception": false,
     "start_time": "2023-12-06T00:34:15.694710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def importance(final_model):\n",
    "    importance = pd.DataFrame({\n",
    "        'feature_name': final_model.booster_.feature_name(),\n",
    "        'importance_gain': final_model.booster_.feature_importance(importance_type='gain'),\n",
    "        'importance_split': final_model.booster_.feature_importance(importance_type='split'),\n",
    "    }).sort_values('importance_gain', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    importance['pct'] = importance.importance_gain / importance.importance_gain.sum()\n",
    "    importance['running'] = importance.pct.cumsum()\n",
    "    \n",
    "    return importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc60d24e",
   "metadata": {
    "papermill": {
     "duration": 0.012351,
     "end_time": "2023-12-06T00:34:15.849524",
     "exception": false,
     "start_time": "2023-12-06T00:34:15.837173",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15897dd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T00:34:15.877438Z",
     "iopub.status.busy": "2023-12-06T00:34:15.876924Z",
     "iopub.status.idle": "2023-12-06T00:34:15.897530Z",
     "shell.execute_reply": "2023-12-06T00:34:15.896018Z"
    },
    "papermill": {
     "duration": 0.038018,
     "end_time": "2023-12-06T00:34:15.900105",
     "exception": false,
     "start_time": "2023-12-06T00:34:15.862087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_folds = 5\n",
    "models = []\n",
    "\n",
    "if flag != 'submission':\n",
    "    lgb_params = {\n",
    "        'objective': 'mae',\n",
    "        'n_estimators': 8000,\n",
    "        'num_leaves': 32,\n",
    "        'max_depth': 122,\n",
    "        'learning_rate': 0.008,\n",
    "        'device': 'gpu',\n",
    "        'n_jobs': 4,\n",
    "        'verbosity': -1,\n",
    "        'importance_type': 'gain'\n",
    "    }\n",
    "\n",
    "    feature_name = list(X.columns)\n",
    "\n",
    "    # The total number of date_ids is 480, we split them into 5 folds\n",
    "    fold_size = 480 // num_folds\n",
    "    gap = 5\n",
    "    \n",
    "    scores = []\n",
    "\n",
    "    for i in range(num_folds):\n",
    "        start = i * fold_size\n",
    "        end = start + fold_size\n",
    "\n",
    "    # Purging\n",
    "        if i < num_folds - 1:  # No need to purge after the last fold\n",
    "            purged_start = end - 2\n",
    "            purged_end = end + gap + 2\n",
    "            train_indices = (date_ids >= start) & (date_ids < purged_start) | (date_ids > purged_end)\n",
    "        else:\n",
    "            train_indices = (date_ids >= start) & (date_ids < end)\n",
    "\n",
    "        test_indices = (date_ids >= end) & (date_ids < end + fold_size)\n",
    "\n",
    "        # End Purge\n",
    "\n",
    "        # Train Test Split\n",
    "        df_fold_train = X[train_indices]\n",
    "        df_fold_train_target = Y[train_indices]\n",
    "        df_fold_valid = X[test_indices]\n",
    "        df_fold_valid_target = Y[test_indices]\n",
    "\n",
    "        print(f'Fold {i+1} model Training')\n",
    "\n",
    "        # Train a LightGBM model for the current fold\n",
    "        lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "        lgb_model.fit(\n",
    "            df_fold_train[feature_name],\n",
    "            df_fold_train_target,\n",
    "            eval_set=[(df_fold_valid[feature_name], df_fold_valid_target)],\n",
    "            callbacks=[\n",
    "                lgb.callback.early_stopping(stopping_rounds=100),\n",
    "                lgb.callback.log_evaluation(period=100),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Append the model to the list\n",
    "        models.append(lgb_model)\n",
    "        model_filename = f'/kaggle/working/lgbm_{i+1}.pkl'\n",
    "        joblib.dump(lgb_model, model_filename)\n",
    "\n",
    "        # Evaluate model performance on the validation set\n",
    "        fold_predictions = lgb_model.predict(df_fold_valid[feature_name])\n",
    "        fold_score = mae(fold_predictions, df_fold_valid_target)\n",
    "        scores.append(fold_score)\n",
    "        print(f'Fold {i+1} MAE: {fold_score}')\n",
    "\n",
    "        col_importance = importance(lgb_model)\n",
    "        print(col_importance[col_importance.running < 0.80])\n",
    "        print(col_importance[col_importance.running > 0.95].feature_name.tolist())\n",
    "        print('COMPLETELY uselesss features')\n",
    "        print(col_importance[col_importance.pct == 0].feature_name.tolist())\n",
    "\n",
    "        # Free up memory by deleting fold specific variables\n",
    "        del df_fold_train, df_fold_train_target, df_fold_valid, df_fold_valid_target\n",
    "        gc.collect()\n",
    "\n",
    "    # Calculate the average best iteration from all regular folds\n",
    "    average_best_iteration = int(np.mean([model.best_iteration_ for model in models]))\n",
    "\n",
    "    # Update the lgb_params with the average best iteration\n",
    "    final_model_params = lgb_params.copy()\n",
    "    final_model_params['n_estimators'] = average_best_iteration\n",
    "\n",
    "    print(f'Training final model with average best iteration: {average_best_iteration}')\n",
    "\n",
    "    # Train the final model on the entire dataset\n",
    "    final_model = lgb.LGBMRegressor(**final_model_params)\n",
    "    final_model.fit(\n",
    "        X[feature_name],\n",
    "        Y,\n",
    "        callbacks=[\n",
    "            lgb.callback.log_evaluation(period=100),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Append the final model to the list of models\n",
    "    models.append(final_model)\n",
    "    model_filename = f'/kaggle/working/lgbm_6.pkl'\n",
    "    joblib.dump(final_model, model_filename)\n",
    "\n",
    "    # Average scores for all models\n",
    "    print(f'Average MAE across all folds: {np.mean(scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bef1bbd",
   "metadata": {
    "papermill": {
     "duration": 0.011982,
     "end_time": "2023-12-06T00:34:15.924898",
     "exception": false,
     "start_time": "2023-12-06T00:34:15.912916",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9b8614f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T00:34:15.953190Z",
     "iopub.status.busy": "2023-12-06T00:34:15.952704Z",
     "iopub.status.idle": "2023-12-06T00:34:15.964518Z",
     "shell.execute_reply": "2023-12-06T00:34:15.962907Z"
    },
    "papermill": {
     "duration": 0.029608,
     "end_time": "2023-12-06T00:34:15.967996",
     "exception": false,
     "start_time": "2023-12-06T00:34:15.938388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def standardize_new_data(train, new, by):\n",
    "    columns = ['imbalance_size', 'matched_size', 'ref_mid_spread_ratio', 'far_near_ratio']\n",
    "    groups = train.groupby(by)\n",
    "    means = groups[columns].mean().reset_index()\n",
    "    means = means.rename(columns={c: c+'_mean' for c in means.columns if c not in ['stock_id']})\n",
    "    stds = groups[columns].std().reset_index()\n",
    "    stds = stds.rename(columns={c: c+'_std' for c in stds.columns if c not in ['stock_id']})\n",
    "\n",
    "    feat = new \\\n",
    "        .merge(\n",
    "            means, \n",
    "            how = 'inner', \n",
    "            left_on = 'stock_id', \n",
    "            right_on = 'stock_id'\n",
    "        ) \\\n",
    "            .merge(\n",
    "                stds, \n",
    "                how = 'inner', \n",
    "                left_on = 'stock_id', \n",
    "                right_on = 'stock_id'\n",
    "            )\n",
    "    \n",
    "    for column in columns:\n",
    "        col_mean = column + '_mean'\n",
    "        col_std = column + '_std'\n",
    "        new_col = column + '_standardized'\n",
    "        feat[new_col] = (feat[column] - feat[col_mean]) / feat[col_std]\n",
    "\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090c35e2",
   "metadata": {
    "papermill": {
     "duration": 0.012092,
     "end_time": "2023-12-06T00:34:15.992784",
     "exception": false,
     "start_time": "2023-12-06T00:34:15.980692",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Competition Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bdd4406",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-06T00:34:16.020115Z",
     "iopub.status.busy": "2023-12-06T00:34:16.019655Z",
     "iopub.status.idle": "2023-12-06T00:39:02.018464Z",
     "shell.execute_reply": "2023-12-06T00:39:02.017101Z"
    },
    "papermill": {
     "duration": 286.01621,
     "end_time": "2023-12-06T00:39:02.021532",
     "exception": false,
     "start_time": "2023-12-06T00:34:16.005322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n",
      "10 queries per second: 1.673547124862671\n",
      "20 queries per second: 1.6412729263305663\n",
      "30 queries per second: 1.6507878065109254\n",
      "40 queries per second: 1.6520367562770844\n",
      "50 queries per second: 1.657225294113159\n",
      "60 queries per second: 1.6639484484990439\n",
      "70 queries per second: 1.691105318069458\n",
      "80 queries per second: 1.6985257685184478\n",
      "90 queries per second: 1.6995767831802369\n",
      "100 queries per second: 1.702619800567627\n",
      "110 queries per second: 1.7015141833912242\n",
      "120 queries per second: 1.7143314599990844\n",
      "130 queries per second: 1.7237979558797982\n",
      "140 queries per second: 1.7240573780877249\n",
      "150 queries per second: 1.7218062957127889\n",
      "160 queries per second: 1.720225267112255\n",
      "The code will take approximately 1.97 hours\n"
     ]
    }
   ],
   "source": [
    "if flag == 'submission':\n",
    "    def zero_sum(prices, volumes):\n",
    "        std_error = np.sqrt(volumes)\n",
    "        step = np.sum(prices)/np.sum(std_error)\n",
    "        out = prices-std_error*step\n",
    "\n",
    "        return out\n",
    "\n",
    "    import optiver2023\n",
    "    env = optiver2023.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "\n",
    "    flag = 'test'\n",
    "    counter = 0\n",
    "    keep_col = [x for x in X.columns.tolist() if '_x' not in x]\n",
    "    \n",
    "    models = []\n",
    "    for i in range(num_folds + 1):\n",
    "        model_filename = f'/kaggle/input/trained-models/lgbm_{i+1}.pkl'\n",
    "        m = joblib.load(model_filename)\n",
    "        models.append(m)\n",
    "\n",
    "    # equal weights for each model\n",
    "    model_weights = [1 / len(models)] * len(models)\n",
    "\n",
    "    # placeholder DF to cache tests (for computing rolling features)\n",
    "    cache = pd.DataFrame()\n",
    "\n",
    "    qps = []\n",
    "\n",
    "\n",
    "    for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "        \n",
    "        test1 = test.copy()\n",
    "\n",
    "        test = test.drop('currently_scored', axis=1)\n",
    "        now_time = time.time()   \n",
    "        cache = pd.concat([cache, test], ignore_index=True, axis=0)\n",
    "\n",
    "        if counter > 0:\n",
    "            cache = cache.groupby(['stock_id']).tail(21).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n",
    "\n",
    "        test = generate_all_features(cache, correl_feats)[-len(test):]\n",
    "        test = \\\n",
    "        standardize_new_data(\n",
    "            stand_df, \n",
    "            test, \n",
    "            'stock_id'\n",
    "        )\n",
    "\n",
    "        test = test[keep_col]  \n",
    "\n",
    "        predictions = np.zeros(len(test))\n",
    "\n",
    "        for model, weight in zip(models, model_weights):\n",
    "            \n",
    "            predictions += weight * model.predict(test)\n",
    "\n",
    "        predictions = zero_sum(predictions, test1['bid_size'] + test1['ask_size'])\n",
    "\n",
    "        clipped_predictions = np.clip(predictions, y_min, y_max)\n",
    "\n",
    "        sample_prediction['target'] = clipped_predictions\n",
    "\n",
    "        env.predict(sample_prediction)\n",
    "        counter += 1\n",
    "        qps.append(time.time() - now_time)\n",
    "\n",
    "        if counter % 10 == 0:\n",
    "            print(f'{counter} queries per second: {np.mean(qps)}')\n",
    "\n",
    "    time_cost = 1.146 * np.mean(qps)\n",
    "    print(f'The code will take approximately {np.round(time_cost, 2)} hours')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7056235,
     "sourceId": 57891,
     "sourceType": "competition"
    },
    {
     "datasetId": 4095438,
     "sourceId": 7132863,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 855.268077,
   "end_time": "2023-12-06T00:39:04.271178",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-06T00:24:49.003101",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
